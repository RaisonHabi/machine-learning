### 1.随机初始化
 np.random.randn(维度) 来随机赋值：
### 2.何氏初始化（He Initialization）
论文He et al., 2015.中提出了一种方法，我们称之为He Initialization，它就是在我们随机初始化了之后，乘以 [公式] ，这样就避免了参数的初始值过大或者过小，因此可以取得比较好的效

He initialization的思想是：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0，所以，要保持variance不变，只需要在Xavier的基础上再除以2：
### 3.初始化为0的可行性？
答案是不可行。   
为什么将所有W初始化为0是错误的呢？  
是因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的    
--- gradient相同，weight update也相同。这显然是一个不可接受的结果。
### 4.Batch Normalization Layer
Batch Normalization是一种巧妙而粗暴的方法来削弱bad initialization的影响，其基本思想是：If you want it, just make it!

我们想要的是在非线性activation之前，输出值应该有比较好的分布（例如高斯分布），以便于back propagation时计算gradient，更新weight。  
Batch Normalization将输出值强行做一次Gaussian Normalization和线性变换：
### 5.Xavier initialization
Xavier initialization可以解决上面的问题！其初始化方式也并不复杂。Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。

&nbsp;
## reference
[聊一聊深度学习的weight initialization](https://zhuanlan.zhihu.com/p/25110150)   
[神经网络参数初始化的学问](https://zhuanlan.zhihu.com/p/39076763)
