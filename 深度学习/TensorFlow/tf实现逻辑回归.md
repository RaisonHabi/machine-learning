## Tensorflow实现逻辑回归
首先获取MNIST数据集，并将其读取到程序中。代码实现如下所示：
```

```
在设计逻辑回归模型之前，先设定模型的输入和待求变量，代码如下所示：
```
```
其中x是一个placeholder，它用来保存我们之后喂给模型的手写体数字图片，它的两个维度中None表示图片的数量，784表示每张图片的像素个数。  
**第一个维度采用None在Tensorflow里表示无穷，一开始这样使用是因为我们还不确定每次给模型喂的数据数量是多少，也方便后面调整其他超参数。**    
y同样是一个placeholder，它表示图像的真实分类，因为阿拉伯数字有10个，所以它的第二个维度是10，需要注意的是我们使用了one-hot编码

W和b是待求参数，它们再模型的训练过程中是不断变化的，所以采用了Variable来实现。  
W的维度为[784,10]，这表示每张图片的784个像素都需要被预测到0-9这10个数字上，也就是每个像素都对应一个10为的one-hot向量。而b则表示在这10个数字预测结果上的偏置。  
W和b可以随机初始化，也可以采用零值初始化，这里使用零值进行初始化。

下面我们正式构建逻辑回归模型：
```
```
让我们一行行看这部分代码，首先中就实现了逻辑回归公式，表示图片属于某一个类别的分数，然后外层我们再使用softmax作为激活函数，将其转换为每张图片属于10个数字的概率。需要注意的是，多分类任务一般使用softmax作为激活函数，二分类任务采用sigmoid。sotfmax函数将数据进行归一化处理，使所有数据都在0和1之间，并且求和为1。

再下一行我们定义了成本函数。这里我们没有完全依照逻辑回归的成本函数计算公式，而是使用了较简易的版本，将每个样本的损失函数定义为，其中表示样本所属的真实类别，表示我们模型的预测类别，然后利用reduce_sum函数将所有样本的损失函数值相加（注意reduction_indices=1表示将矩阵中的元素按行相加）再取负求平均，就得到了成本函数值。

然后optm使用了梯度下降法来对模型进行拟合，学习率我们设置为0.01，这里的学习目标是使成本函数值最小。

pred比较了预测结果和实际样本的类别，这里使用了argmax获取了每个向量最大值的索引，因为是one-hot编码，所以索引也就对应了实际的数字。

最后，accr将比较结果的bool类型变量转换为float型，再将得到的所有0和1求平均就是模型预测的准确率了。

下面是具体的训练过程代码：
```
```
首先，init初始化了所有的变量，training_epochs设置为50表示把所有的样本迭代50次，batch_size为100表示每个epoch中每次取100个样本进行训练，display_step用于输出训练过程，这里设置成每5个epoch输出一次。

在之后的for循环中，外层的for表示训练的epoch，在内层for循环之前初始化了avg_cost变量，num_batch计算了每个epoch的batch数量。内层for循环中的batch_xs和batch_ys获取每个batch的样本和标签，然后将其输入feed_dict来喂给模型，模型训练需要的一个参数是数据，另外一个就是optm优化器。最后再利用cost函数计算了平均成本函数值。在if条件结构中，我们利用了accr、训练数据和测试数据计算了模型在训练集和测试集上的准确率并打印输出使得我们能够方便的观察训练效果。

&nbsp;
## References
[Tensorflow实现逻辑回归](https://blog.csdn.net/hfutdog/article/details/82315194)   
