<p>值得注意的是，模型是无法处理文本字符的，所以不管是英文还是中文，我们都需要通过预训练模型BERT自带的字典vocab.txt将每一个字或者单词转换成字典索引（即id）输入。</p><p>(1) segment embedding的目的：有些任务是两句话一起放入输入X，而segment便是用来区分这两句话的。在Input那里就是用“[SEP]”作为标志符号。而“[CLS]”用来分类输入的两句话是否有上下文关系。</p><p>(2) position embedding的目的：因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding。构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding：使用正弦函数，位置维度对应曲线，而且方便序列之间的选对位置，使用正弦会比余弦好的原因是可以在训练过程中，将原本序列外拓成比原来序列还要长的序列，如公式（8.1）~（8.2）所示。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/50/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_hd.jpg?source=1940ef5c" data-caption="" data-size="normal" data-rawwidth="706" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="706" data-original="https://picb.zhimg.com/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_r.jpg?source=1940ef5c"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;706&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="706" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="706" data-original="https://picb.zhimg.com/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_r.jpg?source=1940ef5c" data-actualsrc="https://pic4.zhimg.com/50/v2-4c4d98fd827d401663cdc0d6ab8fbbd0_hd.jpg?source=1940ef5c"/></figure>

## reference
[Position Encoding 是怎么回事？](https://www.zhihu.com/question/56476625/answer/1034797392)
