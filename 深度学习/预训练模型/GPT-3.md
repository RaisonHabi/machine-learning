GPT-3依旧延续自己的**单向语言模型训练方式，只不过这次把模型尺寸增大到了1750亿，并且使用45TB数据进行训练**。  

同时，GPT-3主要聚焦于更通用的NLP模型，解决当前BERT类模型的两个缺点：
```
对领域内有标签数据的过分依赖：
虽然有了预训练+精调的两段式框架，但还是少不了一定量的领域标注数据，否则很难取得不错的效果，而标注数据的成本又是很高的。

对于领域数据分布的过拟合：
在精调阶段，因为领域数据有限，模型只能拟合训练数据分布，如果数据较少的话就可能造成过拟合，致使模型的泛华能力下降，更加无法应用到其他领域。
```
因此GPT-3的主要**目标是用更少的领域数据、且不经过精调步骤去解决问题**。


## reference
[如何评价GPT-3](https://www.zhihu.com/question/398114261/answer/1253942032)
