## shap值
```
import shap
from scipy.interpolate import interp1d
shap.initjs()

#模型文件和预测结果
model = xgb_model_increment_new
X = idx_trian.loc[:,feature_new]
pred = model.predict(xgb.DMatrix(data=idx_trian.loc[:,feature_new], label=idx_trian.loc[:,"label"]))

# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)
shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])
```
每个特征的shap值：  
```
shap.summary_plot(shap_values, X)
```
样本粒度shap值
```
shap_df = pd.DataFrame(shap_values, columns=X.columns.to_list(), index=X.index)
shap_df.head()
```
shap值转换为概率
```
def shap2deltaprob(features, 
                   shap_df, 
                   shap_sum,
                   probas,
                   func_shap2probas):
    '''
    map shap to Δ probabilities
    --- input ---
    :features: list of strings, names of features
    :shap_df: pd.DataFrame, dataframe containing shap values
    :shap_sum: pd.Series, series containing shap sum for each observation
    :probas: pd.Series, series containing predicted probability for each observation
    :func_shap2probas: function, maps shap to probability (for example interpolation function)
    --- output ---
    :out: pd.Series or pd.DataFrame, Δ probability for each shap value
    '''
    # 1 feature
    if type(features) == str or len(features) == 1:
        return probas - (shap_sum - shap_df[features]).apply(func_shap2probas)
    # more than 1 feature
    else:
        return shap_df[features].apply(lambda x: shap_sum - x).apply(func_shap2probas)\
                .apply(lambda x: probas - x)
```
```
probas_cat = pd.Series(pred, index=X.index)

# build interpolation function to map shap into probability
shap_sum = shap_df.sum(axis = 1)
shap_sum_sort = shap_sum.sort_values()
probas_cat_sort = probas_cat[shap_sum_sort.index]

intp = interp1d(shap_sum_sort,
                probas_cat_sort, 
                bounds_error = False, 
                fill_value = (0, 1))

# show Δ probabilities
temp = shap2deltaprob(X.columns.to_list(),
                      shap_df,
                      shap_sum,
                      probas_cat,
                      func_shap2probas=intp).head()\
        .applymap(lambda x:('+'if x>0 else '')+str(round(x*100,2))+'%')

temp.style.apply(lambda x: ["background:orangered" if float(v[:-1])<0 else "background:lightgreen"
                                for v in x], axis = 1)
```

&nbsp;
## psi
```
# 查看下PSI
from scipy.stats import entropy as sc_entropy
class  PSI():
#     def __init__(self
#                  ,df
#                  ,binning=False
#                  ,seg_key='shouxin_m'
#                 ,refer_seg_key=None):

#         self.binning=binning
#         self.seg_key = seg_key
#         self.refer_seg_key=refer_seg_key
        
            
    def seg_stat(self,df,seg_key="shouxin_m",binning=False,bins=6,refer_seg_key=None):
        psicnt_dict = dict()
        refer_na_ratio = dict()
        refer_entropy = dict()
        cols = [x for x in df.columns if x not in [seg_key]]
        if binning:
            for  x  in cols: 
                df_seg = df[df[seg_key]==refer_seg_key]
                seg_cnt = len(df_seg)
                null_cnt = df_seg[x].isnull().sum()
                if (null_cnt==seg_cnt):
                    print("【{}】is all null in seg {}".format(x,refer_seg_key))
                    continue
                    
                refer_na_ratio[x]=null_cnt/seg_cnt
                
                
                uni_cnt = len(df_seg[x].unique())
                
                # category
                if df[x].dtype == 'object' or uni_cnt < bins:
                    df_c = df[[seg_key,x]].fillna('nan').groupby([seg_key,x]).size()
                    df_c = df_c.reset_index(name='count').pivot(index=x,columns=seg_key,values='count')
                    psicnt_dict[x]=df_c
                else: 
                    # get x cut point
                    _,cuts = pd.qcut(df_seg[x],q=bins,duplicates='drop',retbins=True)
                    # factor 
                    if len(cuts)< bins and  uni_cnt < 3*bins:
                        df_c = df[[seg_key,x]].fillna('nan').groupby([seg_key,x]).size()
                    # numeric
                    else:
                        print("binning:",x)
                        df_b = df[[seg_key,x]].copy()
                        df_b[x] = pd.cut(df_b[x],bins=cuts).astype('str').fillna('nan')
                        df_c = df_b.groupby([seg_key,x]).size()
                    
                    df_c= df_c.reset_index(name='count').pivot(index=x,columns=seg_key,values='count')
                    psicnt_dict[x]=df_c
                    
                refer_entropy[x] = sc_entropy(df_c[refer_seg_key].fillna(0))
        else:
            for x in cols:
                df_c = df[[seg_key,x]].fillna('nan').groupby([seg_key,x]).size()
                df_c= df_c.reset_index(name='count').pivot(index=x,columns=seg_key,values='count')
                psicnt_dict[x]=df_c
                
                df_seg = df[df[seg_key]==refer_seg_key]
                seg_cnt = len(df_seg)
                null_cnt = df_seg[x].isnull().sum()
                
                refer_na_ratio[x] = null_cnt/seg_cnt
#                 display(df_c )
                refer_entropy[x] = sc_entropy(df_c[refer_seg_key].fillna(0))
                
        self.psicnt_dict = psicnt_dict
        self.refer_na_ratio = refer_na_ratio
        self.refer_entropy = refer_entropy
                    
    def psi(self,df,seg_key='shouxin_m',binning=False,bins=6,refer_seg_key=None):
        print("shape:",df.shape)
        display(df.groupby(seg_key).size())
        self.seg_stat(df,seg_key=seg_key,refer_seg_key = refer_seg_key,binning=binning,bins=bins)
        
        psi_pct_dict =  dict()
        # cal psi 
        for k,v in self.psicnt_dict.items():
            psi_pct_dict[k]= self.psicnt_to_psi(v,refer_seg_key) 
        self.psi_pct_dict = psi_pct_dict
        # final psi_df
        psi_df = pd.DataFrame()
        
        
        for k,v in self.psi_pct_dict.items(): 
#             print(k)
#             display(v)
            psi_v = v.loc[(v.index=='psi'),v.columns!=refer_seg_key]
            psi_v['min'],psi_v['max'],psi_v['mean'] = psi_v.min(axis=1),psi_v.max(axis=1),psi_v.mean(axis=1)
            psi_v.insert(loc=0, column='refer_na_ratio', value=self.refer_na_ratio[k])
            psi_v.insert(loc=0,column='refer entropy',value=self.refer_entropy[k])
            psi_v.insert(loc=0,column='bins',value= self.psicnt_dict[k].shape[0])
            psi_v.insert(loc=0, column='x', value=k)
            psi_df = psi_df.append(psi_v)
        psi_df = psi_df.sort_values(by='max',ascending=False)
        psi_df = psi_df.reset_index(drop=True)
        psi_df = psi_df.set_index('x')
        
        self.psi_df = psi_df
        
    @staticmethod
    def psicnt_to_psi(psicnt,refer_seg_key,smooth=0.0000001):
        psi_pct = psicnt.fillna(0)/psicnt.sum()
        psi_pct = psi_pct.clip(lower=smooth)
        psi_v = (np.log(psi_pct.div(psi_pct[refer_seg_key],axis=0))
                 *psi_pct.sub(psi_pct[refer_seg_key],axis=0)).sum(axis=0)
        psi_pct = psi_pct.append(psi_v.to_frame(name='psi').T)
        return psi_pct
```
```
# 计算变量PSI
df_woe_train_notsame_new['psi_seg'] = df_woe_train_notsame_new['_mt_month']
df_woe_train_notsame_new.loc[df_woe_train_notsame_new['_mt_month']<='2019-08','psi_seg']='2019-4-8'
df_woe_train_notsame_new['psi_seg'].value_counts()
psi_pboc = PSI()  
cols = df_woe_train_notsame_new.columns[1:]
print(len(cols))

psi_pboc.psi(round(df_woe_train_notsame_new[cols],4),binning=True,bins=6
          ,seg_key='psi_seg'
         ,refer_seg_key='2019-4-8')

psi_filtered = psi_pboc.psi_df[psi_pboc.psi_df['max']<=0.1]

psi_filtered_columns = psi_filtered.index.values
```
