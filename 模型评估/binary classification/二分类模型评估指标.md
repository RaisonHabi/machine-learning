## 一、评价指标
## 结论：
### AUC 适用于正负样本相对balance的情况，且分类问题对模型预测概率的准确度没有要求的情况。
### AP 刚好弥补AUC曲线的不足，适用于正负样本imbalance的情况，或者我们更关心模型在正样本上表现的情况。但AP同样不能保证模型预测概率的准确率。
### cross-entropy弥补了AP和AUC的不足。如果分类目标其实是获得对真实概率的估计的话，使用cross-entropy应该是你的选择。
### 1、基于阈值的指标
#### ***1、当目标是对正样本进行准确预测 - precision, recall, F1***
```
precision从预测的角度衡量预测为正的准确率，recall从真实分布的角度衡量预测为正的准确率。
precision和recall存在trade-off, 想要挑选出更多的正样本，就要承担预测为正准确率下降的风险。

既然有trade-off，一般就会用可以综合两个指标的复合指标 - F1 Score
```
#### ***2、当目标是对真实分布进行准确预测 - sensitivity(recall), specifity, fpr***
```
sensitivity, sepcifity都从真实分布的角度，分别衡量正/负样本预测的准确率。
这一对搭配最常在医学检验中出现，衡量实际生病/没生病的人分别被正确检验的概率。
正确检验出一个人有病很重要，同时正确排除一个人没生病也很重要。

如果specifity对很多人来说很陌生的话，它兄弟很多人一定知道fpr。fpr和recall(tpr)一起构成了ROC曲线。
这一对的tradeoff同样用医学检验的逻辑来解释就是，医生既不希望遗漏病人的病情（recall），要不希望把本身没病的人吓出病来（fpr）。
```
### 2、综合评价指标
综合评价指标基本都是对上述指标再加工的产物。
#### ***1、tpr(recall) + fpr = ROC-> AUC***
```
随着阈值从1下降到0，我们预测为正的样本会逐渐变多，被正确筛选出的正样本会逐渐增多，但同时负样本被误判为正的概率也会逐渐上升。

整个遍历阈值的过程可以用ROC曲线来表示，横轴是误判率（fpr)，纵轴是准确率(tpr/recall/sensitivity)。
但是给你两个分类器想要直接比较谁的ROC曲线会有点困难，所以我们用一个scaler来描述ROC曲线就是AUC - Area under curve。 
ROC曲线下的面积越大越接近完美的分类器，而对角线50%是随机猜正负就可以得到的AUC。
```
**AUC 适用于正负样本相对balance的情况，且分类问题对模型预测概率的准确度没有要求的情况。**
#### ***2、precision + recall = AUCPR（AP）***
```
和上述ROC-AUC的思路相同。随着阈值从1下降到0，预测为正的样本变多，被正确筛选出的正样本增多，但同时预测为正的准确率会下降。

这样我们得到PR曲线，以及曲线下的面积AUCPR。有时AUCPR也被称作AP，就是对所有recall取值对应的precision求平均。
第一眼看上去我也被糊弄了，一直当成两个概念来记。但是式子一写出来，妈呀这俩不是一个东西么。
```
**AP 刚好弥补AUC曲线的不足，适用于正负样本imbalance的情况，或者我们更关心模型在正样本上表现的情况。但AP同样不能保证模型预测概率的准确率。**
#### ***3、cross-entropy loss***
```
cross-entropy放在这里会有点奇怪，因为本质上它是和这里其他所有指标都不同的存在。
其他的评价指标评价的是0/1的分类效果，或者更准确说是对排序效果（根据阈值把预测值从大到小分成0/1两半）进行评价。
但是cross-entropy是直接对预测概率是否拟合真实概率进行评价。
```
**cross-entropy弥补了AP和AUC的不足。如果分类目标其实是获得对真实概率的估计的话，使用cross-entropy应该是你的选择。**
#### ***4、*Mean F1 Score***
```
第一次见到这个指标是在Instacart的kaggle比赛里面。这里的mean不是指的对所有阈值下的F1求平均值而是对每个order_id的多个product_id求F1，再对所有order_id的F1求平均，有点绕...

之所以把这个评价指标也放在这里是因为这个特殊的评价方法会对你如何split训练集/测试集，以及如何选定最优的阈值产生影响。
```

&nbsp;
## 二、问题解决
### 问题1 Rank or Probability?
分类问题可以根据对输出形式的要求分成两类

一种我们只关心排序。比如电商场景下，用户是否会回购某商品，我们更关心用户回购商品A的概率是否高于用户回购商品B的概率，然后把回购概率更高的商品放在推荐列表前面。这时分类问题其实是用来排序的。--样本间的相对排序比较比绝对概率更重要
另一种我们关心概率。比如现在大家都在谈增长，我们想知道一个用户明天在app活跃的概率，只知道用户A比用户B活跃的概率高并不够，我们需要明确知道用户A活跃的概率，究竟是90%还是50%，这样才能对高/低于特定概率的用户进行一定（促活/唤醒）操作。这时分类问题是对真实概率的估计 --样本的绝对概率需要接近真实概率，并且天极稳定
有人会问，上述两种需求究竟对解决一个二分类问题有什么影响？ 答案是损失函数/评价指标
### 2、问题2 Imbalanced Sample ？
AP是recall和precision组成的PR的曲线下面积。这里recall和precision分别从真实分布和预测分布两个角度衡量了对正样本的预测准确率。说到这里已经有人反应过来了。是的这一对trade-off指标都是针对正样本的，在计算中没有用到True negative.所以当你的数据集存在Imbalance的时候，AP一般会是更好的选择。

&nbsp;
## References
[聊聊评价指标的那些事儿](https://cloud.tencent.com/developer/article/1500247?from=article.detail.1500245)  
[聊聊评价指标的那些事儿-实战](https://cloud.tencent.com/developer/article/1500245)
