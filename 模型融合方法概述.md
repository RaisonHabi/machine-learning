## Voting
模型融合其实也没有想象的那么高大上，从最简单的Voting说起，这也可以说是一种模型融合。  
**假设对于一个二分类问题，有3个基础模型，那么就采取投票制的方法，投票多者确定为最终的分类。**
## Averaging
对于回归问题，一个简单直接的思路是取平均。  
**稍稍改进的方法是进行加权平均**。  
权值可以用排序的方法确定，举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6  
这两种方法看似简单，其实后面的高级算法也可以说是基于此而产生的，Bagging或者Boosting都是一种把许多弱分类器这样融合成强分类器的思想。
## Bagging
**Bagging就是采用 *有放回的方式进行抽样*， 用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合**。  
大概分为这样两步：  
> **1.重复K次**  
有放回地重复抽样建模;    
训练子模型  
**2.模型融合**  
分类问题：voting;    
回归问题：average

Bagging算法不用我们自己实现，随机森林就是基于Bagging算法的一个典型例子，采用的基分类器是决策树。R和python都集成好了，直接调用。

## Boosting
***Bagging算法可以并行处理***，**而Boosting的思想是一种迭代的方法，每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例**。  
**最终将这些弱分类器进行加权相加**。引用加州大学欧文分校Alex Ihler教授的两页PPT  

同样地，基于Boosting思想的有AdaBoost、GBDT等，在R和python也都是集成好了直接调用。  

PS：理解了这两点，面试的时候关于Bagging、Boosting的区别就可以说上来一些，问Randomfroest和AdaBoost的区别也可以从这方面入手回答。  
也算是留一个小问题，随机森林、Adaboost、GBDT、XGBoost的区别是什么？
## Stacking
Stacking模型本质上是一种分层的结构，这里简单起见，只分析二级Stacking.假设我们有3个基模型M1、M2、M3。(详见参考文档)    

Stacking本质上就是这么直接的思路，但是这样肯定是不行的，问题在于P1的得到是有问题的，  
***用整个训练集训练的模型反过来去预测训练集的标签，毫无疑问过拟合是非常非常严重的,***        
**因此现在的问题变成了如何在解决过拟合的前提下得到P1、P2、P3，这就变成了熟悉的节奏——K折交叉验证。** 

**[详见下面两个英文文档的图，清晰解释了k折交叉在stacking的应用]**     

### StackingClassifier
An ensemble-learning meta-classifier for stacking.  
***from mlxtend.classifier import StackingClassifier***  

**Overview**  
> Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier.   
The individual classification models are trained based on the complete training set;  
then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble.   
The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.

***Please note that this type of Stacking is prone to overfitting due to information leakage.***    
The related StackingCVClassifier.md does not derive the predictions for the 2nd-level classifier from the same datast that was used for training the level-1 classifiers and is recommended instead.

### StackingCVClassifier
An ensemble-learning meta-classifier for stacking **using cross-validation to prepare the inputs for the level-2 classifier to prevent overfitting.**  
***from mlxtend.classifier import StackingCVClassifier***  







## reference
[【机器学习】模型融合方法概述](https://zhuanlan.zhihu.com/p/25836678)  
[集成学习总结 & Stacking方法详解](https://blog.csdn.net/willduan1/article/details/73618677)  
[StackingClassifier](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)  
[StackingCVClassifier](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)
