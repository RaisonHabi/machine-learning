## Bagging
放回抽样，多数表决（分类）或简单平均（回归）,同时Bagging的基学习器之间属于并列生成，不存在强依赖关系  

Random Forest（随机森林）是Bagging的扩展变体，它在以决策树 为基学习器构建Bagging集成的基础上，**进一步在决策树的训练过程中引入了随机特征选择**，因此可以概括RF包括四个部分：  
> 1、随机选择样本（放回抽样）；  
2、随机选择特征；  
3、构建决策树；  
4、随机森林投票（平均）

随机选择特征是指在树的构建中，会**从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属性用于划分**，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型

在构建决策树的时候，RF的每棵决策树**都最大可能的进行生长而不进行剪枝**；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF的重要特性是**不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估**，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。

随机森林的优点较多:  
> 1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；  
2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；  
3、容易做成并行化方法。

RF的缺点：在噪声较大的分类或者回归问题上会过拟合。

&nbsp;
## Boosting算法
GBDT、XGBoost、LightGBM都属于Boosting方法，且GBDT是机器学习算法，XGBoost和LightGBM是GBDT的算法实现。

Boosting串行训练，通过关注被已有分类器错分的那些数据来获得新的分类器。  
由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而**Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度**。

### GBDT
GradientBoosting算法关键就是**利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树**。  
GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，与传统的Boosting中关注正确错误的样本加权有着很大的区别。

**注意**：GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。

GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显：  
> 1、它能灵活的处理各种类型的数据；  
2、在相对较少的调参时间下，预测的准确度较高。  

当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。

### XGBoost
优点：  
> 1、传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；  
　　2、XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性；  
　　3、shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；  
　　4、列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算；  
　　5、对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向；
  
缺点：  
> 1、level-wise 建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。  
　　2、预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)
　
### LightGBM
1、leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。

　　2、lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。  
　　（1）内存上优势：很明显，直方图算法的内存消耗为(#data* #features * 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 * #data * #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。  
　　（2）计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)
  
　　3、直方图做差加速: 一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

　　4、lightgbm支持直接输入categorical 的feature:在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。

　　5、多线程优化

&nbsp;
## reference
[RF、GBDT、XGBoost、lightGBM原理与区别](https://blog.csdn.net/data_scientist/article/details/79022025)  
[GBDT、XGBoost、LightGBM的区别和联系](https://www.jianshu.com/p/765efe2b951a)  
[XGBoost, LightGBM性能大对比](https://zhuanlan.zhihu.com/p/24498293) 
