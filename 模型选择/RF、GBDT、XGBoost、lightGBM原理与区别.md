## QA
### 1.gbdt用于分类和回归的区别
[深入剖析梯度提升决策树(GBDT)分类与回归](https://blog.csdn.net/pxhdky/article/details/84938536)   

GBDT的基学习器限定为决策树，且是回归树。无论是GBDT分类算法还是回归算法，弱学习器都是回归树，这是由残差本质决定的。

在提升方法中，每次迭代的优化问题可以分为两部分：一、求叶结点区域；二、给定叶结点区域，求区域内最优拟合值。   
对于第二个问题，它是一个简单的“定位”估计，最优解很容易得到；   
但对于第一个问题，当损失函数不是平方误差和指数损失，而是一般损失函数时，求解区域是困难的，最小化损失函数问题的简单、快速求解算法是不存在的。   
**针对这一问题，梯度提升决策树利用最速下降法来近似求解加法模型中的每一颗决策树，具体来说，就是在每次迭代中，使新建的决策树都沿损失函数减少最快的方向——负梯度方向减少损失函数**。  
(最速下降法（Steepest descent）是梯度下降法的一种更具体实现形式，其理念为在每次迭代中选择合适的步长αk，使得目标函数值能够得到最大程度的减少。 有意思的是，最速下降法每次更新的轨迹都和上一次垂直。   
在确定搜索方向时，梯度下降和最速下降只用到了目标函数的一阶导数（梯度）    
[02 梯度下降法、最速下降法、牛顿法、共轭方向法、拟牛顿法](https://www.cnblogs.com/wuliytTaotao/p/10603576.html#%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95))

**当损失函数是平方误差时，当前模型的负梯度就等于残差，沿负梯度方向减少损失函数就相当于拟合残差**。

**但当损失函数不是平方误差时，负梯度就是残差的近似值，称为“广义残差或伪残差”**。例如，当损失函数是绝对误差时，负梯度是残差的符号函数，因此在每次迭代时，决策树将拟合当前残差的符号。

总之，GBDT利用广义残差来拟合每一轮迭代中的回归树。

**在分类任务中，由于样本输出是离散值，无法从输出类别拟合残差，因此使用类别的预测概率值和真实概率值的差来当做残差**。

GBDT分类算法的损失函数可以取指数损失函数和对数似然函数，如果选择指数损失函数，则GBDT退化为AdaBoost。因此我们这里只讨论对数似然损失函数。

二元分类的对数似然损失函数是：

&nbsp;
## Bagging
放回抽样，多数表决（分类）或简单平均（回归）,同时Bagging的基学习器之间属于并列生成，不存在强依赖关系  

Random Forest（随机森林）是Bagging的扩展变体，它在以决策树 为基学习器构建Bagging集成的基础上，**进一步在决策树的训练过程中引入了随机特征选择**，因此可以概括RF包括四个部分：  
> 1、随机选择样本（放回抽样）；  
2、随机选择特征；  
3、构建决策树；  
4、随机森林投票（平均）

随机选择特征是指在树的构建中，会**从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属性用于划分**，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型

在构建决策树的时候，RF的每棵决策树**都最大可能的进行生长而不进行剪枝**；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF的重要特性是**不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估**，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。

随机森林的优点较多:  
> 1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；  
2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；  
3、容易做成并行化方法。

RF的缺点：在噪声较大的分类或者回归问题上会过拟合。

&nbsp;
## Boosting算法
GBDT、XGBoost、LightGBM都属于Boosting方法，且GBDT是机器学习算法，XGBoost和LightGBM是GBDT的算法实现。

Boosting串行训练，通过关注被已有分类器错分的那些数据来获得新的分类器。  
由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而**Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度**。

### GBDT
GradientBoosting算法关键就是**利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树**。  
GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，与传统的Boosting中关注正确错误的样本加权有着很大的区别。

**注意**：GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。

GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显：  
> 1、它能灵活的处理各种类型的数据；  
2、在相对较少的调参时间下，预测的准确度较高。  

当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。

### XGBoost
优点：  
> 1、传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；  
　　2、XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性；  
　　3、shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；  
　　4、列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算；  
　　5、对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向；
  
缺点：  
> 1、level-wise 建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。  
　　2、预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)
　
### LightGBM
1、leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。

　　2、lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。  
　　（1）内存上优势：很明显，直方图算法的内存消耗为(#data* #features * 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 * #data * #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。  
　　（2）计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)
  
　　3、直方图做差加速: 一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

　　4、lightgbm支持直接输入categorical 的feature:在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。

　　5、多线程优化

&nbsp;
## reference
[RF、GBDT、XGBoost、lightGBM原理与区别](https://blog.csdn.net/data_scientist/article/details/79022025)  
[GBDT、XGBoost、LightGBM的区别和联系](https://www.jianshu.com/p/765efe2b951a)  
[XGBoost, LightGBM性能大对比](https://zhuanlan.zhihu.com/p/24498293) 
