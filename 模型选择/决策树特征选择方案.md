不同的决策树算法有着不同的特征选择方案。ID3用信息增益，C4.5用信息增益率，CART分类用gini系数、回归用均方误差。 

熵：表示随机变量的不确定性。    
条件熵：在一个条件下，随机变量的不确定性。    
信息增益=熵 - 条件熵，表示信息不确定性减少的程度（等价于训练集中类与特征的互信息）。    

极端情况：若每个属性中每种类别都只有一个样本，那这样属性信息熵就等于零(条件熵为0，信息增益特别大，但毫无意义)，根据信息增益就无法选择出有效分类特征。   
一般情况：信息增益存在偏向于选择取值较多的特征的问题，用信息增益比校正：特征A对训练集D的信息增益与训练集D关于特征A的熵之比。

GINI指数 ：   
分类问题中，假设有K个类，样本点属于第k类的概率为pk ，则概率分布的基尼指数定义为1-pk 平方和。   
总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。比如体温为恒温时包含哺乳类5个、鸟类2个，则： 
GINI=1−[(5/7)^2+(2/7)^2]=20/49 

### 而XGBoost则提出了一种新的增益计算方法。
如果已经确定了树的结构 {𝑅𝑗}𝐽1 ，则直接对 (1.4) 式求导，最优值为：

。。。

该式的优点是除了能作为树分裂的衡量标准外，还能使 XGboost 适用于各种不同的损失函数，所以 XGBoost 包中支持自定义损失函数，但前提是一阶和二阶可导。

从另一角度看， (1.5) 式就类似于传统决策树中的不纯度指标，在决策树中我们希望一次分裂后两边子树的不纯度越低越好，对应到XGBoost中则是希望 (1.5)式 越小越好，即 𝐺2𝑗𝐻𝑗+𝜆 越大越好，这样分裂前后的增益定义为：

𝐺2𝐿𝐻𝐿+𝜆  为分裂后左子树的分数，𝐺2𝑅𝐻𝑅+𝜆 为分裂后右子树的分数，(𝐺𝐿+𝐺𝑅)2𝐻𝐿+𝐻𝑅+𝜆 为分裂前的分数，𝐺𝑎𝑖𝑛 越大说明越值得分裂。当然 (1.6) 式中 𝐺𝑎𝑖𝑛 可能会变成负的，这个时候分裂后的目标函数不会减少，但这并不意味着不会分裂 。事实上 XGBoost 采用的是后剪枝的策略，建每棵树的时候会一直分裂到指定的最大深度(max_depth)，然后递归地从叶结点向上进行剪枝，对之前每个分裂进行考察，如果该分裂之后的 𝐺𝑎𝑖𝑛⩽0，则咔咔掉。 𝛾 是一个超参数，具有双重含义，一个是在 (1.3) 式中对叶结点数目进行控制的参数；另一个是 (1.6) 式中分裂前后 𝐺𝑎𝑖𝑛 增大的阈值，当然二者的目的是一样的，即限制树的规模防止过拟合。

[集成学习之Boosting —— XGBoost](https://www.cnblogs.com/massquantity/p/9794480.html)

&nbsp;
### 信息增益(Information Gain)
　　下面来介绍信息增益，所谓的信息增益，是要针对于具体的属性来讲的，比如说，数据集D中含有两个类别，分别是好人和坏人，那么，随便选择一个属性吧，比如说性别，性别这个属性中包含两个值，男人和女人，如果用男人和女人来划分数据集D的话，会得到两个集合，分别是和。划分后的两个集合中各自有 好人和坏人，所以可以分别计算划分后两个集合的纯度，计算之后，把这两个集合的信息熵求加权平均，跟之前没有划分的时候比，用后者减去前者，得到的就是属性-性别对样本集D划分所得到的信息增益。可以通俗理解为，信息增益就是纯度提升值，用属性对原数据集进行划分后，得到的信息熵的差就是纯度的提升值。信息增益的公式如下： 
　　 
　　先解释一下上式中的参数，是数据集，是选择的属性，中一共有个取值，用这个取值去划分数据集，分别得到数据集到，分别求这个数据集的信息熵，并将其求加权平均。两者的差得到的就是信息增益。 
　　那么这个信息增益有什么用呢？有用，可以根据信息增益值的大小来判断是否要用这个属性去划分数据集，如果得到的信息增益比较大，那么就说明这个属性是用来划分数据集比较好的属性，否则则认为该属性不适合用来划分数据集。这样有助于去构建决策树。著名的算法ID3就是采用信息增益来作为判断是否用该属性划分数据集的标准。
### 信息增益率(Information Gain Ratio)
　　为什么要提出信息增益率这种评判划分属性的方法？信息增益不是就很好吗？其实不然，用信息增益作为评判划分属性的方法其实是有一定的缺陷的，书上说，信息增益准则对那些属性的取值比较多的属性有所偏好，也就是说，采用信息增益作为判定方法，会倾向于去选择属性取值比较多的属性。那么，选择取值多的属性为什么就不好了呢？举个比较极端的例子，如果将身份证号作为一个属性，那么，其实每个人的身份证号都是不相同的，也就是说，有多少个人，就有多少种取值，它的取值很多吧，让我们继续看，如果用身份证号这个属性去划分原数据集，那么，原数据集中有多少个样本，就会被划分为多少个子集，每个子集只有一个人，这种极端情况下，因为一个人只可能属于一种类别，好人，或者坏人，那么此时每个子集的信息熵就是0了，就是说此时每个子集都特别纯。这样的话，会导致信息增益公式的第二项整体为0，这样导致的结果是，信息增益计算出来的特别大，然后决策树会用身份证号这个属性来划分原数据集，其实这种划分毫无意义。因此，为了改变这种不良偏好带来的不利影响，提出了采用信息增益率作为评判划分属性的方法。 
　　公式如下： 
　　 
　　其中的计算方式如下： 
　　 
　　被称为是的“固有值”，这个的公式是不是很熟悉啊，简直和信息熵的计算公式一毛一样，就是看属性的纯度，如果只含有少量的取值的话，那么的纯度就比较高，否则的话，的取值越多，的纯度越低，的值也就越大，因此，最后得到的信息增益率就越低。 

### ID3\C4.5
采用信息增益率可以解决ID3算法中存在的问题，因此将采用信息增益率作为判定划分属性好坏的方法称为C4.5。  
需要注意的是，增益率准则对属性取值较少的时候会有偏好，为了解决这个问题，C4.5并不是直接选择增益率最大的属性作为划分属性，而是之前先通过一遍筛选，先把信息增益低于平均水平的属性剔除掉，之后从剩下的属性中选择信息增益率最高的，这样的话，相当于两方面都得到了兼顾。 

## reference
[信息熵、信息增益与信息增益率
](https://blog.csdn.net/songhao22/article/details/77150972)
