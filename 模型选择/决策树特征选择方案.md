不同的决策树算法有着不同的特征选择方案。ID3用信息增益，C4.5用信息增益率，CART分类用gini系数、回归用均方误差。 

熵：表示随机变量的不确定性。    
条件熵：在一个条件下，随机变量的不确定性。    
信息增益=熵 - 条件熵，表示信息不确定性减少的程度（等价于训练集中类与特征的互信息）。    

极端情况：若每个属性中每种类别都只有一个样本，那这样属性信息熵就等于零，根据信息增益就无法选择出有效分类特征。   
一般情况：信息增益存在偏向于选择取值较多的特征的问题，用信息增益比校正：特征A对训练集D的信息增益与训练集D关于特征A的熵之比。

GINI指数 ：   
分类问题中，假设有K个类，样本点属于第k类的概率为pk ，则概率分布的基尼指数定义为1-pk 平方和。   
总体内包含的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。比如体温为恒温时包含哺乳类5个、鸟类2个，则： 
GINI=1−[(5/7)^2+(2/7)^2]=20/49 
