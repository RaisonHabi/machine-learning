将主流 Boosting 实现方式做一个分析汇总
## 一、Boosting算法
Boosting算法特征如下：通过将一些表现效果一般（可能仅仅优于随机猜测）的模型通过特定方法进行组合来获得一个表现效果较好的模型。  

从抽象的角度来看，Boosting算法是借助convex loss function在函数空间进行梯度下降的一类算法。  

Gradient Boost和Adaboost就是其中比较常见的两种。

&nbsp;
## 二、AdaBoost（Adaptive Boosting)
详见参考文档解释过程，简单易懂。

在训练过程中，**每个新的模型都会基于前一个模型的表现结果进行调整，这也就是为什么 AdaBoost 是自适应（adaptive）的原因**。

AdaBoost确实采用的是指数损失，基分类器最常见的是决策树（在很多情况下是决策树桩，深度为1的决策树）。  

**在每一轮提升相应错分类点的权重可以被理解为调整错分类点的observation probability**。

最终我们得到了多个线性分类器，把这些线性分类器的结果做一个线性组合，我们就得到了整个集成模型的结果。  
**每个线性分类器的结果的系数（权重）取决于它的表现，表现越好，权重越高**。  
比如第一条线段的分类错误就优于第二条线段，那么它获得的权重也就会更大。集成模型的效果非常好。

&nbsp;
## 三、Gradient Boosting
在 AdaBoost 发表后不久，Breiman 等人发表了……   
可以说 AdaBoost 是 Gradient Boosting 的一个特例或者Gradient Boosting是对AdaBoost进行推广。

**模型的训练过程是对一任意可导目标函数的优化过程**。   
**通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化**。因此可以说 Gradient Boosting = Gradient Descent + Boosting。

和 AdaBoost 一样，Gradient Boosting 也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。  
### 1.不同
**AdaBoost 是通过提升错分数据点的权重来定位模型的不足**  
而 **Gradient Boosting 是通过算梯度（gradient）来定位模型的不足**。因此相比 AdaBoost, Gradient Boosting 可以使用更多种类的目标函数。

### 2.Gradient Boosting for Regression
在这里**用回归树拟合残差实际上就是用回归树拟合负梯度（当损失函数不为square loss时残差并不一定等于负梯度！）**。   
我们**实际上是在通过梯度下降法对模型参数进行更新**。这样理解的好处在于我们可以把这一算法推广到其它的损失函数上。  
要注意regression并不一定会用square loss。square loss的优点是便于理解和实现，缺点在于对于异常值它的鲁棒性较差。

&nbsp;
## 四、对比
### 1.AdaBoost V.S. GBDT
最主要的区别在于两者如何识别模型的问题。  
**AdaBoost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型**。  
**Gradient Boosting通过负梯度来识别问题，通过计算负梯度来改进模型**。

### 2.GBDT V.S. LR(Linear Regression? Logistic Regression?)
从决策边界来说，  
线性回归的决策边界是一条直线，  
**逻辑回归的决策边界根据是否使用核函数可以是一条直线或者曲线**，  
而GBDT的决策边界可能是很多条线。

### 3.GBDT 与 XGBoost 区别
```
1.传统GBDT以CART作为基分类器，xgboost还支持线性分类器，
这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

2.传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。
顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。

3.xgboost在代价函数里加入了正则项，用于控制模型的复杂度。
正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。
从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。

4.Shrinkage（缩减），相当于学习速率（xgboost中的eta）。
xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。
实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

5.列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，
还能减少计算，这也是xgboost异于传统gbdt的一个特性。

6.对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

7.xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？
注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。
xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），
xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。
这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，
最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。（就是 XGBoost 论文中介绍的加权直方图，这里权值是特征的二阶梯度，因为其目标函数可以化简为二次项系数为 H 的二次多项式）
```

&nbsp;
## reference
[Adaboost, GBDT 与 XGBoost 的区别](https://zhuanlan.zhihu.com/p/42740654)
