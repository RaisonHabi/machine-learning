### 1.xgb缺点和lightgbm的改进
XGBoost的缺点：
```
每次迭代训练时需要读取整个数据集，耗时耗内存；

使用Basic Exact Greedy Algorithm计算最佳分裂节点时需要预先将特征的取值进行排序，
排序之后为了保存排序的结果，费时又费内存；

计算分裂节点时需要遍历每一个候选节点，然后计算分裂之后的信息增益，费时；

生成决策树是level-wise级别的，也就是预先设置好树的深度之后，每一颗树都需要生长到设置的那个深度，
这样有些树在某一次分裂之后效果甚至没有提升但仍然会继续划分树枝，然后再次划分....之后就是无用功了，耗时。
```
为了避免上述XGB的缺陷，并且能够在不损害准确率的条件下加快GBDT模型的训练速度，lightGBM在传统的GBDT算法上加了两个技术：
```
单边梯度采样 Gradient-based One-Side Sampling (GOSS)；

互斥稀疏特征绑定 Exclusive Feature Bundling (EFB)
```
使用**GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGB遍历所有特征值节省了不少开销**；

**使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的**。
### 2.XGB和LGB区别：
```
1、直方图优化，对连续特征进行分桶，在损失了一定精度的情况下大大提升了运行速度，并且在gbm的框架下，基学习器的“不精确”分箱反而增强了整体的泛化性能；

2、goss 树的引入；

3、efb，对稀疏特征做了“捆绑”的优化功能；

4、直接支持对于类别特征进行训练（实际上内部是对类别特征做了类似编码的操作了）

5、树的生长方式由level-wise变成leaf-wise；
```

&nbsp;
## reference
[快的不要不要的lightGBM](https://zhuanlan.zhihu.com/p/31986189)   
[gbdt、xgb、lgb、cat面经整理——from牛客](https://zhuanlan.zhihu.com/p/82521899)
