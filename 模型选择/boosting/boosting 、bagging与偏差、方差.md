### 1.随机森林的树深度往往大于 GBDT 的树深度？
就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias）和方差（variance）。    
偏差指的是算法的**期望预测与真实预测之间的偏差程度**，反应了模型本身的拟合能力；   
方差度量了同等大小的训练集的变动导致学习性能的变化，**刻画了数据扰动**所导致的影响。

当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。

当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。

对于 Bagging 算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差（variance），因为采用了相互独立的基分类器多了以后，h 的值自然就会靠近。  
**所以对于每个基分类器来说，目标就是如何降低这个偏差（bias），所以我们会采用深度很深甚至不剪枝的决策树**。

对于 Boosting 来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias），   
所以对于每个基分类器来说，**问题就在于如何选择 variance 更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树**。

### 2.偏差方差图
[偏差（Bias）与方差（Variance）](https://zhuanlan.zhihu.com/p/38853908)
