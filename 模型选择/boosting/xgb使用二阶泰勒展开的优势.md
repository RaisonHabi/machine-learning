因为这样做使得我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。

实际上使用二阶泰勒展开是为了xgboost能够【自定义loss function】，如果按照最小二乘法的损失函数直接推导，同样能够得到陈佬最终的推导式子：  

二阶泰勒展开实际不是 ～ 最小二乘法，平方损失函数的二阶泰勒展开=最小二乘法。   
但陈佬为何想用二阶泰勒展开呢，我猜是为了xgboost库的可扩展性，因为任何损失函数只要二阶可导即能【复用】陈佬所做的关于最小二乘法的任何推导。   
而且泰勒的本质是尽量去模仿一个函数，我猜二阶泰勒展开已经足以近似大量损失函数了，典型的还有基于分类的对数似然损失函数。   
嘿，这样同一套代码就能完成回归或者分类了，而不是每次都推导一番，重写训练代码。



（一阶、二阶函数，如sgd、牛顿法）
## reference
[xgboost是用二阶泰勒展开的优势在哪？ - 宋佳慧的回答 - 知乎](https://www.zhihu.com/question/61374305/answer/393060885)
