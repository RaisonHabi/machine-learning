## 一、模型融合
在机器学习训练完模型之后我们要考虑模型的效率问题，常用的模型效率分析手段有：
```
1.研究模型学习曲线，判断模型是否过拟合或者欠拟合，并做出相应的调整；
2.对于模型权重参数进行分析，对于权重绝对值高/低的特征，可以对特征进行更细化的工作，也可以进行特征组合；
3.进行bad-case分析，对错误的例子分析是否还有什么可以修改挖掘
4.模型融合：模型融合就是训练多个模型，然后按照一定的方法集成过个模型，应为它容易理解、实现也简单，
    同时效果也很好，在工业界的很多应用，在天池、kaggle比赛中也经常拿来做模型集成。
```
### 1、模型融合的认识、分析：
**模型融合的概念**： 下图介绍了模型融合的一般结构:先产生一组”个体学习器 ，再用某种策略将它们结合起来，加强模型效果。
### 2、模型融合应用广发的原因：
可以通过数学证明模型，**随着集成中个体分类器数目T 的增大，集成的错误率将指数级下降，最终趋向于零**。具体证明在周志华和李航老师的书中都有。
### 3、模型融合的条件：
个体学习器准确性越高、多样性越大，则融合越好。
```
Base Model 之间的相关性要尽可能的小。
这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。
Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。

Base Model 之间的性能表现不能差距太大。
这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。
但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。
```
### 4、模型融合的分类： 
按照个体学习器的关系可以分为两类:
```
个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表Boosting方法；
个体学习器间不存在强依赖关系、可同时生成的并行化方法，代表是Bagging 和”随机森林” 。
```

&nbsp;
## 二、模型融合策略 
### 1、投票法/Voting
模型融合其实也没有想象的那么高大上，从最简单的Voting说起，这也可以说是一种模型融合。  
**假设对于一个二分类问题，有3个基础模型，那么就采取投票制的方法，投票多者确定为最终的分类。**

有绝对多数投票（得票超过一半）、相对多数投票（得票最多）、加权投票。这个也好理解，一般用于分类模型。在bagging模型中使用。

&nbsp;
### 2、平均法/Averaging
对于回归问题，一个简单直接的思路是取平均。  
**稍稍改进的方法是进行加权平均**。  
权值可以用排序的方法确定，举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6  
这两种方法看似简单，其实后面的高级算法也可以说是基于此而产生的，Bagging或者Boosting都是一种把许多弱分类器这样融合成强分类器的思想。

平均法有一般的评价和加权平均，这个好理解。对于平均法来说一般用于回归预测模型中，在Boosting系列融合模型中，一般采用的是加权平均融合。

&nbsp;
### 3、学习法：
一种更为强大的结合策略是使用”学习法”，即通过另一个学习器来进行结合，把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。常见的有Stacking和Blending两种

#### 3.1、Stacking方法： 
Stacking 先从初始数据集训练出初级学习器，然后”生成”一个新数据集用于训练次级学习器。  
**在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记**。  
stacking一般使用交叉验证的方式，初始训练集D 被随机划分为k 个大小相似的集合D1 ， D2 ， … ， Dk，每次用k-1个部分训练T个模型，对另个一个部分产生T个预测值作为特征，遍历每一折后，也就得到了新的特征集合，标记还是源数据的标记，用新的特征集合训练一个集合模型。
#### 3.2、Blending方法： 
Blending与Stacking大致相同，只是Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，  
**而是建立一个Holdout集，例如说10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合**。  
说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。

### 4、多样性增强： 
模型的多样性可以改善融合后的效果，增强的手段有：
```
数据样本的扰动：bagging中提到的给定初始数据集， 可从中产生出不同的数据子集， 再利用不同的数据子集训练出不同的个体学习器。
输入属性的扰动：随机森林提到的使用控制属性的子空间的不同，产生差异较大的模型。
输出表示的扰动：输出表示进行操纵以增强多样性，可对训练样本的类标记稍作变动。
算法参数的扰动：基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等，通过随机设置不同的参数，往往可产生差别较大的个体学习器。
```

### 5、bagging、boosting的对比： 
Bagging主要在优化variance（即模型的鲁棒性），boosting主要在优化bias（即模型的精确性）    
[为什么说bagging是减少variance，而boosting是减少bias? - 知乎用户的回答 - 知乎](https://www.zhihu.com/question/26760839/answer/40337791)  
Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance（事实上，各模型的分布也近似相同，但不独立）。  
由于（公式），所以bagging后的bias和单个子模型的接近，一般来说不能显著降低bias。另一方面，若各子模型独立，则有（公式），此时可以显著降低variance。若各子模型完全相同，则（公式），此时不会降低variance。  
bagging方法得到的各子模型是有一定相关性的，属于上面两个极端状况的中间态，因此可以一定程度降低variance。为了进一步降低variance，Random forest通过随机选取变量子集做拟合的方式de-correlated了各子模型（树），使得variance进一步降低。


#### 5.1、bagging： 
Bagging 是 Bootstrap Aggregating 的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance。

#### 5.2、Boosting： 
boosting从优化角度来看，是用forward-stagewise这种贪心法去最小化损失函数（指数函数），**boosting是在sequential地最小化损失函数，其bias自然逐步下降**。  
**但由于是采取这种sequential、adaptive的策略，各子模型之间是强相关的，于是子模型之和并不能显著降低variance。所以说boosting主要还是靠降低bias来提升预测精度**。

————————————————  
原文链接：https://blog.csdn.net/u014248127/java/article/details/78993753

&nbsp;
## 三、其他
### Bagging
**Bagging就是采用 *有放回的方式进行抽样*， 用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合**。  
大概分为这样两步：  
> **1.重复K次**  
有放回地重复抽样建模;    
训练子模型  
**2.模型融合**  
分类问题：voting;    
回归问题：average

Bagging算法不用我们自己实现，随机森林就是基于Bagging算法的一个典型例子，采用的基分类器是决策树。R和python都集成好了，直接调用。

&nbsp;
### Boosting
***Bagging算法可以并行处理***，**而Boosting的思想是一种迭代的方法，每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例**。  
**最终将这些弱分类器进行加权相加**。引用加州大学欧文分校Alex Ihler教授的两页PPT  

同样地，基于Boosting思想的有AdaBoost、GBDT等，在R和python也都是集成好了直接调用。  

PS：理解了这两点，面试的时候关于Bagging、Boosting的区别就可以说上来一些，问Randomfroest和AdaBoost的区别也可以从这方面入手回答。  
也算是留一个小问题，随机森林、Adaboost、GBDT、XGBoost的区别是什么？

&nbsp;
### Stacking
Stacking模型本质上是一种分层的结构，这里简单起见，只分析二级Stacking.假设我们有3个基模型M1、M2、M3。(详见参考文档)    

Stacking本质上就是这么直接的思路，但是这样肯定是不行的，问题在于P1的得到是有问题的，  
***用整个训练集训练的模型反过来去预测训练集的标签，毫无疑问过拟合是非常非常严重的,***        
**因此现在的问题变成了如何在解决过拟合的前提下得到P1、P2、P3，这就变成了熟悉的节奏——K折交叉验证。** 

**[详见下面两个英文文档的图，清晰解释了k折交叉在stacking的应用]**     

#### StackingClassifier
An ensemble-learning meta-classifier for stacking.  
***from mlxtend.classifier import StackingClassifier***  

**Overview**  
> Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier.   
The individual classification models are trained based on the complete training set;  
then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble.   
The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.

***Please note that this type of Stacking is prone to overfitting due to information leakage.***    
The related StackingCVClassifier.md does not derive the predictions for the 2nd-level classifier from the same datast that was used for training the level-1 classifiers and is recommended instead.

#### StackingCVClassifier
An ensemble-learning meta-classifier for stacking **using cross-validation to prepare the inputs for the level-2 classifier to prevent overfitting.**  
***from mlxtend.classifier import StackingCVClassifier***  


&nbsp;
## reference  
[机器学习模型优化之模型融合](https://blog.csdn.net/u014248127/article/details/78993753).  
[【机器学习】模型融合方法概述](https://zhuanlan.zhihu.com/p/25836678)  
[集成学习总结 & Stacking方法详解](https://blog.csdn.net/willduan1/article/details/73618677)  
[StackingClassifier](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)  
[StackingCVClassifier](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)
