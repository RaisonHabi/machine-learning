## 分类与回归区别
### 分类：离散值、定性；回归：连续值、定量

&nbsp;
<p><b><i>分类模型和回归模型本质一样，分类模型可将回归模型的输出离散化（下面例子1. 2. 4. 5.），回归模型也可将分类模型的输出连续化（下面例子3.）</i></b></p><p>举几个例子:</p><ol><li>Logistic Regression 和 Linear Regression：</li><ol><li><b>Linear Regression</b>： 输出一个标量 wx+b，这个值是连续值，所以可以用来处理<b>回归</b>问题</li><li><b>Logistic Regression</b>：把上面的 wx+b 通过 sigmoid 函数映射到(0,1)上，并划分一个阈值，大于阈值的分为一类，小于等于分为另一类，可以用来处理<b>二分类</b>问题</li><li>更进一步：对于N分类问题，则是先得到N组w值不同的 wx+b，然后归一化，比如用 softmax 函数，最后变成N个类上的概率，可以处理<b>多分类</b>问题</li></ol><li>Support Vector Regression 和 Support Vector Machine:</li><ol><li><b>SVR</b>：输出 wx+b，即某个样本点到分类面的距离，是连续值，所以是<b>回归</b>模型</li><li><b>SVM</b>：把这个距离用 sign(·) 函数作用，距离为正(在超平面一侧)的样本点是一类，为负的是另一类，所以是<b>分类</b>模型</li></ol><li><b>Naive Bayes</b> 用于分类 和 回归:</li><ol><li>用于分类：y是离散的类别，所以得到离散的 p(y|x)，给定 x ，输出每个类上的概率</li><li>用于回归：对上面离散的 p(y|x)求期望 ΣyP(y|x)，就得到连续值。但因为此时y本身是连续的值，所以最地道的做法是，得到连续的概率密度函数p(y|x)，然后再对y求期望。参考 <a href="https://link.zhihu.com/?target=http%3A//www.cs.waikato.ac.nz/~eibe/pubs/nbr.pdf" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://www.</span><span class="visible">cs.waikato.ac.nz/~eibe/</span><span class="invisible">pubs/nbr.pdf</span><span class="ellipsis"></span></a> </li></ol><li><b>前馈神经网络(如 CNN 系列) </b>用于 分类 和 回归:</li><ol><li>用于回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出可以看做向量 v，现全部连到一个神经元上，则这个神经元输出 wv+b，是一个连续值，可以处理回归问题，跟上面 Linear Regression 思想一样</li><li>用于N分类：现在这m个神经元最后连接到 N 个神经元，就有 N 组w值不同的 wv+b，同理可以归一化（比如用 softmax ）变成 N个类上的概率（补充一下，如果不用 softmax，而是每个 wx+b 用一个 sigmoid，就变成<b>多标签</b>问题，跟多分类的区别在于，样本可以被打上多个标签）</li></ol><li><b>循环神经网络(如 RNN 系列)</b> 用于分类 和 回归：</li><ol><li>用于回归 和 分类： 跟 CNN 类似，输出层的值 y = wv+b，可做分类可做回归，只不过区别在于，RNN 的输出跟时间有关，即输出的是 {y(t), y(t+1),...}序列（关于时间序列，见下面的更新）</li></ol></ol><p>上面的例子其实都是从 prediction 的角度举例的，如果从 training 角度来看，分类模型和回归模型的目标函数不同，分类常见的是 log loss, hinge loss, 而回归是 square loss（关于 loss function，又是另一个story了，在此不展开了）</p><br/><p>==== 进一步思考后的<b>重要更新，谈谈时间序列模型 </b>========</p><p>上面的例子 1~4 解决的是常见的分类/回归问题，而例5 解决的是 时间序列问题。</p><ol><li>上面例1~4 的模型只适用于：这些样本的 y，没有时间上的相关性，比如：</li><ol><li>人脸识别（分类问题），输入 x 是人脸的图像矩阵，识别目标 y 是人的ID，离散值，显然人与人的ID没有时间上的关系</li><li>人脸年龄预测（回归问题），输入 x 还是人脸图像矩阵，识别目标 y 是人的年龄，连续值，显然人与人之间的年龄亦没有时间上的关系</li></ol><li>而当这些样本的 y 在时间上有相关性时，就变成了 时间序列问题，如果我们依然用非时间序列的方法来处理，就割裂了y的时间相关性，所以常见手段是用例5提到的RNN，（当然，还有 HMM, CRF 这些）但注意别用统计学里面那些愚蠢的 AR 模型（参考我的回答 <a href="https://www.zhihu.com/question/31833683/answer/152064596" class="internal">时间序列建模问题，如何准确的建立时间序列模型？ - 知乎用户的回答 - 知乎</a>）。应用场景：</li><ol><li>NLP 里的命名体识别（分类问题），输入是一句话，可以看做是由单词组成的时间序列（准确说是: 事件序列），输出是每个单词所属的标签</li><li>气温预测（回归问题），输入是历史时间的气温记录，输出是未来1天或多天的气温</li></ol></ol><p>总结一下，我认为，机器学习模型(有监督)本质是：</p><blockquote>对一系列样本 <img src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" alt="(x,y)" eeimg="1"/> 构建 <img src="https://www.zhihu.com/equation?tex=f%28x%29%5Crightarrow+y" alt="f(x)\rightarrow y" eeimg="1"/>  的映射</blockquote><p>所以，对于时间序列问题，其实是构建一个 <img src="https://www.zhihu.com/equation?tex=f%28x_t%2Cx_%7Bt%2B1%7D%2C...%2Cx_%7Bt%2Bdt%7D%29%5Crightarrow+y_%7Bt%2B1%7D%2C...%2Cy_%7Bt%2Bdt%2B1%7D" alt="f(x_t,x_{t+1},...,x_{t+dt})\rightarrow y_{t+1},...,y_{t+dt+1}" eeimg="1"/> 的映射关系</p></span></div>

&nbsp;
## reference
[分类与回归区别是什么？](https://www.zhihu.com/question/21329754)
