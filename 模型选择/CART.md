xgboost如何选取根节点（最相关特征？）、如何生成树   
xgboost对应的模型：一堆CART树。   
对于分类问题，由于CART树的叶子节点对应的值是一个实际的分数，而非一个确定的类别，这将有利于实现高效的优化算法。    
**分裂指标（分类树：Gini指数）：在CART分割时,按照Gini指数最小来确定分割点的位置**. 

CART算法由以下两步组成：   
1.决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；    
2.决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。    
CART决策树的生成就是递归地构建二叉决策树的过程。CART决策树既可以用于分类也可以用于回归。本文我们仅讨论用于分类的CART。对分类树而言，CART用Gini系数最小化准则来进行特征选择，生成二叉树。

### CART
CART本质是对特征空间进行二元划分(即CART生成的决策树是一棵二叉树),它能够对类别变量与连续变量进行分裂,大体的分割思路是：
```
先对某一维数据进行排序(这也是为什么我们对无序的类别变量进行编码的原因），
然后对已经排好后的特征进行切分,切分的方法就是if ... else ...的格式.

然后计算衡量指标(分类树用Gini指数,回归树用最小平方值),最终通过指标的计算确定最后的划分点,然后按照下面的规则生成左右子树：
If x < A: Then go to left; else: go to right.
```
分裂指标（分类树：Gini指数）

对于给定的样本集D的Gini指数: 分类问题中,假设有[公式]个类,样本点属于第[公式]类的概率为[公式],则概率分布的基尼指数为:[公式] .
```
①.从 [公式] 指数的数学形式中,我们可以很容易的发现,当 [公式] 的时候 [公式] 指数是最大的,
这个时候分到每个类的概率是一样的,判别性极低,对我们分类带来的帮助很小,可以忽略.

②.当某些 [公式] 较大,即第 [公式] 类的概率较大,此时我们的 [公式] 指数会变小意味着判别性较高.样本中的类别不平衡.
```
在给定特征A的条件下,样本集合D的基尼指数(在CART中)为:

#### 在CART分割时,我们按照 Gini 指数最小来确定分割点的位置.


[深入浅出XGBoost - 和鲸社区Kesci的文章 - 知乎](https://zhuanlan.zhihu.com/p/52211079)
