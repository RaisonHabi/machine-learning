## 特征离散化优点
在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化后交给逻辑回归模型，这样做的优势有以下几点：
```
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。

2. 离散化后的特征对异常数据有很强的鲁棒性，不易受噪声的影响：比如一个特征是年龄>30是1，否则0。
如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。

3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，
相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。

4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。

5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。
当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。

6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
```
**模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡**。  
既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

&nbsp;
## else
主要目的是获得指数级的表示能力。假如一个n维的连续向量，即使采用最简单的每一个维2值化，也会得到2^n种特征组合。

这种表示方法对LR这种线性分类器是十分关键的。在超高维的特征空间中，很多问题就变为线性可分问题，而从可以极大地提高分类器的能力。

但对于像神经网络的深度非线性模型则意义不大。因为神经网络中的每个神经元都可以看作是一个近似的离散特征生成器，输出为两种状态：兴奋和抑制。本身就可以进行表示学习，所以就不再需要特征工程了。

神经元采样的非线性激活函数，比如logistic sigmoid函数，可以看作是近似的特征离散化操作。

&nbsp;
## reference
[特征离散化（分箱）综述](https://zhuanlan.zhihu.com/p/68865422).   
[逻辑回归LR的特征为什么要先离散化](https://blog.csdn.net/yang090510118/article/details/39478033)   
[连续特征的离散化：在什么情况下将连续的特征离散化之后可以获得更好的效果？](https://www.zhihu.com/question/31989952)
