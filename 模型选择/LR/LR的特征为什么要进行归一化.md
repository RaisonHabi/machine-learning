## 一、归一化
### 1.归一化为什么能提高梯度下降法求解最优解的速度？
#### 解释一：
使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；

而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。

因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。

#### 解释二：
[神经网络之激活函数_Sigmoid和Tanh探索](https://zhuanlan.zhihu.com/p/99937331)   
这里就会看到问题，**当Sigmoid和Tanh因变量过大时，所有过大的因变量落在函数饱和区间内，函数对因变量的收敛几乎不敏感，这就是为什么在做逻辑回归的时候要对数据先做 Normalize 处理的原因**，机器学习教程里很少提到这个点。

### 2.归一化有可能提高精度
一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。  
如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。

&nbsp;
## 二、归一化的类型
### 1.连续特征
**z-score标准化(标准差标准化)**：  
这是最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做 z-score标准化。  
**具体的方法是求出样本特征x的均值mean和标准差std，然后用（x-mean)/std来代替原特征**。  
这样特征就变成了均值为0，方差为1了。在sklearn中，我们可以用StandardScaler来做z-score标准化。  
当然，如果我们是用pandas做数据预处理，可以自己在数据框里面减去均值，再除以方差，自己做z-score标准化。

**max-min标准化（线性归一化）**：  
也称为离差标准化，预处理后使特征值映射到[0,1]之间。  
具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征。  
如果我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单。用(x-min)(b-a)/(max-min)+a来代替原特征即可。  
在sklearn中，我们可以用MinMaxScaler来做max-min标准化。  
这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。  
所以实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。  
这种归一化方法比较适用在数值比较集中的情况。  
这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。

**L1/L2范数标准化**：  
如果我们只是为了统一量纲，那么通过L2范数整体标准化也是可以的，  
**具体方法是求出每个样本特征向量x的L2范数||x||2,然后用x/||x||2代替原样本特征即可。当然L1范数标准化也是可以的，即用x/||x||1代替原样本特征**。  
通常情况下，范数标准化首选L2范数标准化。在sklearn中，我们可以用Normalizer来做L1/L2范数标准化。

### 2.离散特征
对于离散的特征基本就是按照one-hot编码，该离散特征有多少取值，就用多少维来表示该特征。

a)使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。

b)将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。

c)将离散型特征使用one-hot编码，确实会让特征之间的距离计算更加合理。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x1=(1),x2=(2),x3=(3)。两个工作之间的距离是，(x1,x2)=1,d(x2,x3)=1,d(x1,x3)=2。那么x1和x3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x1=(1,0,0),x2=(0,1,0),x3=(0,0,1)x1=(1,0,0),x2=(0,1,0),x3=(0,0,1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理

**d)离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征**。  
就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1

### 3.决策树可以不做标准化和归一化,当然使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进
虽然大部分机器学习模型都需要做标准化和归一化，  
也有不少模型可以不做做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林,bagging和boosting等。**当然此时使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进**。  
基于参数的模型或基于距离的模型，都要进行标准化和归一化

&nbsp;
## reference
[LR为什么要进行归一化](https://blog.csdn.net/weixin_38111819/article/details/79729444)  
[特征的标准化（归一化）](https://blog.csdn.net/u014135752/article/details/80789251)
