##  三种方法实现逻辑回归多分类
### 1、One-Vs-All
One-Vs-All（或者叫 One-Vs-Rest,OvR）的思想是把一个多分类的问题变成多个二分类的问题。转变的思路就如同方法名称描述的那样，选择其中一个类别为正类（Positive），使其他所有类别为负类（Negative）

One-Vs-All 最为一种常用的二分类拓展方法，其优缺点也十分明显。
```
优点：普适性还比较广，可以应用于能输出值或者概率的分类器，同时效率相对较好，有多少个类别就训练多少个分类器。

缺点：很容易造成训练集样本数量的不平衡（Unbalance），尤其在类别较多的情况下，
经常容易出现正类样本的数量远远不及负类样本的数量，这样就会造成分类器的偏向性。
```
### 2、One-Vs-One
相比于 One-Vs-All 由于样本数量可能的偏向性带来的不稳定性，One-Vs-One(OvO) 是一种相对稳健的扩展方法。对于同样的三分类问题，我们像举行车轮作战一样让不同类别的数据两两组合训练分类器，可以得到 3 个二元分类器。

当然 One-Vs-One 的优点也很明显，它在一定程度上规避了数据集 unbalance 的情况，性能相对稳定，并且需要训练的模型数虽然增多，但是每次训练时训练集的数量都降低很多，其训练效率会提高。
### 3、Softmax
在二元的逻辑回归模型中，我们用 **Sigmoid 函数将一个多维数据（一个样本）映射到一个 0 - 1 之间的数值上，有没有什么方法从数学上让一个样本映射到多个 0 - 1 之间的数值呢？答案是通过 Softmax 函数**。

在处理一些样本可能丛属多个类别的分类问题是，使用 one vs one 或 one vs all 有可能达到更好的效果。  
Softmax 回归适合处理一个样本尽可能属于一种类别的多分类问题。
### 总结
如果类别之间是互斥的，那么用softmax会比较合适，如果类别之间不是互斥的，用ovr比较合适。

对于一个多分类的问题，是直接选择多分类器直接计算还是选择多个二分类器进行计算取决于问题中类别之间是否互斥。
```
是互斥的 –> 多分类器或者多个二分类器
不是互斥的 –> 多个二分类器
```


&nbsp;
## reference
[3 种方法实现逻辑回归多分类](https://zhuanlan.zhihu.com/p/46599015)  
[SVM、LR如何做多分类](https://blog.csdn.net/csdn_lzw/article/details/80170178)
