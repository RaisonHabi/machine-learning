## 一、逻辑回归如何处理非线性可分任务？
1.核逻辑回归

2.多个逻辑回归模型组合（神经网络）

&nbsp;
## 二、LR可以用核。加l2正则项，和svm类似，加l2正则项可以用核方便处理
1.将软间隔支持向量机看做正则化模型   
[【机器学习基础】核逻辑回归](https://blog.csdn.net/JasonDing1354/article/details/45207023)

2.从软间隔SVM到正则化   
[机器学习之核函数逻辑回归（机器学习技法）](https://blog.csdn.net/qq_34993631/article/details/79345889)

&nbsp;
## 三、逻辑回归与其他模型
[逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降）](https://blog.csdn.net/kejiaming/article/details/64439664)
### 1.逻辑回归与最大熵
**最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归**。  
为了证明最大熵模型跟逻辑回归的关系，那么就要证明两者求出来的模型是一样的，即求出来的h(x)的形式应该是一致的。由于最大熵是通过将有约束条件的条件极值问题转变成拉格朗日对偶问题来求解，模型的熵为

### 2.逻辑回归与svm
相同点:
```
都是分类算法
都是监督学习算法
都是判别模型
都能通过核函数方法针对非线性情况分类
目标都是找一个分类超平面
都能减少离群点的影响
```

不同点:
```
损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss

逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。
这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。

逻辑回归对概率建模，svm对分类超平面建模

逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有

逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响

逻辑回归是统计方法，svm是几何方法
```
### 3.逻辑回归与朴素贝叶斯
这两个算法有一些相似之处，并且在对比判别模型和生成模型，它们作为典型的分类算法经常被提及，因此这里也做一个小小的总结。

相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，**当假设朴素贝叶斯的条件概率服从高斯分布时Gaussian Naive Bayes，它计算出来的形式跟逻辑回归是一样的**[18]。

不同的地方在于，**逻辑回归为判别模型求的是，朴素贝叶斯为生成模型求的是。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好**。   
由于朴素贝叶斯假设了条件概率是条件独立的，也就是每个特征权重是独立的，**如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好**。
