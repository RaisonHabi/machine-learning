## 线性回归、逻辑回归
线性回归（linear regression）：  
将输入项分别乘以一些常量，再将结果加起来得到输出。   
如果数据的特征比样本点还多，也就是说输入数据的矩阵X不是满秩矩阵，非满秩矩阵在求逆时会出现问题。为了解决这个问题，可以使用岭回归（ridge regression）、lasso法、前向逐步回归。 

“逻辑回归假设数据服从伯努利分布（EX= p,DX=p(1-p)，零一分布、两点分布），通过极大似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的”。

&nbsp;
## 在逻辑回归模型中，我们最大化似然函数和最小化损失函数实际上是等价的
### 1.解释1
[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)

<p>逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。</p><p>设： </p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P%28Y%3D1%7Cx%29+%26%3D+p%28x%29+%5C%5C++P%28Y%3D0%7Cx%29+%26%3D+1-+p%28x%29+%5Cend%7Baligned%7D%5C%5C" alt="\begin{aligned} P(Y=1|x) &amp;= p(x) \\  P(Y=0|x) &amp;= 1- p(x) \end{aligned}\\" eeimg="1"/> </p><p>似然函数： </p><p><img src="https://www.zhihu.com/equation?tex=L%28w%29%3D%5Cprod%5Bp%28x_%7Bi%7D%29%5D%5E%7By_%7Bi%7D%7D%5B1-p%28x_%7Bi%7D%29%5D%5E%7B1-y_%7Bi%7D%7D++%5C%5C" alt="L(w)=\prod[p(x_{i})]^{y_{i}}[1-p(x_{i})]^{1-y_{i}}  \\" eeimg="1"/> </p><p>为了更方便求解，我们对等式两边同取对数，写成对数似然函数： </p><p><img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Baligned%7D+L%28w%29%26%3D%5Csum%5By_%7Bi%7Dlnp%28x_%7Bi%7D%29%2B%281-y_%7Bi%7D%29ln%281-p%28x_%7Bi%7D%29%29%5D+%5C%5C+%26%3D%5Csum%5By_%7Bi%7Dln%5Cfrac%7Bp%28x_%7Bi%7D%29%7D%7B1-p%28x_%7Bi%7D%29%7D%2Bln%281-p%28x_%7Bi%7D%29%29%5D++%5C%5C+%26%3D%5Csum%5By_%7Bi%7D%28w+%5Ccdot+x_%7Bi%7D%29+-+ln%281%2Be%5E%7Bw+%5Ccdot+x_%7Bi%7D%7D%29%5D+%5Cend%7Baligned%7D+%5C%5C" alt=" \begin{aligned} L(w)&amp;=\sum[y_{i}lnp(x_{i})+(1-y_{i})ln(1-p(x_{i}))] \\ &amp;=\sum[y_{i}ln\frac{p(x_{i})}{1-p(x_{i})}+ln(1-p(x_{i}))]  \\ &amp;=\sum[y_{i}(w \cdot x_{i}) - ln(1+e^{w \cdot x_{i}})] \end{aligned} \\" eeimg="1"/> </p><p>在机器学习中我们有损失函数的概念，其衡量的是模型预测错误的程度。如果取整个数据集上的平均对数似然损失，我们可以得到:  </p><p><img src="https://www.zhihu.com/equation?tex=J%28w%29%3D-%5Cfrac%7B1%7D%7BN%7DlnL%28w%29+%5C%5C" alt="J(w)=-\frac{1}{N}lnL(w) \\" eeimg="1"/> </p><p>即在逻辑回归模型中，我们<b>最大化似然函数</b>和<b>最小化损失函数</b>实际上是等价的。</p>

### 2.解释2
损失函数表征预测值与真实值之间的差异程度，如果预测值与真实值越接近则损失函数应该越小。  
在此损失函数可以取为最大似然估计函数的相反数，其次除以m这一因子并不改变最终求导极值结果，通过除以m可以得到平均损失值，避免样本数量对于损失值的影响。

[机器学习--LR逻辑回归与损失函数理解](https://blog.csdn.net/u014106644/article/details/83660226)

&nbsp;
## 逻辑回归

<p><b>一、线性回归（Linear Regression）</b></p><p>线性回归的表达式：</p><p><img src="https://www.zhihu.com/equation?tex=f%28%5Cbm%7Bx%7D%29+%3D+%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D+%2B+b" alt="f(\bm{x}) = \bm{w}^T\bm{x} + b" eeimg="1"/></p><p>线性回归对于给定的输入 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="\bm{x}" eeimg="1"/> ，输出的是一个数值 y ，因此它是一个解决回归问题的模型。</p><p>为了消除掉后面的常数项b，我们可以令 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bmatrix%7D+%5Cbm%7Bx%7D%5E%7B%27%7D+%26%3D+%5B+1+%26+%5Cbm%7Bx%7D%5D%5ET++%5Cend%7Bmatrix%7D" alt="\begin{matrix} \bm{x}^{&#39;} &amp;= [ 1 &amp; \bm{x}]^T  \end{matrix}" eeimg="1"/> ，同时 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bmatrix%7D+%5Cbm%7Bw%7D%5E%7B%27%7D+%26%3D+%5B+b+%26+%5Cbm%7Bw%7D%5D%5ET+%5Cend%7Bmatrix%7D" alt="\begin{matrix} \bm{w}^{&#39;} &amp;= [ b &amp; \bm{w}]^T \end{matrix}" eeimg="1"/> ，也就是说给x多加一项而且值恒为1，这样b就到了w里面去了，直线方程可以化简成为：</p><p><img src="https://www.zhihu.com/equation?tex=f%28%5Cbm%7Bx%27%7D%29+%3D+%5Cbm%7Bw%27%7D%5ET%5Cbm%7Bx%27%7D+" alt="f(\bm{x&#39;}) = \bm{w&#39;}^T\bm{x&#39;} " eeimg="1"/> </p><p>在接下来的文章中为了方便，我们所使用的 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%2C%5Cbm%7Bx%7D" alt="\bm{w},\bm{x}" eeimg="1"/> 其实指代的是 <img src="https://www.zhihu.com/equation?tex=w%27%2Cx%27" alt="w&#39;,x&#39;" eeimg="1"/> 。</p><p class="ztext-empty-paragraph"><br/></p>

<p><b>三、如何用连续的数值去预测离散的标签值呢？</b></p><p>线性回归的输出是一个数值，而不是一个标签，显然不能直接解决二分类问题。那我如何改进我们的回归模型来预测标签呢？</p><p>一个最直观的办法就是设定一个阈值，比如0，如果我们预测的数值 y &gt; 0 ，那么属于标签A，反之属于标签B，采用这种方法的模型又叫做<b>感知机</b>（Perceptron）。</p><p>另一种方法，我们不去直接预测标签，而是去预测标签为A概率，我们知道概率是一个[0,1]区间的连续数值，那我们的输出的数值就是标签为A的概率。一般的如果标签为A的概率大于0.5，我们就认为它是A类，否则就是B类。这就是我们的这次的主角<b>逻辑回归模型 </b>(Logistics Regression)。</p><p class="ztext-empty-paragraph"><br/></p>

<p><b>四、逻辑回归（logistics regression）</b></p><p>明确了预测目标是标签为A的概率。</p><p>我们知道，概率是属于[0,1]区间。但是线性模型 <img src="https://www.zhihu.com/equation?tex=f%28%5Cbm%7Bx%7D%29+%3D+%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D" alt="f(\bm{x}) = \bm{w}^T\bm{x}" eeimg="1"/> 值域是 <img src="https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C%5Cinfty%29" alt="(-\infty,\infty)" eeimg="1"/> 。</p><p>我们不能直接基于线性模型建模。需要找到一个模型的值域刚好在[0,1]区间，同时要足够好用。</p><p>于是，选择了我们的sigmoid函数。</p><p>它的表达式为： <img src="https://www.zhihu.com/equation?tex=%5Csigma%28x%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" alt="\sigma(x) =\frac{1}{1+e^{-x}}" eeimg="1"/> 。</p><p>它的图像：</p><figure data-size="normal"><noscript><img src="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_b.jpg" data-size="normal" data-rawwidth="468" data-rawheight="167" class="origin_image zh-lightbox-thumb" width="468" data-original="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;468&#39; height=&#39;167&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="468" data-rawheight="167" class="origin_image zh-lightbox-thumb lazy" width="468" data-original="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_r.jpg" data-actualsrc="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_b.jpg"/><figcaption>sigmoid函数</figcaption></figure><p>这个函数的有很多非常好的性质，一会儿你就会感受到。但是我们不能直接拿了sigmoid函数就用，毕竟它连要训练的参数 w 都没得。</p><p>我们结合sigmoid函数，线性回归函数，把线性回归模型的输出作为sigmoid函数的输入。于是最后就变成了逻辑回归模型：</p><p><img src="https://www.zhihu.com/equation?tex=y%3D%5Csigma%28f%28%5Cbm%7Bx%7D%29%29+%3D+%5Csigma%28%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D" alt="y=\sigma(f(\bm{x})) = \sigma(\bm{w}^T\bm{x})=\frac{1}{1+e^{-\bm{w}^T\bm{x}}}" eeimg="1"/> </p><p>假设我们已经训练好了一组权值 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5ET" alt="\bm{w}^T" eeimg="1"/> 。只要把我们需要预测的 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="\bm{x}" eeimg="1"/> 代入到上面的方程，输出的y值就是这个标签为A的概率，我们就能够判断输入数据是属于哪个类别。</p><p>接下来就来详细介绍，如何利用一组采集到的真实样本，训练出参数w的值。</p><p class="ztext-empty-paragraph"><br/></p>

<p><b>五、逻辑回归的损失函数（Loss Function）</b></p><p>损失函数就是用来衡量模型的输出与真实输出的差别。</p><p>假设只有两个标签1和0， <img src="https://www.zhihu.com/equation?tex=y_n+%5Cin%5C%7B0%2C+1%5C%7D" alt="y_n \in\{0, 1\}" eeimg="1"/> 。我们把采集到的任何一组样本看做一个事件的话，那么这个事件发生的概率假设为p。我们的模型y的值等于标签为1的概率也就是p。</p><p><img src="https://www.zhihu.com/equation?tex=P_%7By%3D1%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%3D+p" alt="P_{y=1}=\frac{1}{1+e^{-\bm{w}^T\bm{x}}} = p" eeimg="1"/> </p><p>因为标签不是1就是0，因此标签为0的概率就是： <img src="https://www.zhihu.com/equation?tex=P_%7By%3D0%7D+%3D+1-p" alt="P_{y=0} = 1-p" eeimg="1"/> 。</p><p>我们把单个样本看做一个事件，那么这个事件发生的概率就是：</p><p><img src="https://www.zhihu.com/equation?tex=P%28y%7C%5Cbm%7Bx%7D%29%3D%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+p%2C+y%3D1+%5C%5C+1-p%2Cy%3D0+%5Cend%7Baligned%7D+%5Cright." alt="P(y|\bm{x})=\left\{ \begin{aligned} p, y=1 \\ 1-p,y=0 \end{aligned} \right." eeimg="1"/> </p><p>这个函数不方便计算，它等价于:</p><p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7C%5Cbm%7Bx%7D_i%29+%3D+p%5E%7By_i%7D%281-p%29%5E%7B1-%7By_i%7D%7D" alt="P(y_i|\bm{x}_i) = p^{y_i}(1-p)^{1-{y_i}}" eeimg="1"/> 。</p><p>解释下这个函数的含义，我们采集到了一个样本 <img src="https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29" alt="(\bm{x_i},y_i)" eeimg="1"/> 。对这个样本，它的标签是 <img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i" eeimg="1"/> 的概率是 <img src="https://www.zhihu.com/equation?tex=p%5E%7By_i%7D%281-p%29%5E%7B1-%7By_i%7D%7D" alt="p^{y_i}(1-p)^{1-{y_i}}" eeimg="1"/> 。 （当y=1，结果是p；当y=0，结果是1-p）。</p><p>如果我们采集到了一组数据一共N个， <img src="https://www.zhihu.com/equation?tex=%5C%7B%28%5Cbm%7Bx%7D_1%2Cy_1%29%2C%28%5Cbm%7Bx%7D_2%2Cy_2%29%2C%28%5Cbm%7Bx%7D_3%2Cy_3%29...%28%5Cbm%7Bx%7D_N%2Cy_N%29%5C%7D" alt="\{(\bm{x}_1,y_1),(\bm{x}_2,y_2),(\bm{x}_3,y_3)...(\bm{x}_N,y_N)\}" eeimg="1"/> ，这个合成在一起的合事件发生的总概率怎么求呢？其实就是将每一个样本发生的概率相乘就可以了，即采集到这组样本的概率：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P_%7B%E6%80%BB%7D+%26%3D+P%28y_1%7C%5Cbm%7Bx%7D_1%29P%28y_2%7C%5Cbm%7Bx%7D_2%29P%28y_3%7C%5Cbm%7Bx%7D_3%29....P%28y_N%7C%5Cbm%7Bx%7D_N%29+%5C%5C++%26%3D+%5Cprod_%7Bn%3D1%7D%5E%7BN%7Dp%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D++%5Cend%7Baligned%7D" alt="\begin{aligned} P_{总} &amp;= P(y_1|\bm{x}_1)P(y_2|\bm{x}_2)P(y_3|\bm{x}_3)....P(y_N|\bm{x}_N) \\  &amp;= \prod_{n=1}^{N}p^{y_n}(1-p)^{1-y_n}  \end{aligned}" eeimg="1"/> </p><p>注意<img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB+%7D" alt="P_{总 }" eeimg="1"/> 是一个函数，并且未知的量只有 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> （在p里面）。</p><p>由于连乘很复杂，我们通过两边取对数来把连乘变成连加的形式，即：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+F%28%5Cbm%7Bw%7D%29%3Dln%28P_%7B%E6%80%BB%7D+%29++%26%3D+ln%28%5Cprod_%7Bn%3D1%7D%5E%7BN%7Dp%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D+%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7Dln+%28p%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%28y_n+ln+%28p%29+%2B+%281-y_n%29ln%281-p%29%29+%5Cend%7Baligned%7D+" alt="\begin{aligned} F(\bm{w})=ln(P_{总} )  &amp;= ln(\prod_{n=1}^{N}p^{y_n}(1-p)^{1-y_n} ) \\ &amp;= \sum_{n=1}^{N}ln (p^{y_n}(1-p)^{1-y_n}) \\ &amp;= \sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) \end{aligned} " eeimg="1"/> </p><p>其中， <img src="https://www.zhihu.com/equation?tex=p+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+" alt="p = \frac{1}{1+e^{-\bm{w}^T\bm{x}}} " eeimg="1"/> </p><p>这个函数 <img src="https://www.zhihu.com/equation?tex=F%28%5Cbm%7Bw%7D%29" alt="F(\bm{w})" eeimg="1"/>  又叫做它的<b>损失函数</b>。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。这里的损失函数的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号。</p><p class="ztext-empty-paragraph"><br/></p><p><b>六、最大似然估计MLE(Maximum Likelihood Estimation)</b></p><p>我们在真实世界中并不能直接看到概率是多少，我们只能观测到事件是否发生。也就是说，我们只能知道一个样本它实际的标签是1还是0。那么我们如何估计参数 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 跟b的值呢？</p><p>最大似然估计MLE(Maximum Likelihood Estimation)，就是一种估计参数 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的方法。在这里如何使用MLE来估计 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 呢？</p><p>在上一节，我们知道损失函数 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/> 是正比于总概率 <img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D" alt="P_{总}" eeimg="1"/> 的，而 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/> 又只有一个变量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 。也就是说，通过改变 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的值，就能得到不同的总概率值 <img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D" alt="P_{总}" eeimg="1"/> 。那么当我们选取的某个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> 刚好使得总概率 <img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D" alt="P_{总}" eeimg="1"/> 取得最大值的时候。我们就认为这个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> 就是我们要求得的 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的值，这就是最大似然估计的思想。</p><p>现在我们的问题变成了，找到一个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> ，使得我们的总事件发生的概率，即损失函数 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/> 取得最大值，这句话用数学语言表达就是：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D+%3D+arg%5Cmax_%7Bw%7DF%28%5Cbm%7Bw%7D%29+%3D+-arg%5Cmin_%7Bw%7DF%28%5Cbm%7Bw%7D%29+" alt="\bm{w^*} = arg\max_{w}F(\bm{w}) = -arg\min_{w}F(\bm{w}) " eeimg="1"/> </p><p class="ztext-empty-paragraph"><br/></p>



&nbsp;
## reference
[逻辑回归 logistics regression 公式推导](https://zhuanlan.zhihu.com/p/44591359)
