## xgb/随机森林
XGBoost 在原理方面的改进主要就是在损失函数上作文章。一是在原损失函数的基础上添加了正则化项产生了新的目标函数，这类似于对每棵树进行了剪枝并限制了叶结点上的分数来防止过拟合。二是对目标函数进行二阶泰勒展开，以类似牛顿法的方式来进行优化

 **XGBoost 采用的是后剪枝的策略，建每棵树的时候会一直分裂到指定的最大深度(max_depth)，然后递归地从叶结点向上进行剪枝，对之前每个分裂进行考察，如果该分裂之后的 𝐺𝑎𝑖𝑛⩽0，则咔咔掉**。
 
 传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的（自带交叉验证？）。 不过我并不觉得这是“纯粹”的后剪枝，因为一般还是要预先限制最大深度的呵呵。  
[集成学习之Boosting —— XGBoost](https://www.cnblogs.com/massquantity/p/9794480.html)  

#### 5.剪枝
**当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法**。  

**XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂**。

**这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂**。
#### 6.内置交叉验证
XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。  
而GBM使用网格搜索，只能检测有限个值。  
[机器学习算法梳理—XGBOOST](https://zhuanlan.zhihu.com/p/58221959)

&nbsp;

### 随机森林
通常情况下， 随机森林不需要后剪枝

**随机森林已经通过随机选择样本和特征，保证了随机性，不用后剪枝应该也能避免过拟合**

[随机森林是否需要后剪枝？sklearn为什么没有实现这个功能，是否有人实现了这个功能？](https://www.zhihu.com/question/59826974)

&nbsp;
## 前剪枝和后剪枝的比较
（1）前阈值的设定很敏感，一点点的变动，会引起整颗树非常大的变动，不好设定。  
（2）前剪枝生成比后剪枝简洁的树  
（3）一般用后剪得到的结果比较好  

&nbsp;

##  剪枝
### 决策树的过拟合问题
决策树是一种分类器，通过ID3，C4.5和CART等算法可以通过训练数据构建一个决策树。但是，算法生成的决策树非常详细并且庞大，每个属性都被详细地加以考虑，决策树的树叶节点所覆盖的训练样本都是“纯”的。  
因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现完好，误差率极低且能够正确得对训练样本集中的样本进行分类。训练样本中的错误数据也会被决策树学习，成为决策树的部分，但是对于测试数据的表现就没有想象的那么好，或者极差，这就是所谓的过拟合(Overfitting)问题。
### 决策树的剪枝
决策树的剪枝有两种思路：预剪枝（Pre-Pruning）和后剪枝（Post-Pruning）
#### 预剪枝（Pre-Pruning）
在构造决策树的同时进行剪枝。所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。
#### 后剪枝（Post-Pruning）
决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。后剪枝是目前最普遍的做法。
后剪枝的剪枝过程是删除一些子树，然后用其叶子节点代替，这个叶子节点所标识的类别通过大多数原则(majority class criterion)确定。所谓大多数原则，是指剪枝过程中, 将一些子树删除而用叶节点代替,这个叶节点所标识的类别用这棵子树中大多数训练样本所属的类别来标识,所标识的类 称为majority class ，（majority class 在很多英文文献中也多次出现）。
### 后剪枝算法
后剪枝算法有很多种，这里简要总结如下：
#### Reduced-Error Pruning (REP,错误率降低剪枝）
这个思路很直接，完全的决策树不是过度拟合么，我**再搞一个测试数据集来纠正它**。  
对于完全决策树中的每一个非叶子节点的子树，我们尝试着把它替换成一个叶子节点，该叶子节点的类别我们用子树所覆盖训练样本中存在最多的那个类来代替，这样就产生了一个简化决策树，然后比较这两个决策树在测试数据集中的表现，如果简化决策树在测试数据集中的错误比较少，那么该子树就可以替换成叶子节点。  
该算法以bottom-up的方式遍历所有的子树，直至没有任何子树可以替换使得测试数据集的表现得以改进时，算法就可以终止。
#### Pessimistic Error Pruning (PEP，悲观剪枝）
PEP剪枝算法是在C4.5决策树算法中提出的， 把一颗子树（具有多个叶子节点）用一个叶子节点来替代（我研究了很多文章貌似就是用子树的根来代替）的话，比起REP剪枝法，它**不需要一个单独的测试数据集**。   
详见参考文档
####  其他
```
Cost-Complexity Pruning(CCP，代价复杂度剪枝)
CART决策树算法中用的就是CCP剪枝方法。也是不需要额外的测试数据集。

Minimum Error Pruning(MEP)
Critical Value Pruning(CVP)
Optimal Pruning(OPP)
Cost-Sensitive Decision Tree Pruning(CSDTP)
```

&nbsp;
## reference
[集成学习之Boosting —— XGBoost](https://www.cnblogs.com/massquantity/p/9794480.html)  
[决策树的先剪枝和后剪枝](https://blog.csdn.net/t15600624671/article/details/78895267)  
[决策树的剪枝问题](https://www.jianshu.com/p/794d08199e5e)
