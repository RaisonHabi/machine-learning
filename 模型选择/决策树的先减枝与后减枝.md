## 
XGBoost 在原理方面的改进主要就是在损失函数上作文章。一是在原损失函数的基础上添加了正则化项产生了新的目标函数，这类似于对每棵树进行了剪枝并限制了叶结点上的分数来防止过拟合。二是对目标函数进行二阶泰勒展开，以类似牛顿法的方式来进行优化

 XGBoost 采用的是后剪枝的策略，建每棵树的时候会一直分裂到指定的最大深度(max_depth)，然后递归地从叶结点向上进行剪枝，对之前每个分裂进行考察，如果该分裂之后的 𝐺𝑎𝑖𝑛⩽0，则咔咔掉。
 
 传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的。 不过我并不觉得这是“纯粹”的后剪枝，因为一般还是要预先限制最大深度的呵呵。


&nbsp;
## reference
[集成学习之Boosting —— XGBoost](https://www.cnblogs.com/massquantity/p/9794480.html)
