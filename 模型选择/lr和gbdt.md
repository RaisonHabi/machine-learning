### 1. 从机器学习三要素的角度：
#### 1.1 模型
本质上来说，他们都是监督学习，判别模型，直接对数据的分布建模，不尝试挖据隐含变量，这些方面是大体相同的。但是又因为一个是线性模型，一个是非线性模型，因此其具体模型的结构导致了VC维的不同： 其中，Logistic Regression作为线性分类器，它的VC维是d+1，而 GBDT 作为boosting模型，可以无限分裂，具有无限逼近样本VC维的特点，因此其VC维远远大于d+1，这都是由于其线性分类器的特征决定的，归结起来，是Logistic Regression对数据线性可分的假设导致的
#### 1.2 策略
从 Loss(经验风险最小化) + 正则(结构风险最小化) 的框架开始说起；

从Loss的角度：
#### 1.3 算法
Logistic Regression 若采用 SGB, Momentum, SGD with Nesterov Acceleration 等算法，只用到了一阶导数信息，若用 AdaGrad, AdaDelta / RMSProp, Adam, Nadam, 牛顿法则用到了二阶导数信息，

而GBDT 直接拟合上一轮组合函数的特梯度，只用到了一阶倒数信息，XGBoost 则是用到了二阶导数信息。

&nbsp;
### 2. 从特征的角度：
#### 2.1 特征组合：
如前所说，GBDT 特征选择方法采用最小化均方损失来寻找分裂特征及对应分裂点，所以自动会在当前根据特征 A 分裂的子树下寻求其他能使负梯度最小的其他特征 B，这样就自动具备寻求好的特征组合的性能，因此也能给出哪些特征比较重要（根据该特征被选作分裂特征的次数）。

而 LR 只是一次性地寻求最大化熵的过程，对每一维的特征都假设独立，因此只具备对已有特征空间进行分割的能力，更不会对特征空间进行升维（特征组合）
#### 2.2 特征的稀疏性：
如前所述，Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，因此只具有线性分界面，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。

而对于 GBDT，其更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低纬稠密特征，也进一步引入了语意信息，有利于特征的表达。

&nbsp;
### 3. 数据假设不同：
Logistic Regression的数据分布假设：
```
噪声是高斯分布的
数据服从伯努利分布
特征独立
```
而 GBDT 并未对数据做出上述假设。

&nbsp;
## reference
[GBDT 与 LR 区别总结](https://zhuanlan.zhihu.com/p/60952744)
