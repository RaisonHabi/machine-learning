## 正则化（Regularization）
机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 ℓ1-norm 和 ℓ2-norm，中文称作 L1正则化 和 L2正则化，或者 L1范数 和 L2范数。  
L1正则化和L2正则化可以看做是损失函数的惩罚项。  
所谓『惩罚』是指对损失函数中的某些参数做一些限制。  
***对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）***。  
## L1正则化和L2正则化
L1正则化是指权值向量w中各个元素的**绝对值之和**，通常表示为∣∣w∣∣1  
L2正则化是指权值向量w中各个元素的**平方和然后再求平方根**（可以看到Ridge回归的L2正则化项有平方符号），通常表示为∣∣w∣∣2
## L1正则化和L2正则化的作用
L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择  
L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合  
## 稀疏模型与特征选择
上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？  
稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0.   
通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。  
在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。  
这就是稀疏模型与特征选择的关系。
## L1和L2正则化的直观理解
***考虑二维的情况，即只有两个权值w1 和w2***    
**图解详见参考文档**  
图中等值线是J0 的等值线，黑色方形是L 函数的图形。  
二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0 与L 相交时使得w1 或w2 等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。


## reference
[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)
