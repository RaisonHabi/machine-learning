## Mean encodings
均值编码是一种非常强大的技术，它有很多名字，例如:likelihood encoding、target encoding，但这里我们叫它均值编码。  
***用平均数这样的统计量度来对分类值进行编码，这就叫均值编码。***  
示例见参考[数据转图像、表征学习、均值编码、转换目标变量](https://www.cnblogs.com/wzdLY/p/9591075.html)  

数据分析中经常会遇到类别属性，比如日期、性别、街区编号、IP地址等。  
绝大部分数据分析算法是无法直接处理这类变量的，需要先把它们先处理成数值型量。  
如果这些变量的可能值很多，也就是**高基数**，那么在这种情况下，*使用label encoding会出现一系列连续数字，在特征中添加噪声标签和编码会导致精度不佳，使用one-hot编码，随着特征不断增加，数据集的维数也在不断增加，这会阻碍编码*。  
因此，这时均值编码是最好的选择之一。***但它也有缺点，就是容易过拟合，所以使用时要配合适当的正则化技术***：CV 、Regularization Smoothing、Regularization Expanding mean。

## 基本思路与原理
平均数编码是一种有监督（supervised）的编码方式，适用于分类和回归问题。为了简化讨论，以下的所有代码都以分类问题作为例子。  
假设在分类问题中，目标y一共有C个不同类别，具体的一个类别用target表示；某一个定性特征variable一共有K个不同类别，具体的一个类别用k表示。  
先验概率（prior）：数据点属于某一个target（y）的概率，[公式]  
后验概率（posterior）：该定性特征属于某一类时，数据点属于某一个target（y）的概率，[公式]   
**本算法的基本思想：将variable中的每一个k，都表示为（估算的）它所对应的目标y值概率：**  
[公式]。（估算的结果都用“^”表示，以示区分）  
（备注）因此，整个数据集将增加（C-1）列。  
***是C-1而不是C的原因：***    
[公式]，所以最后一个[公式]的概率值必然和其他[公式]的概率值线性相关。  
在线性模型、神经网络以及SVM里，不能加入线性相关的特征列。如果你使用的是基于决策树的模型（gbdt、rf等），个人仍然不推荐这种over-parameterization。
[平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程](https://zhuanlan.zhihu.com/p/26308272)  


## reference
[数据转图像、表征学习、均值编码、转换目标变量](https://www.cnblogs.com/wzdLY/p/9591075.html)  
[平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程](https://zhuanlan.zhihu.com/p/26308272)  
[均值编码-处理高基数类别属性的一个方法](https://blog.csdn.net/z0n1l2/article/details/80791352)
