## 一、FM概述
FM（Factorization Machines，因子分解机）最早由Steffen Rendle于2010年在ICDM上提出，它是一种通用的预测方法，在**即使数据非常稀疏的情况下，依然能估计出可靠的参数进行预测**。

与传统的简单线性模型不同的是，因子分解机考虑了特征间的交叉，对所有嵌套变量交互进行建模（类似于SVM中的核函数），因此在推荐系统和计算广告领域关注的点击率CTR（click-through rate）和转化率CVR（conversion rate）两项指标上有着良好的表现。

此外，FM的模型还具有可以用线性时间来计算，以及能够与许多先进的协同过滤方法（如Bias MF、svd++等）相融合等优点。

&nbsp;
## 二、FM原理
线性模型的优点是简单、方便、易于求解，但缺点在于线性模型中假设不同特征之间是独立的。

为了解决简单线性模型无法学得特征间交叉影响的问题，SVM通过引入核函数来实现特征的交叉，实际上和多项式模型是一样的，这里以只考虑两个特征交叉的二阶多项式模型为例：

多项式模型的问题在于二阶项的参数过多，设特征维数为 n，那么二阶项的参数数目为 n(n+1)/2。 对于广告点击率预估问题，由于存在大量id特征，导致 n 可能为 [公式] 维，这样一来，模型参数的量级为 [公式] ，这比样本量 4× [公式] 多得多！    
**这导致只有极少数的二阶组合模式才能在样本中找到，而绝大多数模式在样本中找不到，因而模型无法学出对应的权重**。  
**例如，对于某个 wij ，样本中找不到xi=1,xj=1 （这里假定所有的特征都是离散的特征，只取0和1两个值）这种样本，那么wij的梯度恒为0，从而导致参数学习失败！也正是这个缘故，我们实际使用SVM时所用到的核函数都是预先定义好的，而非在训练过程中学习得到**。

从而导致参数学习失败！也正是这个缘故，我们实际使用SVM时所用到的核函数都是预先定义好的，而非在训练过程中学习得到。

为了降低上述模型中习得参数的难度，我们很容易想到，可以**对二阶项参数施加某种限制，减少模型参数的自由度**。  
FM 施加的限制是要求二阶项系数矩阵是低秩的，能够分解为低秩矩阵的乘积：

这样一来，就将参数个数减少到 kn，可以设置较少的k值（一般设置在100以内，k<<n），极大地减少模型参数，增强模型泛化能力，这跟矩阵分解的方法是一样的。  
**向量 vi 可以解释为第i个特征对应的隐因子或隐向量。 以user和item的推荐问题为例，如果该特征是user，可以解释为用户向量，如果是item，可以解释为物品向量**。  
**本质上是对特征进行embedding化表征，将0/1这种二值硬匹配转换为向量软匹配，使得样本中未出现的值也能通过向量计算出来，具备良好的泛化性**。


&nbsp;
## 三、FFM
FFM(Field Factorization Machine)是在FM的基础上引入了“场（Field）”的概念而形成的新模型。  
在FM中计算特征 [公式] 与其他特征的交叉影响时，使用的都是同一个隐向量 [公式] 。   
而FFM将特征按照事先的规则分为多个场(Field)，特征 [公式] 属于某个特定的场f。   
每个特征将被映射为多个隐向量 [公式] ，每个隐向量对应一个场。当两个特征 [公式] ,组合时，用对方对应的场对应的隐向量做内积:

**FFM 由于引入了场，使得每两组特征交叉的隐向量都是独立的，可以取得更好的组合效果， FM 可以看做只有一个场的 FFM**。

&nbsp;
## 四、DeepFM
...

&nbsp;
## References
[FM、FFM、DeepFM学习笔记](https://zhuanlan.zhihu.com/p/61096338)
