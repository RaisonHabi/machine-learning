## 贝叶斯优化
贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息**，从而更好的调整当前的参数。

他与常规的网格搜索或者随机搜索的区别是：
```
贝叶斯调参采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息
贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸
贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部优最
```
#### 例子
目前可以做贝叶斯优化的包非常多,光是python就有:
```
BayesianOptimization
bayesopt
skopt
...
```
本文使用BayesianOptimization为例，利用sklearn的随机森林模型进行分类


[强大而精致的机器学习调参方法：贝叶斯优化](https://www.cnblogs.com/yangruiGB2312/p/9374377.html)

&nbsp;
## 介绍
机器学习的寻找最优超参数是个老大难问题，scikit-learn提供了网格搜索GridSearchCV和随机搜索RandomizedSearchCV这两个函数来帮助寻找这些超参数。

网格搜索的本质就是对参数空间形成的所有参数组合进行一个个的尝试，然后选出得分最高的那个，可能会忽略这些组合以外的参数，同时随着参数的增多，计算量也会指数增加。

随机搜索是对参数的随机搜索，但没有充分利用搜索空间的结构。

skopt是一个超参数优化库，包括随机搜索、贝叶斯搜索、决策森林和梯度提升树等，用于辅助寻找机器学习算法中的最优超参数。  
这里是利用skopt的贝叶斯搜索来替代scikit-learn中的默认搜索方法，从而更快更好地寻找到最优超参数。

本文是对这个官方文档的翻译。

&nbsp;
## 示例代码
### 1.官方示例代码
#### 使用多个模型及其对应的搜索空间
```
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

from sklearn.datasets import load_digits
from sklearn.svm import LinearSVC, SVC
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

X, y = load_digits(10, True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0)

pipe = Pipeline(
    [
        ('model', SVC())
    ])

# 隐式地指定参数类型
linsvc_search = {
    'model': [LinearSVC(max_iter=10000)],
    'model__C': (1e-6, 1e+6, 'log-uniform'),
}
# 显式地指定参数类型
svc_search = {
    'model': Categorical([SVC()]),
    'model__C': Real(1e-6, 1e+6, prior='log-uniform'),
    'model__gamma': Real(1e-6, 1e+1, prior='log-uniform'),
    'model__degree': Integer(1,8),
    'model__kernel': Categorical(['linear', 'poly', 'rbf']),
}

opt = BayesSearchCV(
    pipe,
    [(svc_search, 20), (linsvc_search, 16)], # (parameter space, # of evaluations)
)

opt.fit(X_train, y_train)

print("val. score: %s" % opt.best_score_)
print("test score: %s" % opt.score(X_test, y_test))
```

[使用skopt贝叶斯搜索寻找scikit-learn中算法的最优超参数](https://qixinbo.info/2018/09/18/skopt/)

### 2.备份代码
[Santander Bayesian Optimization](https://www.kaggle.com/shaz13/santander-xgboost-bayesian-optimization)
```
# Classifier
from skopt import BayesSearchCV
from sklearn.model_selection import StratifiedKFold

ITERATIONS = 30

bayes_cv_tuner = BayesSearchCV(
    estimator = xgb.XGBClassifier(
        n_jobs = 1,
        objective = 'binary:logistic',
        eval_metric = 'auc',
        booster = 'gbtree',
        silent=1,
        tree_method='approx'
    ),
    search_spaces = {
        'learning_rate': (0.01, 1.0, 'log-uniform'),
        'min_child_weight': (1, 10),
        'max_depth': (1, 50),
        'max_delta_step': (0, 20),
        'subsample': (0.6, 1.0, 'uniform'),
        'colsample_bytree': (0.01, 1.0, 'uniform'),
        'colsample_bylevel': (0.01, 1.0, 'uniform'),
        'reg_lambda': (0.5, 2, 'log-uniform'),
        'reg_alpha': (0.5, 1.0, 'log-uniform'),
        'gamma': (0.1, 1.0, 'log-uniform'),
        'min_child_weight': (1, 10),
        'n_estimators': (10, 100),
        'scale_pos_weight': (1, 100, 'log-uniform')
    },    
    scoring = 'roc_auc',
    cv = StratifiedKFold(
        n_splits=3,
        shuffle=True,
        random_state=42
    ),
    n_jobs = 3,
    n_iter = ITERATIONS,   
    verbose = 0,
    refit = True,
    random_state = 42
)

def status_print(optim_result):
    """Status callback durring bayesian hyperparameter search"""
    
    # Get all the models tested so far in DataFrame format
    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    
    
    # Get current parameters and the best parameters    
    best_params = pd.Series(bayes_cv_tuner.best_params_)
    print('Model #{}\nBest ROC-AUC: {}\nBest params: {}\n'.format(
        len(all_models),
        np.round(bayes_cv_tuner.best_score_, 4),
        bayes_cv_tuner.best_params_
    ))
    
    # Save all model results
#     clf_name = bayes_cv_tuner.estimator.__class__.__name__
#     all_models.to_csv(clf_name+"_cv_results.csv")

```
```
# Fit the model
result = bayes_cv_tuner.fit(train_data_sample.loc[:,x_columns].values
                            , train_data_sample.loc[:,'label'].values, callback=status_print)
```
### 欺诈模型评估
<details>
<summary>展开查看</summary>
<pre><code>
#AUC
pred_train = pd.DataFrame(clf.predict_proba(train_data_sample[train_data_sample.label>=0].loc[:,x_columns])[:,1]
                    , columns=["pred"])
pred = pd.DataFrame(clf.predict_proba(test_data[test_data.label>=0].loc[:,x_columns])[:,1]
                    , columns=["pred"])


#AUC
roc_train = roc_auc_score(train_data_sample[train_data_sample.label>=0]['label'], pred_train)
roc_test = roc_auc_score(test_data[test_data.label>=0]['label'], pred)
print(roc_train, roc_test)

#黑样本率
#没有使用社交变量

result = pd.concat([test_data[test_data.label>=0][['user_id','label']].reset_index()[['user_id','label']],
          pred], axis=1)

result = result.groupby(['user_id']).agg({'label':'max','pred':'max'}).reset_index()
#display(result.head(5))

cut_bin = [0,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,1]
#cut_bin = [0,0.2,0.3,0.35,0.4,0.45,0.5,1]
result_new = pd.concat([result,
        pd.cut(result.pred, cut_bin).rename("pred_bin")], axis=1)

res = result_new[result_new.label>=0].groupby(['pred_bin']).agg(
    {'user_id':'count','label':'sum'}).reset_index()
res.pred_bin = res.pred_bin.astype('str')
res = res.set_index("pred_bin")

res['bad_rate']=(res['label']/res['user_id']).apply(lambda x:str(round(x*100,2))+'%')
res = res.rename(columns={'user_id':'user_cnt','label':'m2+ cnt'}
                ).sort_values(['pred_bin'],ascending=False)

#累计的打扰人数和召回人数
res[['sum_user_cnt','sum_m2+ cnt']]=res[['user_cnt','m2+ cnt']].cumsum()
res['sum_bad_rate'] = (res['sum_m2+ cnt']/res['sum_user_cnt']).apply(lambda x:str(round(x*100,2))+'%')
#res.loc['Total',:]= res.sum(axis=0)
res['累计黑样本率'] = (res['sum_m2+ cnt']/res['sum_user_cnt']).apply(lambda x:str(round(x*100,2))+'%')
res['累计打扰率'] = (res['sum_user_cnt']/result_new.shape[0]).apply(lambda x:str(round(x*100,2))+'%')
res['累计召回率'] = (res['sum_m2+ cnt']/result_new.label.sum()).apply(lambda x:str(round(x*100,2))+'%')

display(res)
</code></pre>
</details>

&nbsp;
## reference
[skopt.BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)   
[使用skopt贝叶斯搜索寻找scikit-learn中算法的最优超参数](https://qixinbo.info/2018/09/18/skopt/)
[sklearn.model_selection.StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)
