## 正则化（Regularization）
机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 ℓ1-norm 和 ℓ2-norm，中文称作 L1正则化 和 L2正则化，或者 L1范数 和 L2范数。  
L1正则化和L2正则化可以看做是损失函数的惩罚项。  
所谓『惩罚』是指对损失函数中的某些参数做一些限制。  
***对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）***。  
## L1正则化和L2正则化
L1正则化是指权值向量w中各个元素的**绝对值之和**，通常表示为∣∣w∣∣1  
L2正则化是指权值向量w中各个元素的**平方和然后再求平方根**（可以看到Ridge回归的L2正则化项有平方符号），通常表示为∣∣w∣∣2
## L1正则化和L2正则化的作用
L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择  
L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合  
## 稀疏模型与特征选择
上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？  
稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0.   
通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。  
在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。  
这就是稀疏模型与特征选择的关系。
## L1和L2正则化的直观理解
***考虑二维的情况，即只有两个权值w1 和w2***      

**图解详见参考文档**   

### 等值线为什么是椭圆形的？
残差平方和是椭圆形的  
[3.4 收缩的方法](https://esl.hohoweiya.xyz/03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html)

### L1
假设只有两个权值w1和w2 ，此时 :  
**L1=∣w1∣+∣w2∣,这个函数画出来就是一个方框**.   

而正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大（上图中的黑色方框）；α越大，L的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值(w1,w2)=(0,w)中的w可以取到很小的值。

### L2
二维平面下L2正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此J0 与L 相交时使得w1或w2等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数w都为0的情况。

&nbsp;
## reference
[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)
