## 机器（统计）学习的两种假设检验
**1.针对特定数据集上的特定模型（如逻辑回归），分析不同变量的显著性**  
**2.在多个数据集上对比多个分类器的性能，分析不同模型表现的差异**

### 1.先讨论第一类检验（特定数据集上变量的显著性）
在社科类文章中的数据分析（如回归）是工具，目的是从数据中归纳结论。而在计算机领域，目的是设计新的模型，而不是分析数据，一般不会专门对实验数据下结论。换句话说：<b>社科类研究中数据分析是工具，而机器学习的目的往往是模型而不是数据本身</b>。</p><p>因为这个原因，我们发现在社科类文章中往往是“对特定数据集上的一个回归模型的变量做显著性分析，来证明某个变量是否对模型有意义”。比如一个数据集有性别、年龄、收入三个变量，分析这几个变量对于患糖尿病的影响。在这种情况下，做统计检验无可厚非。</p><p><b>当然，这样做也有风险和偶然性。</b>比如我常说的一个例子：Freedman在1989年做过的模拟实验 [1]中发现，即使数据全是由噪音构成，在适当的处理后，也能发现数据中显著的相关性：6个特征显著且对回归所做的F-test的p值远小于0.05，即回归存在统计学意义。更多例子可以看：<a href="https://www.zhihu.com/question/66895407/answer/381880059" class="internal">微调：有哪些相关性不等于因果性的例子?</a></p><p><b>除此之外，我们也认为广义线性模型的数据挖掘能力有限，对于复杂的非线性数据可能无法很好的拟合</b>。所以社科类文章中的很多结论也不完全正确，但受限于数据，往往这就是当下的最优解。正因为如此，也有不少研究者在呼吁弱化p值的重要性。</p><p><b>而在机器学习中，一个变量是否重要，往往是通过“特征选择”和“特征重要性排序”来体现的</b>。比如大部分决策树模型和集成树模型都可以提供一个变量重要性排序，可以等同视为统计检验。从实际效果上看，往往更好。</p><p>但为什么大部分社科类研究必须要用广义线性回归模型呢？<b>主要是为了可解释性，来说明不同变量对最终结果的“贡献”，因此另一个附加价值就是统计检验</b>。而机器学习因为其黑箱性导致了不大适合用于数据分析，也就不存在统计检验。<b>值得注意的，虽然没有统计检验，但受益于机器学习中各种复杂模型的有效性，预测结果往往更准确，而且也可以得到很多有价值的分析结果</b>。但用于学术研究的话，往往人们无法信任纯粹的对比和变量重要性排序，因此社科类中很多研究还是基于各种线性回归。

### 2.再讨论第二类情况（对比多个模型在多个数据集上的表现）
其实这是机器学习，尤其是传统机器学习方向的一个趋势，越来越多的论文要求提供统计检验。我最近的一篇论文的审稿意见就有这么一条：“必须做统计检验”。</p><p>当然，这个要求有时并不合理，在特定领域也没有必要，原因如下：</p><ul><li>需要一定数量的数据集（样本量），一般来说大于10个，甚至15个数据集比较好。但这样显然是不现实的，很多领域（如机器视觉）的数据集非常大。如果是深度学习在多个大数据集上运行的开销过大，大部分情况并不现实。</li><li>当数据集已经非常大且具有代表性时，没有必要做统计检验。举个简单的例子，如果有世界上百分之99人的图片，并用其预测剩下百分之1的人性别。那么在99%数据上表现足够好的模型应该就是最好的，不必多此一举。说到底，这个可能是统计学和机器学习的差异，前者更严谨后者更有效，难分优劣。</li><li>不可否认，很多人其实是做过统计检验的，因为不显著于是又删掉了。换句话说，回避统计检验一定程度上也造成了灌水现象...</li></ul><p>但话说回来，在传统机器学习领域，尤其是大量使用UCI上数据集的研究（如很多无监督学习），其实是可以做统计检验的，因为数据集都不大且数量众多。而在机器学习模型上做统计检验的重要性在2006年就有一篇JMLR论文讨论过[3]，结合我的一些经验可以简单归纳为：</p><blockquote>首先结论如下，在对比<b>两个算法</b>在<b>多个数据集</b>上的表现时：<br/>- 如果样本配对（paired）且符合正态分布，优先使用配对t检测（paired t test）。<br/>- 如果样本不符合正态分布，但符合配对，使用Wilcoxon Signed Ranks test。<br/>- 如果样本既不符合正态分布，也不符合配对，甚至样本量都不一样大，可以尝试Mann Whitney U test。值得注意的是，MW是用来处理独立测量（independent measures）数据，要分情况讨论，后文会深入分析。<br/><br/>在对比<b>多个算法</b>在<b>多个数据集</b>上的表现时：<br/>- 如果样本符合ANOVA（repeated measure）的假设（如正态、等方差），优先使用ANOVA。<br/>- 如果样本不符合ANOVA的假设，使用Friedman test配合Nemenyi test做post-hoc。<br/>- 如果样本量不一样，或因为特定原因不能使用Friedman-Nemenyi，可以尝试Kruskal Wallis配合Dunn&#39;s test。值得注意的是，这种方法是用来处理独立测量数据，要分情况讨论。</blockquote><p>更详细的如何用统计检验对比机器学习模型，请参考：<a href="https://www.zhihu.com/question/27306416/answer/372241948" class="internal">微调：常用的机器学习算法比较？</a></p><hr/><p>[1] Freedman, L.S. and Pee, D., 1989. Return to a note on screening regression equations. <i>The American Statistician</i>, <i>43</i>(4), pp.279-282.</p><p>[2] <a href="https://link.zhihu.com/?target=http%3A//www.tylervigen.com/spurious-correlations" class=" wrap external" target="_blank" rel="nofollow noreferrer">15 Insane Things That Correlate With Each Other</a></p><p>[3] Demšar, J., 2006. Statistical comparisons of classifiers over multiple data sets. <i>Journal of Machine learning research</i>, <i>7</i>(Jan), pp.1-30.</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="/question/55420602/answer/394577531">

&nbsp;
## 常用的机器学习算法比较
<p>授人以鱼不如授人以渔，这篇文章会介绍<b>如何通过“统计学检验”来对比机器学习算法性能</b>。掌握了这个方法后，我们就不需要再人云亦云，而可以自己分析算法性能。</p><p>首先结论如下，在对比<b>两个算法</b>在<b>多个数据集</b>上的表现时：</p><ul><li>如果样本配对（paired）且符合正态分布，优先使用配对t检测（paired t test）。</li><li>如果样本不符合正态分布，但符合配对，使用Wilcoxon Signed Ranks test。</li><li>如果样本既不符合正态分布，也不符合配对，甚至样本量都不一样大，可以尝试Mann Whitney U test。值得注意的是，MW是用来处理独立测量（independent measures）数据，要分情况讨论，后文会深入分析。</li></ul><p>在对比<b>多个算法</b>在<b>多个数据集</b>上的表现时：</p><ul><li>如果样本符合ANOVA（repeated measure）的假设（如正态、等方差），优先使用ANOVA。</li><li>如果样本不符合ANOVA的假设，使用Friedman test配合Nemenyi test做post-hoc。</li><li>如果样本量不一样，或因为特定原因不能使用Friedman-Nemenyi，可以尝试Kruskal Wallis配合Dunn&#39;s test。值得注意的是，这种方法是用来处理独立测量数据，要分情况讨论。</li></ul><p>文章结构如下：(1-2) 算法对比的原因及陷阱 （3-4) 如何对比两个算法 （5-6）如何对比多个算法 （7）如何根据数据特性选择对比方法 （8）工具库介绍。 </p><hr/><h3><b>1. 为什么需要对比算法性能？</b></h3><p>统计学家George Box说过：“All models are wrong, but some are useful”（所有模型都是错误，只不过其中一部分是有价值的）。通俗来说，<b>任何算法都有局限性，所以不存在“通用最优算法”，只有在特定情境下某种算法可能是渐进最优的</b>。</p><p>因此，评估算法性能并选择最优算法是非常重要的。不幸的是，统计学评估还没有在机器学习领域普及，很多评估往往是在一个数据上的简单分析，因此证明效果有限。</p><h3><b>2. 评估算法中的陷阱</b></h3><p>首先我们常说的是要选择一个<b>正确的评估标准</b>，常见的有：准确率（accuracy）、召回率（recall）、精准率（precision）、ROC、Precision-Recall Curve、F1等。</p><p>选择评估标准取决于目的和数据集特性。在较为平衡的数据集上（各类数据近似相等的情况下），这些评估标准性能差别不大。而在数据严重倾斜的情况下，选择不适合的评估标准，如准确率，就会导致看起来很好，但实际无意义的结果。举个例子，假设某稀有血型的比例（2%），模型只需要预测全部样本为“非稀有血型”，那么准确率就高达98%，但毫无意义。在这种情况下，选择ROC或者精准率可能就更加适当。这方面的知识比较容易理解，很多科普书都有介绍，我们就不赘述了。</p><p>其次我们要正确理解<b>测量方法</b>，常见的有</p><ul><li>独立测量（independent measures）：不同样本的观测对象是独立的，不存在关联</li><li>重复测量（repeated measures）：样本中使用的观测对象是相同的，仅仅是独立变量在上面的作用结果不同</li><li>以及成对测量（matched pair）：不同样本中采用不同的观测对象，但尽量使得样本间的观测对象成对相似</li></ul><p>举个例子，我们想要分析刷知乎时间（每天3小时 vs. 每天10小时）对于大学生成绩的影响。如果我们使用相同的20个学生，观察他们每天3小时和10小时的区别，那就是重复测量。如果我们选择40个学生，分成两组每组20人，再分别观察那就是独立测量。如果我们先找20个学生，再找20个和他们非常相似的大学生，并配对观察，就是成对相似。</p><p><b>我们发现，当错误的理解测量方式时，就无法使用正确的统计学手段进行分析</b>。</p><p>在这篇文章中我们默认：评估不同算法在多个<b>相同数据集</b>上的表现属于<b>重复测量</b>，而特例将会在第七部分讨论。同时，本文介绍的方法<b>可以用于对比任何评估标准</b>，如准确度、精准度等，本文中默认讨论准确度。</p><h3><b>3. 两种算法间的比较：不恰当方法</b></h3><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/50/v2-237321b60042d06ce188ef066ee86949_hd.jpg" data-size="normal" data-rawwidth="279" data-rawheight="291" class="content_image" width="279"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;279&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="279" data-rawheight="291" class="content_image lazy" width="279" data-actualsrc="https://pic4.zhimg.com/50/v2-237321b60042d06ce188ef066ee86949_hd.jpg"/><figcaption>图1. 两种算法在14个数据集上的准确率 [1]</figcaption></figure><p>图1展示了两种决策树方法（C4.5，C4.5+m）在14个数据集上的准确率。那么该如何对比两种算法呢？先说几种错误（不恰当）的方法：</p><p><b>不恰当方法1：求每个算法在所有数据集上的均值，并比较大小</b>。错误原因：我们对于算法在不同数据集上错误的期望不是相同的，因此求平均没有意义。换句话说，数据不符合相称性（commensurate）。</p><p><b>不恰当方法2</b>：<b>进行配对样本t检测（Paired t test）</b>。显然，t test是统计学方法，可以用来查看两种方法在每个数据上的平均差值是否不等于0。但这个方法不合适原因有几点：</p><ul><li>和平均一样，不同数据集上的错误不符合相称性</li><li>t-test要求样本符合正态分布，显然我们无法保证不同数据集上的准确率符合正态分布</li><li>t-test对样本的大小有一定的要求，一般最低需要&gt;30个样本。在这个例子中我们只有14个，且大部分情况下我们没有30个数据来做实验。</li><li>因为缺乏相称性，统计结果易受到异常值影响（outliers）</li></ul><p><b>不恰当方法3：符号检验（sign test）</b>是一种无参数（non-parametric）的检验，优点是对于样本分布没有要求，不要求正态性。比较方法很简单，就是在每个数据集上看哪个算法更好，之后统计每个算法占优的数据集总数。以这个例子为例，C4.5在2个数据集上最优，2个平手，10个最差。如果我们对这个结果计算置信区间，发现p&lt;0.05需要至少在11个数据集上表现最优。因此这个方法的缺点有：</p><ul><li>符号检验是一种非常弱的检验方法，仅对比优劣损失了大量信息，失去了定量信息（quantitative），比如 <img src="https://www.zhihu.com/equation?tex=0.1%3C0.9" alt="0.1&lt;0.9" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=0.1%3C0.11" alt="0.1&lt;0.11" eeimg="1"/> 的意义是一样的。正因为如此，临界值（critical value）一般都需要很大，比如这个例子中的 <img src="https://www.zhihu.com/equation?tex=%5Calpha%3D0.05" alt="\alpha=0.05" eeimg="1"/> 的临界值是11（图2）。</li><li>另一个问题是，因为缺乏定量信息，很多时候很难确定“优胜”是否来自随机性。举个例子，0.99&lt;0.991是否真的代表算法A更好？一种看法是需要定义一个阈值，仅当差别大于阈值才能说明更好。然而这种看法的问题在于，假设算法A在1000个数据集上都以“微弱优势”胜过了B，那么我们是否需要怀疑显著性？因此，<b>根本问题还是，符号检验需要大样本量才能得出显著性</b>。</li></ul><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-75c465377fe217998581a1cc194bae64_hd.jpg" data-size="normal" data-rawwidth="1166" data-rawheight="162" class="origin_image zh-lightbox-thumb" width="1166" data-original="https://pic1.zhimg.com/v2-75c465377fe217998581a1cc194bae64_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1166&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1166" data-rawheight="162" class="origin_image zh-lightbox-thumb lazy" width="1166" data-original="https://pic1.zhimg.com/v2-75c465377fe217998581a1cc194bae64_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-75c465377fe217998581a1cc194bae64_hd.jpg"/><figcaption>图2. 符号检验的临界值表</figcaption></figure><h3><b>4. 两种算法间的比较：推荐方法</b></h3><p>考虑到通用性，我们需要使用非参数检验。换句话说，我们需要保证对样本的分布不做任何假设，这样更加通用。</p><p><b>方法1：Wilcoxon Signed Ranks Test（WS ）</b>是<b>配对t检验的无参数版本</b>，同样是分析成对数据的差值是否等于0，只不过是通过排名（rank）而已。换个角度看，我们也可以理解为<b>符号检验的定量版本</b>。优点如下：</p><ul><li>无参数，不要求样本符合正态分布</li><li>符合数据相称性，虽然是定性的（与配对t检验相比）</li><li>有一定的定量特性，即较大的差别对于最终结果影响更大（与符号检验相比）</li></ul><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-c2c7489fd9162441e33f04ab9eae0a58_hd.jpg" data-size="normal" data-rawwidth="542" data-rawheight="304" class="origin_image zh-lightbox-thumb" width="542" data-original="https://pic3.zhimg.com/v2-c2c7489fd9162441e33f04ab9eae0a58_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;542&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="542" data-rawheight="304" class="origin_image zh-lightbox-thumb lazy" width="542" data-original="https://pic3.zhimg.com/v2-c2c7489fd9162441e33f04ab9eae0a58_r.jpg" data-actualsrc="https://pic3.zhimg.com/50/v2-c2c7489fd9162441e33f04ab9eae0a58_hd.jpg"/><figcaption>图3. 两种算法在14个数据集上的准确率与排序[1]</figcaption></figure><p><b>方法2（详见第七部分）：Mann Whitney U test</b>（MW）和WS一样，都是无参数的且研究排名的检验方法。MW有以下特性：</p><ul><li>可以用来检测不同的大小的样本，举例A算法在8个数据集上的表现 vs B算法在10个数据集上的表现。</li><li>不存在配对性要求，参看上一点</li><li>对比的是两个样本的分布，因此不同数据集的错误应该符合特定分布，可能不满足相称性</li><li>对于测量方法的假设是：<b>独立测量</b>，这与我们的实际情况不符</li></ul><p>换句话说，MW是当样本量不同时才建议勉强一试，因为不符合独立测量的假设。不同数据集的错误（准确率）不一定符合特定分布，很可能不符合相称性，但在特定情况下有用，详见第七部分。</p><p><b>总结：如果样本配对且符合正态分布，优先使用配对t检测。如果样本不符合正态分布，但符合配对，使用WS。如果样本既不符合正态分布，也不符合配对，可以尝试MW。</b></p><h3><b>5. 多种算法间的比较：不恰当的方法</b></h3><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-9da7ac7b74148bd7f6cc0b711c8eed96_hd.jpg" data-size="normal" data-rawwidth="419" data-rawheight="235" class="content_image" width="419"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;419&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="419" data-rawheight="235" class="content_image lazy" width="419" data-actualsrc="https://pic3.zhimg.com/50/v2-9da7ac7b74148bd7f6cc0b711c8eed96_hd.jpg"/><figcaption>图4. 四种算法在14个数据集上的准确率与排序[1]</figcaption></figure><p>图4提供了四种算法（C4.5，C4.5+m，C4.5+cf，C4.5+m+cf）在14个数据集上的准确率。</p><p><b>不恰当方法1</b>：一种看法是，我们是否可以把两个算法的对比推广到多个算法上。假设有k个算法，我们是否可以对它们进行两两比较，经过 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%281%2B%28k-1%29%29%5Ctimes+%28k-1%29%7D%7B2%7D%3D%5Cfrac%7Bk%5E2-k%7D%7B2%7D" alt="\frac{(1+(k-1))\times (k-1)}{2}=\frac{k^2-k}{2}" eeimg="1"/> 次计算得到一个矩阵。这个是经典的多元假设检验问题，这种穷举法一般都假设了不同对比之间的独立性，一般都不符合现实，需要进行校正，因此就不赘述了。</p><p><b>不恰当方法2</b>：<b>Repeated measures ANOVA</b>是经典的统计学方法，用来进行多样本间的比较是，可以看做是t检验的多元推广。ANOVA不适合对比算法表现的原因如下：</p><ul><li>对样本分布有正态假设，然而不同数据集上的准确度往往不符合这个假设</li><li>不同的样本有相同的总体方差（population variance）</li></ul><p>不幸的是，我们想要对比的算法表现不符合这个情况，因此ANOVA不适合。</p><h3><b>6. 多种算法间的比较：推荐的方法</b></h3><p>我们需要找到一种方法同时解决第5部分中提到的问题，这个方法需要：</p><ul><li>非参数，不对数据的分布做出假设</li><li>不需要，或者尽量不依赖，或者可以自动修正两两对比所带来的误差</li></ul><p>Demšar [1]推荐了非参数的多元假设检验<b>Friedman test</b>。Friedman也是一种建立在排名（rank）上的检验，它假设所有样本的排序均值相等。具体来讲，我们首先给不同算法在每个数据集上排序，并最终计算算法A在所有数据集上排名的均值。如果所有算法都没有性能差别，那么它们的性能的平均排名应该是相等的，这样我们就可以选择特定的置信区间来判断差异是否显著了。</p><p>假设我们通过Friedman test发现有统计学显著（<i>p</i>&lt;0.05），那么我们还需要继续做事后分析（post-hoc）。<b>换句话说，Friedman test只能告诉我们算法间是否有显著差异，而不能告诉我们到底是哪些算法间有性能差异。想要定位具体的差异算法，还需要进行post-hoc分析。</b></p><p>Friedman test一般配套的post-hoc是Nemenyi test，Nemenyi test可以指出两两之间是否存在显著差异。我们一般还会对Nemenyi的结果可视化，比如下图。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-9c0c6b8b11f880cae21aa9d3d35e497a_hd.jpg" data-size="normal" data-rawwidth="599" data-rawheight="366" class="origin_image zh-lightbox-thumb" width="599" data-original="https://pic1.zhimg.com/v2-9c0c6b8b11f880cae21aa9d3d35e497a_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;366&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="599" data-rawheight="366" class="origin_image zh-lightbox-thumb lazy" width="599" data-original="https://pic1.zhimg.com/v2-9c0c6b8b11f880cae21aa9d3d35e497a_r.jpg" data-actualsrc="https://pic1.zhimg.com/50/v2-9c0c6b8b11f880cae21aa9d3d35e497a_hd.jpg"/><figcaption>图5. Nemenyi对10种算法的对比结果，NS代表不显著</figcaption></figure><p>另一个值得提的是，即使Friedman证明算法性能有显著不同，Nemenyi不一定会说明到底是哪些算法间不同，原因是Nemenyi比Friedman要弱（weak），实在不行可以对必须分析的算法成对分析。</p><p>方法2（详见第七部分）：和两两对比一样，在多个样本对比时也有一些特定情况导致我们不能使用Friedman-Nemenyi。另一个或许可以值得一试的无参数方法是Kruskal Wallis test搭配Dunn&#39;s test（作为post-hoc）。 这种方法的特点是：</p><ul><li>可以用来检验不同的大小的样本，举例A算法在8个数据集上的表现 vs B算法在10个数据集上的表现 vs C算法在20个数据集上的表现。</li><li>对于测量方法的假设是：<b>独立测量</b>，这与我们的实际情况不符。</li></ul><h3><b>7. 再看重复测量和独立测量</b></h3><p>我们在第二部分分析了重复测量与独立测量，而且假设机器学习性能的对比<b>应该是建立在“重复测量”上的，也就是说所有的算法都在相同的数据集上进行评估</b>。</p><p>在这种假设下，我们推荐了无参数的：Wilcoxon对两个算法进行比较， Friedman-Nemenyi对多个算法进行对比。</p><p>然而，“<b>重复测量”的假设不一定为真</b>。举个例子，如果我们只有一个数据，并从数据中采样（sample）得到了很多相关的测试集1, 2，3...n，并用于测试不同的算法。</p><ul><li>算法A：测试集1,2</li><li>算法B：测试集3, 4，5，6</li><li>算法N...</li></ul><p>在这种情况下，我们就可以用Mann Whitney U test对比两种算法，Kruskal-Dunn对比多种算法。<b>而且值得注意的是，这种情况常见于人工合成的数据，比如从高斯分布中采样得到数据</b>。因此，要特别分析数据的测量方式，再决定如何评估。</p><h3><b>8. 工具库与实现</b></h3><p>我们知道R上面有所有这些检验，着重谈谈Python上的工具库。幸运的是，上文提到所有检验方法在Python上都有工具库</p><p>Scipy <a href="https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/scipy/reference/stats.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Statistical functions </a>：Wilcoxon，Friedman，Mann Whitney</p><p><a href="https://link.zhihu.com/?target=https%3A//pypi.org/project/scikit-posthocs/" class=" wrap external" target="_blank" rel="nofollow noreferrer">scikit-posthocs</a>：Nemenyi，Dunn&#39;s test</p><hr/><p>文章的配图来自于[1] 以及我的一篇paper [2]，接收后会补上reference。<b>文章的思路和脉络基于[1]，</b>建议阅读。[2]主要着力于特定情况，当重复测量失效时的检验。</p><p>[1] Demšar, J., 2006. Statistical comparisons of classifiers over multiple data sets. <i>Journal of Machine learning research</i>, <i>7</i>(Jan), pp.1-30.</p><p>[2] To complete.</p>

&nbsp;
## reference
[Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/)  
[常用的机器学习算法比较？](https://www.zhihu.com/question/27306416/answer/372241948)  
[常用的机器学习算法比较？ - 于菲的回答 - 知乎](https://www.zhihu.com/question/27306416/answer/36701217)  
[Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf)   
[为什么做机器学习的很少使用假设检验？ - 微调的回答 - 知乎](https://www.zhihu.com/question/55420602/answer/394577531)  
[]()  
