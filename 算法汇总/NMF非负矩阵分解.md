## NMF非负矩阵分解
LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？
### 1、非负矩阵分解(non-negative matrix factorization)
是一种非常常用的矩阵分解方法，它可以适用于很多领域，比如图像特征识别，语音识别等，这里我们会主要关注于它在文本主题模型里的运用。

回顾奇异值分解，它会将一个矩阵分解为三个矩阵：𝐴=𝑈Σ𝑉𝑇  
如果降维到𝑘维，则表达式为：𝐴𝑚×𝑛≈𝑈𝑚×𝑘Σ𝑘×𝑘𝑉𝑇𝑘×𝑛

但是NMF虽然也是矩阵分解，它却使用了不同的思路，它的目标是期望**将矩阵分解为两个矩阵:𝐴𝑚×𝑛≈𝑊𝑚×𝑘𝐻𝑘×𝑛**   
分解成两个矩阵是不是一定就比SVD省时呢？这里的理论不深究，但是NMF的确比SVD快。不过如果大家读过我写的矩阵分解在协同过滤推荐算法中的应用，就会发现里面的FunkSVD所用的算法思路和NMF基本是一致的，只不过FunkSVD聚焦于推荐算法而已。

### 2、NMF的优化思路
NMF期望找到这样的两个矩阵𝑊,𝐻，使𝑊𝐻的矩阵乘积得到的矩阵对应的每个位置的值和原矩阵𝐴对应位置的值相比误差尽可能的小。  
用数学的语言表示就是：
𝑎𝑟𝑔𝑚𝑖𝑛⏟𝑊,𝐻12∑𝑖,𝑗(𝐴𝑖𝑗−(𝑊𝐻)𝑖𝑗)2

我们要求解的有𝑚∗𝑘+𝑘∗𝑛个参数。参数不少，常用的迭代方法有梯度下降法和拟牛顿法。不过如果我们决定加上了L1正则化的话就不能用梯度下降和拟牛顿法了。

Lasso回归有时也叫做线性回归的L1正则化，和Ridge回归的主要区别就是在正则化项，Ridge回归用的是L2正则化，而Lasso回归用的是L1正则化。  
但是Lasso回归有一个很大的问题，导致我们需要把它单独拎出来讲，就是**它的损失函数不是连续可导的，由于L1范数用的是绝对值之和，导致损失函数有不可导的点**。  
也就是说，我们的**最小二乘法，梯度下降法，牛顿法与拟牛顿法对它统统失效了**。那我们怎么才能求有这个L1范数的损失函数极小值呢？

两种全新的求极值解法坐标轴下降法（coordinate descent）和最小角回归法（ Least Angle Regression， LARS）该隆重出场了。　  　  

[lasso回归算法： 坐标轴下降法与最小角回归法小结](https://www.cnblogs.com/pinard/p/6018889.html)

### 3、NMF 用于文本主题模型
回到我们本文的主题，NMF矩阵分解如何运用到我们的主题模

此时NMF可以这样解释：我们输入的有m个文本，n个词，而𝐴𝑖𝑗对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，𝑊𝑖𝑘对应第i个文本的和第k个主题的概率相关度，而𝐻𝑘𝑗对应第j个词和第k个主题的概率相关度

当然也可以反过来去解释：我们输入的有m个词，n个文本，而𝐴𝑖𝑗对应第i个词的第j个文本的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。NMF分解后，𝑊𝑖𝑘对应第i个词的和第k个主题的概率相关度，而𝐻𝑘𝑗对应第j个文本和第k个主题的概率相

注意到这里我们使用的是"概率相关度"，这是因为我们使用的是"非负"的矩阵分解，这样我们的𝑊,𝐻矩阵值的大小可以用概率值的角度去看。从而可以得到文本和主题的概率分布关系。第二种解释用一个图来表示

和LSI相比，我们不光得到了文本和主题的关系，还得到了直观的概率解释，同时分解速度也不错。**当然NMF由于是两个矩阵，相比LSI的三矩阵，NMF不能解决词和词义的相关度问题。这是一个小小的代价**。

### 4、scikit-learn NMF的使用
在 scikit-learn中，NMF在sklearn.decomposition.NMF包中，它支持L1和L2的正则化，**而𝑊,𝐻的求解使用坐标轴下降法来实现**。

NMF需要注意的参数有：  
1） **n_components：即我们的主题数k, 选择k值需要一些对于要分析文本主题大概的先验知识。可以多选择几组k的值进行NMF，然后对结果人为的进行一些验证**。  
2） init : 用于帮我们选择𝑊,𝐻迭代初值的算法， 默认是None,即自动选择值，不使用选择初值的算法。如果我们对收敛速度不满意，才需要关注这个值，从scikit-learn提供的算法中选择一个合适的初值选取算法。  
3）alpha: 即我们第三节中的正则化参数𝛼,需要调参。开始建议选择一个比较小的值，如果发现效果不好在调参增大。  
4) l1_ratio：　即我们第三节中的正则化参数𝜌,L1正则化的比例，仅在𝛼>0时有效，需要调参。开始建议不使用，即用默认值0, 如果对L2的正则化不满意再加上L1正则化。

### 5、Gensim NMF的使用
[models.nmf – Non-Negative Matrix factorization](https://radimrehurek.com/gensim/models/nmf.html)

Train an NMF model using a Gensim corpus
```
>>> from gensim.test.utils import common_texts
>>> from gensim.corpora.dictionary import Dictionary
>>>
>>> # Create a corpus from a list of texts
>>> common_dictionary = Dictionary(common_texts)
>>> common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]
>>>
>>> # Train the model on the corpus.
>>> nmf = Nmf(common_corpus, num_topics=10)
```
### 6、NMF主题模型小结
虽然我们是在主题模型里介绍的NMF，但实际上NMF的适用领域很广，除了我们上面说的图像处理，语音处理，还包括信号处理与医药工程等，是一个普适的方法。在这些领域使用NMF的关键在于将NMF套入一个合适的模型，使得𝑊,𝐻矩阵都可以有明确的意义。

NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。  

**缺点:**  
但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本是无法识别其主题的。  
根本原因在于**NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型**。  
而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。

&nbsp;
## reference
[文本主题模型之非负矩阵分解(NMF)](https://www.cnblogs.com/pinard/p/6812011.html)
