https://www.zhihu.com/question/34681168/answer/369499469

DNN指的是包含多个隐层的神经网络，如图1所示，根据神经元的特点，可以分为MLP、CNNs、RNNs等，下文在区分三者的时候，都从神经元的角度来讲解。MLP是最朴素的DNN，CNNs是encode了空间相关性的DNN，RNNs是encode进了时间相关性的DNN。

MLP是最简单的DNN，它的每一层其实就是fc层（fully connected layer）.

CNNs相对于MLP而言，多了一个先验知识，即数据之间存在空间相关性，比如图像，蓝天附近的像素点是白云的概率会大于是水桶的概率。滤波器会扫过整张图像，在扫的过程中，参数共享。

RNNs相对于MLP而言，也多了一个先验知识，即数据之间存在时间相关性，比如一段文字，前面的字是“上”，后面的字是“学”概率更大，是“狗”的概率很小。RNNs神经元的输入会有多个time step，每个time step的输入进入神经元中时会共享参数。





神经网络
的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。增加节点数：增加维度，即增加线性转换能力。增加层数：增加激活函数的次数，即增加非线性转换次数。


对卡在局部极小值的处理方法：

1.调节步伐：调节学习速率，使每一次的更新“步伐”不同；
2.优化起点：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）。
https://blog.csdn.net/xwd18280820053/article/details/76026523 





https://blog.csdn.net/malefactor/article/details/50436735




DNN以神经网络为载体，重在深度，可以说是一个统称。
RNN，回归型网络，用于序列数据，并且有了一定的记忆效应，辅之以lstm。
CNN应该侧重空间映射，图像数据尤为贴合此场景。





DNN(深度神经网络)
神经网络是基于感知机的扩展，而DNN可以理解为有很多隐藏层的神经网络。多层神经网络和深度神经网络DNN其实也是指的一个东西，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）。
DNN存在的局限：
参数数量膨胀。由于DNN采用的是全连接的形式，结构中的连接带来了数量级的权值参数，这不仅容易导致过拟合，也容易造成陷入局部最优。
局部最优。随着神经网络的加深，优化函数更容易陷入局部最优，且偏离真正的全局最优，对于有限的训练数据，性能甚至不如浅层网络。
梯度消失。使用sigmoid激活函数（传递函数），在BP反向传播梯度时，梯度会衰减，随着神经网络层数的增加，衰减累积下，到底层时梯度基本为0。
无法对时间序列上的变化进行建模。对于样本的时间顺序对于自然语言处理、语音识别、手写体识别等应用非常重要。

CNN(卷积神经网络)
主要针对DNN存在的参数数量膨胀问题，对于CNN，并不是所有的上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在多有图像内是共享的，图像通过卷积操作仍能保留原先的位置关系。
CNN之所以适合图像识别，正式因为CNN模型限制参数个数并挖掘局部结构的这个特点。

RNN(循环神经网络)
针对CNN中无法对时间序列上的变化进行建模的局限，为了适应对时序数据的处理，出现了RNN。
在普通的全连接网络或者CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立（这种就是前馈神经网络）。而在RNN中，神经元的输出可以在下一个时间戳直接作用到自身。
（t+1）时刻网络的最终结果O(t+1)是该时刻输入和所有历史共同作用的结果，这就达到了对时间序列建模的目的。
存在的问题：RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度，而梯度消失的现象出现时间轴上。
