### sigmoid和tanh： 
1.sigmoid的导数值小于1/4,sigmoid在输入处于[-1,1]之间时，函数值变化敏感，一旦接近或者超出区间就失去敏感性，处于饱和状态，影响神经网络预测的精度值；

2.tanh的变化敏感区间较宽，导数值渐进于0、1，符合人脑神经饱和的规律，比sigmoid函数延迟了饱和期； 

3.tanh在原点附近与y=x函数形式相近，当激活值较低时，可以直接进行矩阵运算，训练相对容易. 

4.不过tanh另外有两个好处，均值是０，且在原点附近更接近 y=x （identity function,恒等函数）的形式。   
均值不为０就意味着自带了一个bias，在计算时是额外的负担，这会使得收敛变得更慢；   
更接近 y=x 意味着在几何变换运算中更具有优势，比如在激活值较低时（也可以通过变换把结果normalized到[-1,1]之间，tanh在 [-1, 1]的范围内tanh和 y=x 非常非常接近），可以利用indentity function的某些性质，直接进行矩阵运算。所以**tanh在具体应用中比Sigmoid函数更具有优越性，训练相对容易**

[激活函数Sigmoid,Tanh,ReLu,softplus,softmax](https://zhuanlan.zhihu.com/p/48776056)

TanHyperbolic(tanh)函数又称作双曲正切函数,其函数曲线与Sigmoid函数非常相似, 可以通过缩放平移相互变换，  
<p><img src="https://www.zhihu.com/equation?tex=f%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D%3D%5Cfrac%7Be%5E%7Bx%7D%7D%7Be%5E%7Bx%7D%2B1%7D%3D%5Cfrac%7B1%7D%7B2%7D%2B%5Cfrac%7B1%7D%7B2%7D+%5Ctanh%28%5Cfrac%7Bx%7D%7B2%7D%29" alt="f(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}=\frac{1}{2}+\frac{1}{2} \tanh(\frac{x}{2})" eeimg="1"/> </p>

&nbsp;
### RNN中为什么要采用tanh而不是ReLu作为激活函数？ 
RNN中直接把激活函数换成ReLU会导致非常大的输出值.    
训练过程中会导致神经元死亡的问题(负梯度在经过该ReLU单元时被置为0),如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”. 

