## 一、Feature-based
指利用语言模型的中间结果也就是LM embedding, 将其作为额外的特征，引入到原任务的模型中，   
例如在TagLM[1]中，采用了两个单向RNN构成的语言模型，将语言模型的中间结果引入到序列标注模型中，   
如下图1所示，其中左边部分为序列标注模型，也就是task-specific model，每个任务可能不同，右边是前向LM(Left-to-right)和后向LM(Right-To-Left), 两个LM的结果进行了合并，并将LM embedding与词向量、第一层RNN输出、第二层RNN输出进行了concat操作。

### 通常feature-based方法包括两步：
1.首先在大的语料A上无监督地训练语言模型，训练完毕得到语言模型

2.然后构造task-specific model例如序列标注模型，采用有标记的语料B来有监督地训练task-sepcific model，   
将语言模型的参数固定，语料B的训练数据经过语言模型得到LM embedding，作为task-specific model的额外特征


## 二、Fine-tuning方式
是指在已经训练好的语言模型的基础上，加入少量的task-specific parameters, 例如对于分类问题在语言模型基础上加一层softmax网络，然后在新的语料上重新训练来进行fine-tune。

### LM + Fine-Tuning的方法工作包括两步：
1.构造语言模型，采用大的语料A来训练语言模型

2.在语言模型基础上增加少量神经网络层来完成specific task例如序列标注、分类等，   
然后采用有标记的语料B来有监督地训练模型，这个过程中语言模型的参数并不固定，依然是trainable variables.

## 三、bert
### Masked Language Model
现有的语言模型的问题在于，没有同时利用到Bidirectional信息，现有的语言模型例如ELMo号称是双向LM(BiLM)，但是实际上是两个单向RNN构成的语言模型的拼接，

前向RNN构成的语言模型计算的是：
也就是当前词的概率只依赖前面出现词的概率。

而后向RNN构成的语言模型计算的是：
也就是当前词的概率只依赖后面出现的词的概率。

那么如何才能同时利用好前面词和后面词的概率呢？BERT提出了Masked Language Model，也就是随机去掉句子中的部分token，然后模型来预测被去掉的token是什么。  
这样实际上已经不是传统的神经网络语言模型(类似于生成模型)了，而是单纯作为分类问题，根据这个时刻的hidden state来预测这个时刻的token应该是什么，而不是预测下一个时刻的词的概率分布了。

这样操作存在一个问题，fine-tuning的时候没有[MASK] token，因此存在pre-training和fine-tuning之间的mismatch，为了解决这个问题，采用了下面的策略：   
```
80%的时间中：将选中的词用[MASK]token来代替，   

10%的时间中：将选中的词用任意的词来进行代替，例如   
my dog is hairy → my dog is apple

10%的时间中：选中的词不发生变化  
```
但这也只是缓解、不能解决

### Bert的双向体现在什么地方？
Bert可以看作Transformer的encoder部分。Bert模型舍弃了GPT的attention mask。双向主要体现在Bert的预训练任务一：遮蔽语言模型（MLM）

bert如何构造输入。输入包括三个embedding的求和，分别是:
```
1.Token embedding 表示当前词的embedding
2.Segment Embedding 表示当前词所在句子的index embedding
3.Position Embedding 表示当前词所在位置的index embedding
```
fine-tuning之前对模型的修改非常简单，例如针对sequence-level classification problem(例如情感分析)，取第一个token的输出表示，喂给一个softmax层得到分类结果输出；  
对于token-level classification(例如NER)，取所有token的最后层transformer输出，喂给softmax层做分类。


总结
```
1.BERT采用Masked LM + Next Sentence Prediction作为pre-training tasks, 完成了真正的Bidirectional LM
2.BERT模型能够很容易地Fine-tune，并且效果很好，并且BERT as additional feature效果也很好
3.模型足够泛化，覆盖了足够多的NLP tasks
```

## 四、文本表示方法
下面对文本表示进行一个归纳，也就是对于一篇文本可以如何用数学语言表示呢？
```
基于one-hot、tf-idf、textrank等的bag-of-words；
主题模型：LSA（SVD）、pLSA、LDA；
基于词向量的固定表征：word2vec、fastText、glove
基于词向量的动态表征：elmo、GPT、bert
```

word2vec、fastText：优化效率高，但是基于局部语料；

glove：基于全局语料，结合了LSA和word2vec的优点

### word2vec和fastText对比有什么区别？（word2vec vs fastText）
```
1）都可以无监督学习词向量， fastText训练词向量时会考虑subword；
2） fastText还可以进行有监督学习进行文本分类，其主要特点：
结构与CBOW类似，但学习目标是人工标注的分类结果；
采用hierarchical softmax对输出的分类标签建立哈夫曼树，样本中标签多的类别被分配短的搜寻路径；
引入N-gram，考虑词序特征；
引入subword来处理长词，处理未登陆词问题；
```
