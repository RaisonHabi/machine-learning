## 主成分分析（PCA）、独立成分分析（ICA）和线性判别分析（LDA）
```
PCA只对符合高斯分布的样本点比较有效

ICA对于高斯分布的样本点无效，对于其他分布的有效。
```
```
Pca 主成分分析，无监督映射，投影后方差尽可能大（最大可分）；

Lda 线性判别分析，有监督，投影后组内方差小，组间方差大。

总结为有标签就尽可能利用标签数据（lda），对无监督任务用pca降维
```
```
PCA是无监督的方式，它没有分类标签，降维之后需要采用K-Means或自组织映射网络等无监督的算法进行分类。

LDA是有监督的方式，它先对训练数据进行降维，然后找出一个线性判别函数。
```

**在PCA降维过程中，当进行协方差矩阵上求解特征值时，如果面对维度高达 10000*10000，可想而知耗费的计算量程平方级增长**。  
面对这样一个难点，从而引出奇异值分解(SVD)，利用SVD不仅可以解出PCA的解，而且无需大的计算量。

PCA（主成分分析）和LDA（线性判别分析）有很多的相似点，其本质是要将初始样本映射到维度更低的样本空间中，  
但是PCA和LDA的映射目标不一样：**PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能**。

### 一、PCA步骤
PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。  
这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。
```
1、数据处理，求均值，相减，替换，求方差，替换
2、求特征协方差矩阵
3、求协方差矩阵的特征值和特征向量
4、将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。
5、将样本点投影到选取的特征向量上。
```
### 二、ICA步骤
#### 1、问题 经典的鸡尾酒宴会问题
#### 2、ICA的不确定性
从上面可知，我们只知道一个x，其余两个变量w和s都是不知道的。在没有先验知识的情况下，是无法同时确定这两个相关参数的。

还有，**在已知先验知识的情况下，如果信号服从高斯分布，根据推断，也是不能确定原信号的**。

ICA是盲信号分析领域的一个强有力方法，也是求非高斯分布数据隐含因子的方法。

从之前我们熟悉的样本-特征角度看，我们**使用ICA的前提条件是，认为样本数据由独立非高斯分布的隐含因子产生，隐含因子个数等于特征数**，我们要求的是隐含因子。

### 独立成分分析（Independent Component Analysis）

&nbsp;
## reference
[主成分分析（PCA）、独立成分分析（ICA）和线性判别分析（LDA）](https://www.jianshu.com/p/4f268f21f0ef)   
[独立成分分析（Independent Component Analysis）](https://www.cnblogs.com/jerrylead/archive/2011/04/19/2021071.html)
