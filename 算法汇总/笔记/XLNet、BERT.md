## 一、XLNet:运行机制及和Bert的异同比较
### 1.自回归语言模型（Autoregressive LM）:
**根据上文内容预测下一个可能跟随的单词，或者反过来根据下文预测前面的单词**.
```
GPT 就是典型的自回归语言模型。

ELMO尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归LM，这个跟模型具体怎么实现有关系。
ELMO是做了两个方向（从左到右以及从右到左两个方向的语言模型），但是是分别有两个方向的自回归LM，
然后把LSTM的两个方向的隐节点状态拼接到一起，来体现双向语言模型这个事情的。所以其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。
```
### 2.自回归语言模型有优点有缺点:
#### 缺点:
只能利用上文或者下文的信息，不能同时利用上文和下文的信息，当然，貌似ELMO这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。

#### 优点:
**其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程**。

而Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。


### 3.自编码语言模型（Autoencoder LM）:
自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。  

相比而言，Bert通过在输入X中随机Mask掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，  
如果你对Denoising Autoencoder(去燥自编码)比较熟悉的话，会看出，这确实是典型的DAE的思路。那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM。

### 4.这种DAE LM的优缺点正好和自回归LM反过来
它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。  

缺点是啥呢？**主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的**。DAE吗，就要引入噪音，[Mask] 标记就是引入噪音的手段，这个正常。


### 5.XLNet的出发点就是：能否融合自回归LM和DAE LM两者的优点。
就是说如果站在自回归LM的角度，如何引入和双向语言模型等价的效果；   
如果站在DAE LM的角度看，它本身是融入双向语言模型的，如何抛掉表面的那个[Mask]标记，让预训练和Fine-tuning保持一致。   
当然，XLNet还讲到了一个Bert被Mask单词之间相互独立的问题，我相信这个不太重要，原因后面会说。



XLNet起作用的，如果宏观归纳一下，共有三个因素；
```
1. 与Bert采取De-noising Autoencoder方式不同的新的预训练目标：Permutation Language Model(简称PLM)；
这个可以理解为在自回归LM模式下，如何采取具体手段，来融入双向语言模型。
这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。

2. 引入了Transformer-XL的主要思路：
相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的；

3. 加大增加了预训练阶段使用的数据规模；Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。
XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。
可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。这个明显走的是GPT2.0的路线。
```

(XLNet 则是基于 BERT 的优缺点，提出的一种泛化自回归预训练方法。
它通过最大化因子分解顺序所有排列的期望似然来实现双向上下文的学习；
通过自回归公式克服了 BERT 的局限性，并将来自 Transformer-XL(最先进的自回归模型) 的思想集成到预训练中，在长文本表示的语言任务中表现出了优异的性能。)


&nbsp;
## 二、Transformer-XL
目前在NLP领域中，处理语言建模问题有两种最先进的架构：RNN和Transformer。

RNN按照序列顺序逐个学习输入的单词或字符之间的关系，而Transformer则接收一整段序列，然后使用self-attention机制来学习它们之间的依赖关系。

这两种架构目前来看都取得了令人瞩目的成就，但它们都局限在捕捉长期依赖性上。

### Transformer-XL：
结合了RNN序列建模和Transformer自注意力机制的优点，在输入数据的每个段上使用Transformer的注意力模块，并使用循环机制来学习连续段之间的依赖关系。
```
1.引入循环机制（Recurrence Mechanism）
2.相对位置编码（Relative Positional Encoding）
```
