### 一、随机森林行采样列采样，对特征的操作？
**行采样**使用使用bagging中的Bootstrap aggregating方法。采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那 么采样的样本也为N个。

**列采样**从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树。

特点
```
•随机选取训练样本集
•随机选取分裂属性集
•每棵树任其生长，不进行剪枝
```

&nbsp;
### 二、GBDT的采样是无放回的抽样
### 三、随机森林的构造过程：
[随机森林](https://zhuanlan.zhihu.com/p/22097796)  

N表示训练例子的个数 ，M表示变量的数目。

　　1. 假如有N个样本，则有放回的随机选择N个样本(**每次随机选择一个样本，然后返回继续选择。有放回的方式，可能有重复的样本**)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。

　　2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，**随机从这M个属性中选取出m个属性（无放回的抽样**），满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。

　　3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。

　　4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。
  
#### 对于列采样，采用的方式是按照一定的比例无放回的抽样（相比于行采样的有放回），从M个特征中，选择m个样本（m<M）;

#### 在建立每一棵决策树的过程中，有两点需要注意采样与完全分裂。
首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。  
对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。   
然后进行列采样，从M个feature中，选择m个（m << M）。

之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤——剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。

通过分类，子集合的熵要小于未分类前的状态，这就带来了信息增益（information gain）

