## CNN与RNN对比 
### 相同点：
```
2.1. 传统神经网络的扩展。 
2.2. 前向计算产生结果，反向计算模型更新。 
2.3. 每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。 
```
### 不同点 
```
3.1. CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算 
3.2. RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出 
3.3. CNN高级100+深度，RNN深度有限 
```
存在的问题：RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度，而梯度消失的现象出现时间轴上。 

### 梯度消失梯度爆炸解决：
```
1.修改激活函数；

2.batch normalization:每次SGD，通过mini-batch对相应的activation做规范化操作。均值0，方差1

3.lstm /gru
```

LSTM最核心的部分是cell state，即图中的ct。ct的信息贯穿整个LSTM， 在整个前向传播的过程中只在ct上进行一些简单的线性操作，通过门来控制 ct中信息的增减。 
