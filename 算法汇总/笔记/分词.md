## 一、分词研究：
### 1.基于规则：最大正向、反向、双向匹配法；
### 2.基于统计：基于字标注：
根据字所在词的位置，对每个字打上LL、RR、MM和LR四种标签中的一个。类似于词性标注中的POS(part-of-speech) tags，我们称上述字标签为POC(position-of-character) tags。这样，我们将分词问题转变成对汉字进行序列标注的问题。
字标注本质上是训练出一个字的分类器。学习算法：ME、CRF、SVM、平均感知机等。
### 3.基于深度学习：借助大规模语料，不需要构造额外手工特征，在2014年人民日报语料上取得97.5%的准确率.
BiLSTM+CRF:首先对语料的字进行嵌入(word2vec，字符嵌入的表示可以是纯预训练的，也可以在训练模型的时候再fine-tune，对于fine-tune的情形，可以在字符嵌入后，输入双向LSTM之前加入dropout进一步提升模型效果)，得到字嵌入后，将字嵌入特征输入给双向LSTM，输出层输出深度学习所学习到的特征，并输入给CRF层，得到最终模型.


中文分词是个比较经典的问题，各大互联网公司都会有自己的分词实现。 考虑到性能，可维护性，词库更新，多粒度，以及其他的业务需求，一般工业界中文分词方案都是基于规则。
```
1） 基于规则的常见的就是最大正/反向匹配，以及双向匹配。

2） 规则里糅合一定的统计规则，会采用动态规划计算最大的概率路径的分词
以上说起来很简单，其中还有很多细节，比如词法规则的高效匹配编译，词库的索引结构等。

3） 基于传统机器学习的方法 ，以CRF为主，也有用svm，nn的实现，这类都是基于模型的，跟本文一样，都有个缺陷，
不方便增加用户词典（但可以结合，比如解码的时候force-decode）。 速度上会有损耗。 另外都需要提取特征。
传统CRF一般是定义特征模板，方便性上有所提高。另外传统CRF训练算法(LBFGS)较慢，也有使用sgd的，
但多线程都支持的不好。代表有crf++, crfsuite, crfsgd, wapiti等。
```
深度学习主要是特征学习，端到端训练(end to end:输入是原始数据，输出是最后的结果,缩减人工预处理和后续处理，尽可能使模型从原始输入到最终输出)，适合有大量语料的场景。另外各种工具越来越完善，利用GPU可大幅提高训练速度。


一般在搜索引擎中，构建索引时和查询时会使用不同的分词算法。常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。


### 分词：
基于词典，基于统计（对每个字符进行标注，通过机器学习算法训练分类器进行分词，N-gram，hmm me crf ），基于深度学习（本质是序列标注，有通用性，lstm +Crf bilstm +crf），基于理解（试验阶段，分词的同时进行句法、语义分析）

### 分词工具：jieba snownlp nlpir hanlp

#### 实践：自定义词典，20M左右，一百多万条


## 二、jieba原理：
三种分词模式：精确模式、全模式、搜索引擎模式。  
jieba自带一个dict.txt的词典，放到一个trie树中（前缀树）。
```
1.基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)。

2.采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合。

3.对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。
```


