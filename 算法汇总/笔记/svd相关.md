vsm：Vector Space Model

### M行（文章），N列（词）的矩阵（？M：词、N：文章？）， 
SVD第一个矩阵是对词分类的结果（M行：每行表示一个词，k列：每列表示一个语义相近的词类。各行的每个非零元素表示这个词在每个语义类的重要性）；   
最后一个矩阵是对文本分类的结果(k行：每行对应一个主题，N列：每列对应一篇文本。各列中的每个元素表示这篇文本在不同主题中的相关性)；    
中间的矩阵表示词的类和文章的类之间的相关性。 

### vd相关
LFM（依赖于矩阵分解） 

LSA(LSI)（SVD分解） 

PLSI（EM算法优化，频率学派，参数未知但固定）

LDA（在PLSA基础上加上贝叶斯框架，α,β~dirichlet分布,分别作为主题-文档和词-主题的先验分布；贝叶斯学派的特点是参数是随机变化的，但是服从某个分布，不断的学习新的知识，形成后验）。 

吉布斯采样：用于在难以采样时从某一多变量概率分布中近似抽取样本序列。

贝叶斯参估计的基本过程：先验分布+数据的知识=后验分布。 


### LSA--（概率化）pLSA--（贝叶斯化）LDA--（非参数化）HDP（自动确定topic个数） 
LSA：把高维的文档降到低维空间。将文档表示到潜在语义空间的过程就是SVD奇异值分解和降维的过程。 

LDA它是一种无监督学习算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量k即可。LDA是一种典型的词袋模型。 

LDA：pLSA 加上 topics 的 Dirichlet 先验分布后得到的 Bayesian model，数学上更漂亮。 

pLSA只能对训练样本中进行语义识别，而对不在样本中的文本是无法识别其语义的。而LDA能。 

目前LDA的挑战主要在于长尾分类这块。 
