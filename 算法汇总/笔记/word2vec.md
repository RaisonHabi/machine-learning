## 一、word2vec训练词向量时间：  
用wiki的中文语料训练：用的是gensim，据说比C的版本快。服务器单机跑CPU核心数个线程，跑了35分钟。

tensorflow来训练word2vec比较麻烦，生成batch、定义神经网络的各种参数，都要自己做，但是对于理解算法原理有帮助：
```
1.读取文本数据，分词，清洗，生成符合输入格式的内容；
2.建立词汇表；
3.为skip-gram模型生成训练的batch；
4.定义和训练skip-gram模型；
5.词向量可视化。
```
### 文本分布式表示（二）：用tensorflow和word2vec训练词向量
预训练的词向量1.3G；新闻文本一两百M，训练出来七百多M。tf训练七八个小时左右

&nbsp;
## 二、W2v损失函数、负采样：
Nce-loss:先计算正负样本对应的输出和标签，sigmoid_cross_entropy_with_logits通过sigmoid cross entropy计算输出和标签的损失，从而进行反向传播。

**不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一部分的权重，这样就好降低梯度下降过程中的计算量**。   
当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。

当通过（”fox”, “quick”)词对来训练神经网络时，我们回想起这个神经网络的“标签”或者是“正确的输出”是一个one-hot向量。   
也就是说，对于神经网络中对应于”quick”这个单词的神经元对应为1，而其他上千个的输出神经元则对应为0。  
使用负采样，我们通过随机选择一个较少数目（比如说5个）的“负”样本来更新对应的权重。(在这个条件下，“负”单词就是我们希望神经网络输出为0的神经元对应的单词）。并且我们仍然为我们的“正”单词更新对应的权重（也就是当前样本下”quick”对应的神经元）。

论文说选择5-20个单词对于较小的样本比较合适，而对于大样本，我们可以选择2~5个单词。   
从**本质上来说，选择一个单词来作为负样本的概率取决于它出现频率，对于更经常出现的单词，我们将更倾向于选择它为负样本**。

&nbsp;
## 三、W2v缺点
context 很小，没有使用全局的cooccur，所以实际上对cooccur的利用很少   
未解决一词多义问题
