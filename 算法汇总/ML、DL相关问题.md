### 0.拉普拉斯平滑处理 Laplace Smoothing
拉普拉斯平滑（Laplace Smoothing）又被称为加 1 平滑，是比较常用的平滑方法。平滑方法的存在是为了解决零概率问题。
#### 背景:为什么要做平滑处理?
  零概率问题，就是在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0。在文本分类的问题中，当一个词语没有在训练样本中出现，该词语调概率为0，使用连乘计算文本出现概率时也为0。这是不合理的，不能因为一个事件没有观察到就武断的认为该事件的概率是0。
####  拉普拉斯的理论支撑
假定训练样本很大时，每个分量x的计数加1造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。

#### 总结：分子加一，分母加K，K代表类别数目。
在实际的使用中也经常使用加 lambda（1≥lambda≥0）来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。

[平滑处理--拉普拉斯（Laplace Smoothing）](https://dcpnonstop.github.io/2017/11/24/%E5%B9%B3%E6%BB%91%E5%A4%84%E7%90%86-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%EF%BC%88laplace%EF%BC%89/)

### 1、退化问题：
CNN网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。因为随机梯度是局部最优，不是全局最优。

#### 怎么解决退化问题？
**深度残差网络**。  
如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那现在要解决的就是学习恒等映射函数了。   
但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。   
但如果把网络设计为H(x) = F(x) + x,如下图。我们可以转换为学习一个残差函数F(x) = H(x) - x. 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。

### 1.自动学习特征
```
深度学习与传统模式识别方法的最大不同在于它是从大数据中自动学习特征。 
而非采用手工设计的特征。 
传统的模式识别系统依赖先验知识，需要手工调整参数，因此参数不易过多。然而深度学习模型可以包含千万级别的参数。 
深度学习的关键就是通过多层非线性映射将图像中复杂的因素成功地分开。 
```
机器学习和深度学习的区别？   
机器学习在训练模型之前，需要手动设置特征，即需要做特征工程；深度学习可自动提取特征；所以深度学习自动提取的特征比机器学习手动设置的特征鲁棒性更好； 


### 2.深度学习训练时网络不收敛的原因有哪些？如何解决？ 
答：不收敛一般都是数据不干净，学习率设置不合理（可以从0.1开始尝试，如果loss不下降的意思，那就降低，除以10，用0.01尝试，一般来说0.01会收敛，不行的话就用0.001. 学习率设置过大，很容易震荡。），网络（加深当前网络），数据归一化等问题 

### 3.数据集降维的好处可以是:
```
(1)减少所需的存储空间。 
(2)加快计算速度(例如在机器学习算法中)，更少的维数意味着更少的计算，并且更少的维数可以允许使用不适合大量维数的算法。 
(3)将数据的维数降低到2D或3D可以允许我们绘制和可视化它，可能观察模式，给我们提供直观感受。 
(4)太多的特征或太复杂的模型可以导致过拟合 
```

### 4.如何解决梯度爆炸与消失。 
[神经网络训练中的梯度消失与梯度爆炸 - PENG的文章 - 知乎](https://zhuanlan.zhihu.com/p/25631496)   

可见，<img src="https://www.zhihu.com/equation?tex=%5Csigma%27%5Cleft%28x%5Cright%29" alt="\sigma&#39;\left(x\right)" eeimg="1"/>的最大值为<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B4%7D" alt="\frac{1}{4}" eeimg="1"/>，而我们初始化的网络权值<img src="https://www.zhihu.com/equation?tex=%7Cw%7C" alt="|w|" eeimg="1"/>通常都小于1，因此<img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%5Cleq%5Cfrac%7B1%7D%7B4%7D" alt="|\sigma&#39;\left(z\right)w|\leq\frac{1}{4}" eeimg="1"/>，因此对于上面的链式求导，层数越多，求导结果<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b_1%7D" alt="\frac{\partial C}{\partial b_1}" eeimg="1"/>越小，因而导致梯度消失的情况出现。</p><p>这样，梯度爆炸问题的出现原因就显而易见了，即<img src="https://www.zhihu.com/equation?tex=%7C%5Csigma%27%5Cleft%28z%5Cright%29w%7C%3E1" alt="|\sigma&#39;\left(z\right)w|&gt;1" eeimg="1"/>，也就是<img src="https://www.zhihu.com/equation?tex=w" alt="w" eeimg="1"/>比较大的情况。但对于使用sigmoid激活函数来说，这种情况比较少。因为<img src="https://www.zhihu.com/equation?tex=%5Csigma%27%5Cleft%28z%5Cright%29" alt="\sigma&#39;\left(z\right)" eeimg="1"/>的大小也与<img src="https://www.zhihu.com/equation?tex=w" alt="w" eeimg="1"/>有关（<img src="https://www.zhihu.com/equation?tex=z%3Dwx%2Bb" alt="z=wx+b" eeimg="1"/>），除非该层的输入值<img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"/>在一直一个比较小的范围内。</p><p>其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。</p>

```
-答：1、预训练加微调 - 梯度剪切、权重正则（针对梯度爆炸） - 
2、使用不同的激活函数 - 
3、使用batchnorm - 
4、使用残差结构 - 
5、使用LSTM网络 
```
### 5.什么是凸集、凸函数、凸学习问题？ 
```
凸集：若对集合C中任意两点u和v，连接他们的线段仍在集合C中，那么集合C是凸集。 
公式表示为：αu+(1-α)v∈C α∈[0, 1] 

凸函数：凸集上的函数是凸函数。凸函数的每一个局部极小值也是全局极小值 
```
### 6.范数
```
L0范数：计算向量中非0元素的个数。 
L0范数和L1范数目的是使参数稀疏化。L1范数比L0范数容易优化求解。 
L2范数是防止过拟合，提高模型的泛化性能。 
```
### 7.牛顿法
牛顿法的最初提出是用来求解方程的根的   
牛顿法求最优值的步骤如下：   
```
1. 随机选取起始点x0； 
2. 计算目标函数f(x)在该点xk的一阶导数和海森矩阵； 
3. 依据迭代公式xk+1=xk−H−1kf′kxk+1=xk−Hk−1fk′更新x值 
如果E(f(xk+1)−f(xk))<ϵE(f(xk+1)−f(xk))<ϵ，则收敛返回，否则继续步骤2,3直至收敛 
```
我们可以看到，**当我们的特征特别多的时候，求海森矩阵的逆的运算量是非常大且慢的**，这对于在实际应用中是不可忍受的，    
因此我们想能否**用一个矩阵来代替海森矩阵的逆呢，这就是拟牛顿法的基本思路**。 

### 8.梯度下降法和牛顿法的优缺点？
```
优点：
梯度下降法：可用于数据量较大的情况； 
牛顿法：收敛速度更快； 

缺点：
梯度下降法：每一步可能不是向着最优解的方向； 
牛顿法：每次迭代的时间长；需要计算一阶和二阶导数； 
```
### 9.解决训练样本类别不平衡问题？ 
现象：训练样本中，正负样本数量的比例较大。
```
1. 过采样。增加正例样本数量，使得正负样本数量接近，然后再进行学习。 
2. 欠采样。去除反例样本数量，使得正负样本数量接近，然后再进行学习。 
3. 设置阈值。基于原始数据集学习，当使用已训练好的分类器进行预测时，将正负样本数量的比例作为阈值嵌入到决策过程中。 
```
### 10.如何解决数据不平衡问题？ 
```
答：1、利用重采样中的下采样和上采样，对小数据类别采用上采样，通过复制来增加数据，不过这种情况容易出现过拟合，
建议用数据扩增的方法，对原有数据集进行翻转，旋转，平移，尺度拉伸，对比度，亮度，色彩变化来增加数据。
对大数据类别剔除一些样本量。数据增强具体代码：常用数据增强方法代码 

2、组合不同的重采样数据集：
假设建立十个模型，选取小数据类1000个数据样本，然后将大数据类别10000个数据样本分为十份，每份为1000个，并训练十个不同的模型。 

3、更改分类器评价指标： 
在传统的分类方法中，准确率是常用的指标。 
然而在不平衡数据分类中，准确率不再是恰当的指标，采用精准率即查准率P：真正例除以真正例与假正例之和。
召回率即查全率F。真正例除以真正例与假反例之和。或者F1分数查全率和查准率加权平衡=2*P*R/(P+R)。 
```
### 11.如何改善训练模型的效果呢？ 
```
答：1、通过提升数据，获取良好的数据。对数据预处理；零均值1方差化，数据扩充或者增强， 
2、诊断网络是否过拟合欠拟合。通过偏差方差。正则化解决过拟合，早停法遏制过拟合。 
3、通过学习率，激活函数的选择，改善网络全连接层个数啊层数啊，优化算法，随机梯度，RMSprop,动量，adam，使用batchnormlization. 
4、权值初始化Xavier初始化，保持输入与输出端方差一致，避免了所有输出都趋向于0； 
```
### 12.各个激活函数的优缺点？ 
```
Sigmoid激活函数 缺点： 
1. 不是关于原点对称； 
2. 需要计算exp 

Tanh 激活函数 优点： 
1. 关于原点对称 
2. 比sigmoid梯度更新更快 

ReLU激活函数 优点： 
1. 神经元输出为正时，没有饱和区 
2. 计算复杂度低，效率高 
3. 在实际应用中，比sigmoid、tanh更新更快 
4. 相比于sigmoid更加符合生物特性 

ReLU激活函数 缺点： 
1. 神经元输出为负时，进入了饱和区 
2. 神经元的输出在非0中心 
3. 使得数据存在Active ReLU、Dead ReLU(当wx+b<0时，将永远无法进行权值更新，此时的神经元将死掉)的问题 
```
### 13.正则化的理解：
正则化是在损失函数中加入对模型参数的惩罚项，以平衡因子lamda控制惩罚力度，其通过在训练过程中降低参数的数量级，从而降低模型的过拟合现象。 

从贝叶斯的角度来看，正则化等价于对模型参数引入先验分布：对参数引入高斯先验分布等价于L2正则化，对参数引入拉普拉斯分布等价于L1正则化。 
### 14.逻辑回归和SVM的区别和联系 
```
1.损失函数不同,LR损失函数是对数损失；SVM损失函数时合页损失； 
2.LR考虑了所有点的损失，但通过非线性操作大大减小离超平面较远点的权重；SVM仅考虑支持向量的损失 
3.LR受类别平衡的影响；SVM则不受类别平衡的影响； 
4.LR适合较大数据集；SVM适合较小数据集
```
朴素贝叶斯的朴素是什么意思？ 
朴素指的是各个特征之间相互独立。 
### 15.神经网络的优缺点？ 
```
优点: 
1.拟合复杂的函数 
随着神经网络层数的加深，网络的非线性程度越来越高，从而可拟合更加复杂的函数； 
2.结构灵活 
神经网络的结构可根据具体的任务进行相应的调整，选择适合的网络结构； 
3.神经网络可自动提取特征，比人工设置的特征鲁棒性更好； 

缺点： 
1.由于神经网络强大的假设空间，使得神经网络极易陷入局部最优，使得模型的泛化能力较差； 
2.当网络层数深时，神经网络在训练过程中容易产生梯度消失和梯度下降的问题； 
3.随着网络层数的加深，神经网络收敛速度越来越慢； 
4.神经网络训练参数多，占用内存大； 
```
### 16.随机森林的随机性指的是？ 
行采样、列采样
```
1.决策树训练样本是有放回随机采样的； 
2.决策树节点分裂特征集是随机采样的； 
```

&nbsp;
## 图像和视频
在于图像和视频相关的应用中，最成功的是深度卷积网络，它正是利用了与图像的特殊结构。其中最重要的两个操作，卷积和池化（pooling）都来自于与图像相关的领域知识。 

CNN池化层有什么作用？ 
1、减小图像尺寸，数据降维。 
2、缓解过拟合。 
3、保持一定程度的旋转和平移不变性 

1*1卷积作用。 
答：1. 实现跨通道的交互和信息整合 
2. 进行卷积核通道数的降维和升维 
3、实现多个feature map的线性组合，实现通道个数的变换。 
4、对特征图进行一个比例缩放。 


评价指标有哪些？ 
答、机器学习中评价指标： Accuracy（准确率）、 Precision（查准率或者精准率）、Recall（查全率或者召回率）、F1。 


为什么要使用许多小卷积核(如3x 3 )而不是几个大卷积核？ 
这在VGGNet的原始论文中得到了很好的解释。 
原因有二：首先，您可以使用几个较小的核而不是几个较大的核来获得相同的感受野并捕获更多的空间上下文，但是使用较小的内核时，您使用的参数和计算量较少。 
其次，因为使用更小的核，您将使用更多的滤波器，您将能够使用更多的激活函数，从而使您的CNN学习到更具区分性的映射函数。 


卷积层中感受野大小的计算？ 
与其之前层的卷积核尺寸和步长有关，与padding无关。基于从深层向浅层递归计算的方式。 
计算公式为：Fj-1 = Kj + (Fj - 1)*Sj(最后一层特征图的感受野大小是其计算卷积核大小) 


深度学习要这么深？ 
答：1、一个直观的解释，从模型复杂度角度。如果我们能够增强一个学习模型的复杂度，那么它的学习能力能够提升。如何增加神经网络的复杂度呢？要么变宽，即增加隐层网络神经元的个数；要么变深，即增加隐层的层数。当变宽的时候，只不过是增加了一些计算单元，增加了函数的个数，在变深的时候不仅增加了个数，还增加了函数间的嵌入的程度。 
2、深度学习可以通过多个layer的转换学习更高维度的特征来解决更加复杂的任务。 
3、那现在我们为什么可以用这样的模型？有很多因素，第一我们有了更大的数据；第二我们有强力的计算设备；第三我们有很多有效的训练技巧 
4、像在ZFNet网络中已经体现，特征间存在层次性，层次更深，特征不变性强，类别分类能力越强，要学习复杂的任务需要更深的网络 


等距变换：图像旋转+平移 
相似变换：图像旋转+平移+缩放(放大或缩小原图) 
仿射变换：图像旋转+平移+缩放+切变(虽改变图像的形状，但未改变图像中的平行线) 
投影变换：图像旋转+平移+缩放+切变+射影(不仅改变了图像的形状，而且改变了图像中的平行线) 
 

常用的池化操作有哪些？ 
1.Max pooling:选取滑动窗口的最大值 
2.Average pooling：平均滑动串口的所有值 
3.Global average pooling：平均每页特征图的所有值 
优点： 
1.解决全连接层所造成的过拟合问题 
CNN网络需要将特征图reshape成全连接层，然后再连接输出层，而global average pooling不需要此操作，直接将特征图pooling成输出层 
2.没有权重参数 



1*1卷积核的作用？ 
1.跨通道信息的融合； 
2.通过对通道数的降维和升维，减少计算量； 


卷积层和全连接层的区别？ 
1.卷积层是局部连接，所以提取的是局部信息；全连接层是全局连接，所以提取的是全局信息； 
2.当卷积层的局部连接是全局连接时，全连接层是卷积层的特例； 



针对模糊图像的处理主要分两条路：一种是自我激发型，另外一种属于外部学习型。 
1.自我激发型： 
1）图像增强：不考虑图像质量下降的原因，只是选择地突出图像中感兴趣的特征，抑制其它不需要的特征，主要目的就是提高图像的视觉效果； 
图像增强中常见的几种具体处理方法为：直方图均衡、灰度变换、图像平滑、图像锐化 
2）图像复原： 
图像复原常用二种方法。 
当不知道图像本身的性质时，可以建立退化源的数学模型，然后施行复原算法除去或减少退化源的影响；当有了关于图像本身的先验知识时，可以建立原始图像的模型，然后在观测到的退化图像中通过检测原始图像而复原图像。 
3）图像超分辨率： 
根据多幅低质量的图片间的关系以及一些先验知识来重构一个高分辨的图片 

2.外部学习型 
外部学习型，就如同照葫芦画瓢一样的道理。 
其算法主要是深度学习中的卷积神经网络，我们在待处理信息量不可扩充的前提下（即模糊的图像本身就未包含场景中的细节信息），可以借助海量的同类数据或相似数据训练一个神经网络，然后让神经网络获得对图像内容进行理解、判断和预测的功能，这时候，再把待处理的模糊图像输入，神经网络就会自动为其添加细节，尽管这种添加仅仅是一种概率层面的预测，并非一定准确。 

彩色图像转换为黑白图像极其简单，属于有损压缩数据；反之则很难，因为数据不会凭空增多。 
　　搭建一个神经网络，给一张黑白图像，然后提供大量与其相同年代的彩色图像作为训练数据（色调比较接近），然后输入黑白图像，人工智能按照之前的训练结果为其上色，输出彩色图像
