## 背景
基于风险预测模型的预后研究一直以来都是研究者关注的热点，各种各样的预测模型质量参差不齐，常常让人眼花缭乱，那么如何去评价一个模型的好坏，或者说当你构建出一个疾病风险预测模型后，它到底靠不靠谱，值不值得去推广和使用呢？这是一个我们需要去好好考量的问题。

&nbsp;

一个好的疾病风险预测模型，它不只是简单的因变量和自变量的数学组合，它背后的实际临床意义才是我们所要把握的重点，这就要求预测模型不仅要有很好的**区分度（Discrimination），同时还要具备良好的校准度（Calibration）**。

&nbsp;

Discrimination和Calibration是我们在评价预测模型时最常用到的一对指标，但是2015年Circ Cardiovasc Qual Outcomes杂志（影响因子：4.5）上发表的一项关注心血管疾病预测模型的系统综述发现，63%的研究报告了预测模型的Discrimination信息，但仅36%的研究报告了Calibration信息，使得预测模型的质量成为研究泛滥的重灾区。

&nbsp;
本期内容我们就来向大家介绍一下这两个重要的指标，尤其是常常被人忽略的Calibration。

## 区分度(Discrimination)
介绍Calibration之前，我们先简单介绍一下Discrimination。顾名思义，一个好的疾病风险预测模型，它能够把未来发病风险高、低不同的人群正确地区分开来，预测模型通过设置一定的风险界值，高于界值判断为发病，低于界值则判断为不发病，从而正确区分个体是否会发生结局事件，这就是预测模型的区分度(Discrimination)。

&nbsp;

**评价预测模型区分能力的指标，最常用的就是大家非常熟悉的ROC曲线下面积（AUC），也叫C统计量（C-statistics）**。AUC越大，说明预测模型的判别区分能力越好。一般AUC<0.6认为区分度较差，0.6-0.75认为模型有一定的区分能力，>0.75认为区分能力较好。

&nbsp;

## reference
[Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good Probabilities With Supervised Learning,](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf)  
[你的预测模型靠谱吗？详解区分度和校准度的SPSS操作！](https://www.mediecogroup.com/method_topic_article_detail/230/?ty=methods)  
[专题全面了解诊断试验及ROC](https://www.mediecogroup.com/method_topic_detail/10/1/)  
[模型校准calibration](https://zhuanlan.zhihu.com/p/101766505)   
[概率校准 calibration_curve](https://zhuanlan.zhihu.com/p/90479183)  
[sklearn.calibration.calibration_curve](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html)  
[概率校准(Probability calibration)](https://www.studyai.cn/modules/calibration.html)  
[模型校准calibration ](https://www.zhihu.com/search?type=content&q=%E6%A8%A1%E5%9E%8B%E6%A0%A1%E5%87%86)
