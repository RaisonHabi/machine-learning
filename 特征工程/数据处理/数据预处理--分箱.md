## 变量分箱（即变量离散化）：  
分箱就是将**连续变量离散化**、**将多状态的离散变量合并成少状态**。

&nbsp;
## 三类变量分箱方法：
### 1.无序变量分箱 
“一位有效编码” （one-hot Encoding），通常叫做虚变量或者哑变量（dummpy variable）。  
在模型中引入多个虚拟变量时，虚拟变量的个数应按下列原则确定：  
a)回归模型有截距：一般的，若该特征下n个属性均互斥（如，男/女;儿童/青年/中年/老年），在生成虚拟变量时，应该生成 n-1个虚变量，这样**可以避免产生多重共线性**。  
b)回归模型无截距项：有n个特征，设置n个虚拟变量  

### 2.有序变量分箱 
在变量中有多个可能会出现的取值，各取值之间还存在等级关系。  
使用 pandas 中的 map（）替换相应变量就行。  

### 3.连续变量的分箱方式:无监督分组、有监督分组 
a）常用的无监督分箱方法有等频分箱，等距分箱和聚类分箱。  
b）有监督分箱主要有best-ks分箱和卡方分箱。  

### 卡方分箱的原理  
卡方分箱是自底向上的(即基于合并的)数据离散化方法。  
它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。   
基本思想:对于精确的离散化，相对类频率在一个区间内应当完全一致。  
因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；  
否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。   

&nbsp;
## 分箱的重要性及其优势
```
一般在建立分类模型时，需要对连续变量离散化，特征离散化后，模型会更稳定，降低了模型过拟合的风险。  
比如在建立申请评分卡模型时用logsitic作为基模型就需要对连续变量进行离散化，离散化通常采用分箱法。
```
**分箱的有以下重要性及其优势：**  
 > 1.离散特征的增加和减少都很容易，易于模型的快速迭代；  
2.稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；  
3.离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；  
4.逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；  
5.离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；    
6.特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；  
7.特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。  
8.可以将缺失作为独立的一类带入模型。  
9.将所有变量变换到相似的尺度上。

[机器学习（十六）特征工程之数据分箱](https://www.jianshu.com/p/0805f185ecdf)
&nbsp;
## 注意问题 
对于分箱需要注意的是，分完箱之后，某些箱区间里，bad或者good分布比例极不均匀，**极端时会出现bad或者good数量直接为0。那么这样子会直接导致后续计算WOE时出现inf无穷大的情况，这是不合理的**。这种情况，说明分箱太细，需要进一步缩小分箱的数量。  

&nbsp;
## reference
[数据预处理——数据分箱](https://zhuanlan.zhihu.com/p/52312186)  
[特征工程之分箱--卡方分箱](https://www.cnblogs.com/wqbin/p/10547167.html)  
[评分卡入门与创建原则——分箱、WOE、IV、分值分配](https://blog.csdn.net/sscc_learning/article/details/78591210)

