## label encoding和one hot encoding
这两种方法在不同的模型和数据集上有不同意义。  
注：这里只谈分类模型。
### 1、两类模型
虽然大多数模型要求输入项是数值型变量，但其对数值的处理方式是完全不同的。 

**有些模型的损失函数对数值大小是敏感的，即变量间的数值大小本身是有比较意义的，如逻辑回归，SVM**等，我们暂将其称为A类模型；  

**有些模型本身对数值变化不敏感，数值存在的意义更多的是为了排序**，即0.1,0.2,0.3与1,2,3是没有区别的，这部分模型绝大部分是树模型，暂将其称为B类模型。
### 2、两类类别变量
类别变量再细分可以分成有序变量和无序变量，  
典型的有序变量就是学历，如博士研究生，硕士研究生，本科生等在业务含义上本身是有高低之分的。  
而无序变量在业务含义上是无序的，如品牌等。 
### 3、label encoding与one hot encoding
label encoding是将类别变量中每一类别赋一数值，从而转换成数值型。  
比如有一列 [dog,cat,dog,mouse,cat]，我们把其转换为[1,2,1,3,2]。  
这里就产生了一个奇怪的现象：dog和mouse的平均值是cat，**所以label encoding最直观的缺点就是赋值难以解释，适用场景更窄**。

one hot encoding的优点就是它的值只有0/1，不同的类型存储在垂直的空间。**缺点就是，当类别的数量很多时，特征空间会变得非常大**。
### 4、两类encoding的使用场景
#### A类模型：  
如果使用的是A类模型，所有的类别变量必须做one hot encoding，因为label encoding的赋值是没有数值含义的。 

但是对于类别很多的变量，做one hot encoding会使得生成的变量过于稀疏，所以这里有一些经验上的处理方式。  
优先考虑的是有没**业务上类别合并的方法**，如城市变量，可以依发展程度分为一线城市，二线城市等等；  
另外一种方法是只one hot出现次数最多的前n个类别，其他类别放在其他类的变量中；  
也可以利用y值（训练集）中positive rate做合并，不过容易出现过拟合的现象。

#### B类模型
如果使用的是B类模型（树模型），并且是有序变量，则优先使用label encoding，且赋值要与顺序一致。

如果是无序变量，则两种方法在很多情况下差别不大，但是在实际使用中label encoding的效果一般要比one hot encoding要好。  
这是因为**在树模型中，label encoding至少可以完成one hot encoding同样的效果，而多出来的那部分信息则是label encoding后的数值本身是有排序作用的，它可以起到类别变量合并的效果，这种效果在类别较多的变量中更明显**。

举个例子，在树模型中，有四个类别，dog,cat,mouse,horse，如果dog类的占比……

但是在实际使用中，由于one hot encoding增加了变量的维度，在树模型中意味着更深的分裂，如果刚好树的深度限制在一数值时，dog这个类别可能不能入模型，其实意味着丢失了dog这一信息，  
而上面第二种label encoding没有这种问题，也意味着拟合的效果会更好，但是因为利用到了y值的信息，也容易发生过拟合。

在Applied Predictive Modeling p372-p377讨论了一方面的问题，有兴趣的可以去翻阅，但是其中的Grouped Category与label encoding还是不完全一样的。

&nbsp;
## reference
[机器学习-Label Encoding与One Hot的区别-20180513](https://zhuanlan.zhihu.com/p/36804348)
