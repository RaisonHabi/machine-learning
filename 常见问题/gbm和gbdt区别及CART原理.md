## 一、gbm和gbdt的区别
GBM (Gradient Boosting Machine) 和 GBDT (Gradient Boosting Decision Tree) 实际上非常相似，以至于在很多文献和实践中它们经常被交替使用。但是，从技术细节上看，GBDT 是 GBM 的一种特例，其中 GBM 是一个更通用的概念。

下面是两者的主要区别：
#### 1、概念上的区别:
GBM: 是一个更广泛的框架，用来描述一类通过梯度提升方法构建的模型。它可以使用不同的基学习器（base learner），如决策树、线性模型等。  
GBDT: 是 GBM 的一个实例，特别指使用 CART (Classification and Regression Trees) 作为基学习器的梯度提升模型。
#### 2、基学习器:
GBM: 可以使用多种类型的基学习器，例如线性模型、决策树等。  
GBDT: 使用 CART 决策树作为基学习器。
#### 3、训练过程:
GBM: 在每次迭代过程中，通过梯度提升方法拟合一个新的基学习器来逼近残差（即当前模型预测与实际目标之间的差异）。这些基学习器被串联起来形成一个强学习器。  
GBDT: 同样通过梯度提升来训练 CART 决策树，每棵树都试图修正前一棵树的错误。

尽管有这些区别，但通常情况下人们提到 GBM 时，他们可能是指使用决策树作为基学习器的情况，也就是 GBDT。在实践中，这两个术语经常互换使用，尤其是在提到使用决策树作为基学习器的情况下。

总结来说：  
如果提到 GBM，通常是指一个广义的梯度提升框架，可以使用多种基学习器。
如果提到 GBDT，则明确指出使用的是 CART 决策树作为基学习器的 GBM 版本。
在大多数情况下，当我们讨论梯度提升的决策树模型时，我们可以认为 GBM 和 GBDT 在概念上是一致的。

## 二、CART 原理
CART（Classification and Regression Trees）是一种决策树算法，可以用于分类和回归任务。CART 算法使用二叉树结构，每个内部节点表示一个特征上的判断条件，每个叶节点表示一个输出值（对于分类任务是类别，对于回归任务是数值）。

下面是 CART 算法的基本原理：
### 分类树（Classification Tree）
#### 1、特征选择:
在每个节点上，选择最佳的特征和阈值来分裂数据集，以最大化纯度（减少不确定性）。  
通常使用基尼指数（Gini Impurity）或信息增益（基于熵的信息增益）作为评估分裂质量的标准。
#### 2、分裂规则:
CART 使用二叉分裂，这意味着每个节点都将数据集分成两部分。  
分裂的选择是基于特征的某个阈值来进行的。
#### 3、递归构建:
递归地对每个子节点应用相同的分裂过程，直到满足停止条件（例如，达到最大深度，节点中的样本数量小于某个阈值等）。
#### 4、剪枝:
为了防止过拟合，可以通过剪枝来简化决策树。  
剪枝可以是预剪枝（在构建过程中提前停止）或后剪枝（构建完成后移除某些分支）。
#### 5、分类:
对于分类任务，叶节点通常表示一个类别，新样本沿着树向下移动直到到达叶节点，并被分类为该叶节点所代表的类别。
### 回归树（Regression Tree）
#### 1、特征选择:
类似于分类树，但在回归树中，分裂的目的是最小化平方误差。
#### 2、分裂规则:
使用特征值来分裂数据，目标是最小化每个子节点内的平方误差。
#### 3、递归构建:
递归地构建树，直到达到停止条件。
#### 4、剪枝:
与分类树类似，可以通过剪枝来简化模型。
#### 5、预测:
对于回归任务，叶节点通常包含一个数值，表示该叶节点内样本的目标值的平均值。  
新样本沿着树向下移动直到到达叶节点，并被赋予该叶节点的数值作为预测值。
### CART 的关键特性
#### 1、二叉树结构:
CART 构建的决策树总是二叉树，即使特征可能有多个可能的取值，也会被分割成两个子集。
#### 2、特征选择准则:
对于分类树，通常使用基尼指数来选择最优的分裂特征；对于回归树，通常使用平方误差来选择最优分裂。
#### 3、递归分割:
CART 采用递归方式构建决策树，从根节点开始，一直分裂到叶节点。
#### 4、剪枝技术:
通过剪枝技术减少过拟合的风险，提高泛化能力。
### 示例
假设我们要构建一个电子邮件分类器来区分垃圾邮件和正常邮件，CART 算法的过程如下：
#### 1、特征选择:
观察电子邮件的特征（如发件人、主题、邮件内容中的关键词等）。  
计算所有可能的二分类的基尼系数增量，以确定最佳的分裂特征。
#### 2、构建决策树:
使用选定的特征分裂数据集。  
递归地构建决策树，直到满足停止条件。
#### 3、剪枝:
删除决策树中一些不必要或过于复杂的节点，以防止过拟合。
#### 4、分类:
使用剪枝后的决策树对新收到的电子邮件进行分类。
