## 导语
经过实践检验后，我们发现xgb+lr相比xgb几乎没有提升。  
xgb不像DNN，其本身的特征交叉能力是有限的，用xgb作为lr的特征交叉部分是远远不够的，所以还是需要在LR这边做大量的人工特征交叉设计，我们当时并没有一个实践检验过很好的单LR模型，因为我们没有时间和人力做精细的人工特征交叉设计。

其实当人工的特征设计足够优秀的时候，特征维度很丰满的单LR已经很强了，xgb那点叶子结点的影响力其实不大。  
一个好的单LR模型其实根本不太需要xgb这点交叉，一个单xgb模型本身也足够优秀，级联一个垃圾LR不如不级联。   
Facebook在2014年的思路是没错的，xgb能够很好的处理连续性型特征，LR来补齐xgb对离散类特征信息的盲区。但是工程上如果想有提升，不可避免还是要做特征工程，要踩得坑太多而且收益甚微，有这个精力不如搞深度学习。

想要有质变的话建议直接上深度学习，Wide&Deep用Deep部分代替xgb，DNN的特征交叉能力和信息量要比xgb大得多。

&nbsp;
## 一、基础知识
### 1.关于xgboost和GBD
集成思想主要分为两大流派，Boosting一族通过将弱学习器提升为强学习器的集成方法来提高预测精度；  
而另一类则为Bagging，生成众多并行式的分类器，通过“少数服从多数”的原则来确定最终的结果，典型算法为随机森林。

GBDT（Gradient Boost Decision Tree）和xgboost都是基于boosting思想与树模型进行有效结合产生的算法。每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型，所以每次迭代都在减少残差的梯度方向新建立一颗树，迭代多少次就会生成多少颗树。关于xgboost和GBDT的区别，此处不再赘述

**那无论是xgboost还是GBDT，其自身原理决定该模型可以发现区分度更高的特征，而从根节点到叶子节点的这个过程，其实就是一种特征筛选和特征组合的过程。所以该模型本身就是一种特征工程**,这也是为何大家会把它作为上一级并与LR级联的原因。

另外需要注意的是，这类树模型使用过程中最好不要混合使用连续型特征和离散型特征，因为一个离散型特征占据多维，而连续型特征只占一维，这样对于连续型特征是不公平的。导致训练出来的模型存在偏差。建议这类树模型全部使用连续型特征，离散型特征可通过内积等方式转换为连续型。
但如果你有需要，可以专门对离散化数据训练一个xgboost或gbdt模型，把连续型特征也进行离散化（如人为分桶），全部使用离散型特征进行训练。但如果不是为了级联，建议直接换其他模型。

### 2.关于LR
逻辑回归（LR,Logistic Regression）是传统机器学习中的一种分类模型，由于LR算法具有简单、高效、易于并行且在线学习（动态扩展）的特点，在点击率预估、情感分类等二分类场景，用户等级分类等多分类场景都适用。

LR是广义线性模型，能够并行化处理大量亿万级特征的训练样本。LR模型是只使用离散型特征的，连续型特征不是不能用，而是用了以后效果不好，对模型有负影响。LR快速高效，完全可以处理高纬度稀疏的离散化特征。

&nbsp;
## 二、级联方案探索和思考
看了Facebook的论文，LR的训练数据全部来自于树模型的叶子节点输出，会不会太少了？  
如果LR加入了除树模型输出的特征以外的离散化特征，这些特征是什么怎么选？  
还要做特征交叉吗，会不会和树模型的结果“重复”呢？

那大概整理下思路，也就出现了以下几种方案：
```
方案一：将树模型输出结果作为LR的输入（和Facebook论文中图片一样）。

方案二：将原始连续型特征和树模型输出结果进行拼接作为LR的输入。

方案三：将原始连续型特征转换为离散型特征，与树模型输出结果进行拼接作为LR的输入。

方案四：将原始连续型特征转换为离散型特征并做特征工程，再与树模型输出结果进行拼接作为LR的输入。
```

方案一感觉损失了部分信息。  
方案二，刚刚上面说过，连续型特征对LR来说非常不友好，其实方案二可以直接毙掉，但是因为代码写起来很简单，我想看看是什么结果。  
方案三和方案四是正解，看了一些文章，并和大佬交流后，也表示这是业界采用的方案。  
当然方案四其中还涉及非常多的细节，主要就是针对LR特征工程这一块，因为树模型的“简陋”特征工程并不能完全替代LR的特征工程。

&nbsp;
## reference
[【CTR预估】xgboost和LR模型级联](https://tech.rstalker.com/algorithm/xgb_lr.html)
