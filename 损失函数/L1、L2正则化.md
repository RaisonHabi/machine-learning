## 一、正则化（Regularization）
### 1、结构风险最小化=经验风险最小化+正则化
正则化是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。

在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，可以有效提高泛化预测精度。  
如果模型过于复杂，变量值稍微有点变动，就会引起预测精度问题。  
正则化之所以有效，就是因为其降低了特征的权重，使得模型更为简单。

### 2、常用正则项
机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 ℓ1-norm 和 ℓ2-norm，中文称作 L1正则化 和 L2正则化，或者 L1范数 和 L2范数。
L1正则化和L2正则化可以看做是损失函数的惩罚项。  

所谓『惩罚』是指对损失函数中的某些参数做一些限制。  

### 3、对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）  

&nbsp;
## 二、L1正则化和L2正则化
L1正则化是指权值向量w中各个元素的**绝对值之和**，通常表示为∣∣w∣∣1  
L2正则化是指权值向量w中各个元素的**平方和(然后再求平方根**（可以看到Ridge回归的L2正则化项有平方符号），通常表示为∣∣w∣∣2)
### 1、L1正则化和L2正则化的作用
L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择  

L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合  
### 2、稀疏模型与特征选择
上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？  
稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0.   

通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。  
在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。  

这就是稀疏模型与特征选择的关系。

&nbsp;
## 三、L1和L2正则化的直观理解（线性回归）
**考虑二维的情况，即只有两个权值w1 和w2**      

**图解详见参考文档**   

### 1、等值线表示什么，为什么是椭圆形的？
**等值线表示残差平方和（线性回归）**，因为残差平方和是椭圆形的  
[3.4 收缩的方法](https://esl.hohoweiya.xyz/03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html)

### 2、L1图像
假设只有两个权值w1和w2 ，此时 :  
**L1=∣w1∣+∣w2∣,这个函数画出来就是一个方框**.   

而正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大（上图中的黑色方框）；α越大，L的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值(w1,w2)=(0,w)中的w可以取到很小的值。

<figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-efc752bd6d1ce09dbf2e18b9766570eb_b.jpg" data-caption="" data-size="normal" data-rawwidth="443" data-rawheight="421" class="origin_image zh-lightbox-thumb" width="443" data-original="https://pic3.zhimg.com/v2-efc752bd6d1ce09dbf2e18b9766570eb_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;443&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="443" data-rawheight="421" class="origin_image zh-lightbox-thumb lazy" width="443" data-original="https://pic3.zhimg.com/v2-efc752bd6d1ce09dbf2e18b9766570eb_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-efc752bd6d1ce09dbf2e18b9766570eb_b.jpg"/></figure>

<ol><li>以同一条原曲线目标等高线来说，现在以最外圈的红色等高线为例，我们看到，对于红色曲线上的每个点都可做一个菱形，根据上图可知，当这个菱形与某条等高线相切（仅有一个交点）的时候，这个菱形最小，上图相割对比较大的两个菱形对应的 L1 范数更大。用公式说这个时候能使得在相同的 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D++%5Csum_%7Bi+%3D+1%7D%5EN%7B%28y_i+-w%5ET+x_i%29%5E2+%7D" alt="\frac{1}{N}  \sum_{i = 1}^N{(y_i -w^T x_i)^2 }" eeimg="1"/> ，由于相切的时候的  <img src="https://www.zhihu.com/equation?tex=C%7C%7Cw%7C%7C_%7B1%7D+" alt="C||w||_{1} " eeimg="1"/> 小，即 <img src="https://www.zhihu.com/equation?tex=%7Cw_1%7C%2B%7Cw_2%7C" alt="|w_1|+|w_2|" eeimg="1"/>所以能够使得<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D++%5Csum%7Bi+%3D+1%7D%5EN%7B%28y_i+-w%5ET+x_i%29%5E2+%7D%2B+C%7C%7Cw%7C%7C_%7B1%7D" alt="\frac{1}{N}  \sum{i = 1}^N{(y_i -w^T x_i)^2 }+ C||w||_{1}" eeimg="1"/> 更小；</li><li>有了第一条的说明我们可以看出，最终加入 L1 范数得到的解一定是某个菱形和某条原函数等高线的切点。现在有个比较重要的结论来了，<b>我们经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴（比如上图），也就是说最终的结果，解的某些维度及其容易是 0，比如上图最终解是</b> <img src="https://www.zhihu.com/equation?tex=w%3D%280%2Cx%29" alt="w=(0,x)" eeimg="1"/> <b>，这也就是我们所说的 L1 更容易得到稀疏解（解向量中 0 比较多）的原因；</b></li><li>当然光看着图说，L1 的菱形更容易和等高线相交在坐标轴一点都没说服力，只是个感性的认识，我们接下来从更严谨的方式来证明，简而言之就是假设现在我们是一维的情况下 <img src="https://www.zhihu.com/equation?tex=h%28w%29+%3D+f%28w%29+%2B+C%7Cw%7C" alt="h(w) = f(w) + C|w|" eeimg="1"/> ，其中 h(w) 是目标函数， <img src="https://www.zhihu.com/equation?tex=f%28w%29+" alt="f(w) " eeimg="1"/>  是没加 L1 正则化项前的目标函数， <img src="https://www.zhihu.com/equation?tex=C%7Cw%7C" alt="C|w|" eeimg="1"/> 是 L1 正则项，要使得 0 点成为最值可能的点，虽然在 0 点不可导，但是我们只需要让 0 点左右的导数异号，即 <img src="https://www.zhihu.com/equation?tex=h_%7Bl%7D%5E%7B%27%7D%280%29++h_%7Br%7D%5E%7B%27%7D%280%29+%3D+%28f%5E%7B%27%7D%280%29+%2B+C%29%28f%5E%7B%27%7D%280%29+-+C%29+%3C+0+" alt="h_{l}^{&#39;}(0)  h_{r}^{&#39;}(0) = (f^{&#39;}(0) + C)(f^{&#39;}(0) - C) &lt; 0 " eeimg="1"/>  即可也就是 <img src="https://www.zhihu.com/equation?tex=+C+%3E%7Cf%5E%7B%27%7D%280%29%7C" alt=" C &gt;|f^{&#39;}(0)|" eeimg="1"/> 的情况下，0 点都是可能的最值点。</li></ol><p>当加入 L2 正则化的时候，分析和 L1 正则化是类似的，也就是说我们仅仅是从菱形变成了圆形而已，同样还是求原曲线和圆形的切点作为最终解。当然与 L1 范数比，我们这样求的 L2 范数的<b>从图上来看，不容易交在坐标轴上，但是仍然比较靠近坐标轴。因此这也就是我们老说的，L2 范数能让解比较小（靠近 0），但是比较平滑（不等于 0）。</b></p><p>综上所述，我们可以看见，加入正则化项，在最小化经验误差的情况下，可以让我们选择解更简单（趋向于 0）的解。</p><p>结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。</p><p><b>因此，加正则化项就是结构风险最小化的一种实现。</b></p><p><b>正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。</b></p>

<p><b>简单总结下</b>：</p><p>给 loss function 加上正则化项，能使新得到的优化目标函数 <img src="https://www.zhihu.com/equation?tex=h+%3D+f%2B%7C%7Cw%7C%7C+" alt="h = f+||w|| " eeimg="1"/> ，需要在 f 和 ||w|| 中做一个权衡，如果还像原来只优化 f 的情况下，那可能得到一组解比较复杂，使得正则项 ||w|| 比较大，那么 h 就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。</p><p>L1 正则化就是在 loss function 后边所加正则项为 L1 范数，加上 L1 范数容易得到稀疏解（0 比较多）。L2 正则化就是 loss function 后边所加正则项为 L2 范数的平方，加上 L2 正则相比于 L1 正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于 0（但不是等于 0，所以相对平滑）的维度比较多，降低模型的复杂度。</p>

[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)
### 3、L2图像
二维平面下L2正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此J0 与L 相交时使得w1或w2等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数w都为0的情况。

&nbsp;
## reference
[深入理解L1、L2正则化](https://zhuanlan.zhihu.com/p/29360425)   
[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)    
[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)
