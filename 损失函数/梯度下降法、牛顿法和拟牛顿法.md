## 一、梯度下降
在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。

### 0、最小二乘法
[最小二乘法（least sqaure method）](https://zhuanlan.zhihu.com/p/38128785)  

<p>最小二乘法是由勒让德在19世纪发现的，形式如下式：</p><p><img src="https://www.zhihu.com/equation?tex=%E6%A0%87%E5%87%BD%E6%95%B0+%3D+%5Csum%EF%BC%88%E8%A7%82%E6%B5%8B%E5%80%BC-%E7%90%86%E8%AE%BA%E5%80%BC%EF%BC%89%5E2%5C%5C" alt="标函数 = \sum（观测值-理论值）^2\\" eeimg="1"/> <br/>观测值就是我们的多组样本，理论值就是我们的假设拟合函数。目标函数也就是在机器学习中常说的损失函数，我们的目标是得到使目标函数最小化时候的拟合函数的模型。举一个最简单的线性回归的简单例子，比如我们有 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"/> 个只有一个特征的样本： <img src="https://www.zhihu.com/equation?tex=%28x_i%2C+y_i%29%28i%3D1%2C+2%2C+3...%2Cm%29" alt="(x_i, y_i)(i=1, 2, 3...,m)" eeimg="1"/> </p><p>样本采用一般的 <img src="https://www.zhihu.com/equation?tex=h_%7B%5Ctheta%7D%28x%29" alt="h_{\theta}(x)" eeimg="1"/> 为 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 次的多项式拟合， <img src="https://www.zhihu.com/equation?tex=h_%7B%5Ctheta%7D%28x%29%3D%5Ctheta_0%2B%5Ctheta_1x%2B%5Ctheta_2x%5E2%2B...%5Ctheta_nx%5En%2C%5Ctheta%28%5Ctheta_0%2C%5Ctheta_1%2C%5Ctheta_2%2C...%2C%5Ctheta_n%29" alt="h_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2+...\theta_nx^n,\theta(\theta_0,\theta_1,\theta_2,...,\theta_n)" eeimg="1"/> 为参数</p><p>最小二乘法就是要找到一组 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%28%5Ctheta_0%2C%5Ctheta_1%2C%5Ctheta_2%2C...%2C%5Ctheta_n%29" alt="\theta(\theta_0,\theta_1,\theta_2,...,\theta_n)" eeimg="1"/>  使得 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En%28h_%7B%5Ctheta%7D%28x_i%29-y_i%29%5E2" alt="\sum_{i=1}^n(h_{\theta}(x_i)-y_i)^2" eeimg="1"/>  (残差平方和) 最小，即，求 <img src="https://www.zhihu.com/equation?tex=min%5Csum_%7Bi%3D1%7D%5En%28h_%7B%5Ctheta%7D%28x_i%29-y_i%29%5E2" alt="min\sum_{i=1}^n(h_{\theta}(x_i)-y_i)^2" eeimg="1"/> </p><hr/>
### 1、梯度下降算法
梯度下降法(Gradient descent)或最速下降法(steepest descent)是求解无约束最优化问题的一种常用的、实现简单的方法。

梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

### 2、梯度下降的算法调优
在使用梯度下降时，需要进行调优。哪些地方需要调优呢？
```
a)算法的步长选择。步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。
步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。

b)算法参数的初始值选择。初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；
当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

c)归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。
```
### 3.梯度下降法大家族（BGD，SGD，MBGD）
```
a) 批量梯度下降法（Batch Gradient Descent）
批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。

b) 随机梯度下降法（Stochastic Gradient Descent）
随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的样本的数据，而是仅仅选取一个样本来求梯度。
随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。
自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，
而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。
对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。
对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

c) 小批量梯度下降法（Mini-batch Gradient Descent）
小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于 m 个样本，我们采用 x 个样子来迭代， 1<x<m 。
一般可以取 x=16,32,64… ，当然根据样本的数据，可以调整这个 x 的值。
```
### 4. 批量梯度下降法python实现
```
def train(X, y, W, B, alpha, max_iters):
    '''
    使用了所有的样本进行梯度下降
    X: 训练集,
    y: 标签,
    W: 权重向量,
    B: bias,
    alpha: 学习率,
    max_iters: 最大迭代次数.
    '''
    dW = 0 # 权重梯度收集器
    dB = 0 # Bias梯度的收集器
    m = X.shape[0] # 样本数
    for i in range(max_iters):
        dW = 0 # 每次迭代重置
        dB = 0
        for j in range(m):
            # 1. 迭代所有的样本
            # 2. 计算权重和bias的梯度保存在w_grad和b_grad,
            # 3. 通过增加w_grad和b_grad来更新dW和dB
            W = W - alpha * (dW / m) # 更新权重
            B = B - alpha * (dB / m) # 更新bias

    return W, B 
```
### 5. 梯度下降法和最小二乘法
梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。  
梯度下降法是迭代求解，最小二乘法是计算解析解。  
**如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快**。  
**但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵**，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

&nbsp;
## 二、牛顿法
一般来说，牛顿法主要应用在两个方面，1：求方程的根；2：最优化。
### 1.求解方程
并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。

原理是利用泰勒公式，在 x0 处展开，且展开到一阶……
### 2.最优化
无约束最优化问题 ……
设 f(x) 具有二阶连续偏导数，若第 k 次迭代值为……
### 3.牛顿法与梯度下降法
梯度下降法和牛顿法相比，两者都是迭代求解，不过**梯度下降法是梯度求解，而牛顿法是用二阶的海森矩阵的逆矩阵求解**。  
相对而言，使用**牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长**。  
<p>梯度下降法： <img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%2B1%29%7D%3Dx%5E%7B%28k%29%7D-%5Clambda+%5Cnabla+f%28x%5E%7Bk%7D%29" alt="x^{(k+1)}=x^{(k)}-\lambda \nabla f(x^{k})" eeimg="1"/> </p><p>牛顿法： <img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%2B1%29%7D%3Dx%5E%7B%28k%29%7D-%5Clambda+%28H%5E%7B%28k%29%7D%29%5E%7B-1%7D+%5Cnabla+f%28x%5E%7Bk%7D%29" alt="x^{(k+1)}=x^{(k)}-\lambda (H^{(k)})^{-1} \nabla f(x^{k})" eeimg="1"/> </p>

<p>如下图是一个最小化一个目标方程的例子，红色曲线是利用牛顿法迭代求解，绿色曲线是利用梯度下降法求解。<br/></p><figure data-size="normal"><noscript><img src="https://picb.zhimg.com/v2-a25a1119558a899809d5f254f3ce2ce7_b.jpg" data-caption="" data-size="normal" data-rawwidth="335" data-rawheight="383" class="content_image" width="335"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;335&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="335" data-rawheight="383" class="content_image lazy" width="335" data-actualsrc="https://picb.zhimg.com/v2-a25a1119558a899809d5f254f3ce2ce7_b.jpg"/></figure><p>至于为什么牛顿法收敛更快，通俗来说梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。更多的可见：最优化问题中，<a href="https://www.zhihu.com/question/19723347" class="internal">牛顿法为什么比梯度下降法求解需要的迭代次数更少？</a></p>

&nbsp;
## 三、拟牛顿法的思路
<p>在牛顿法的迭代中，需要计算海森矩阵的逆矩阵 <img src="https://www.zhihu.com/equation?tex=H%5E%7B-1%7D" alt="H^{-1}" eeimg="1"/> ，这一计算比较复杂，考虑用一个 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/> 阶矩阵 <img src="https://www.zhihu.com/equation?tex=G_k%3DG%28x%5E%7B%28k%29%7D%29" alt="G_k=G(x^{(k)})" eeimg="1"/> 来近似代替 <img src="https://www.zhihu.com/equation?tex=H_k%5E%7B-1%7D%3DH%5E%7B-1%7D%28x%5E%7B%28k%29%7D%29" alt="H_k^{-1}=H^{-1}(x^{(k)})" eeimg="1"/> 。这就是拟牛顿法的基本想法。</p><p>要找到近似的替代矩阵，必定要和 <img src="https://www.zhihu.com/equation?tex=H_k" alt="H_k" eeimg="1"/> 有类似的性质。先看下牛顿法迭代中海森矩阵 <img src="https://www.zhihu.com/equation?tex=H_k" alt="H_k" eeimg="1"/> 满足的条件。首先 <img src="https://www.zhihu.com/equation?tex=H_k" alt="H_k" eeimg="1"/> 满足以下关系：</p>

&nbsp;
## 四、其他算法
下面介绍Broyden类拟牛顿法
### 1.DFP(Davidon-Fletcher-Powell)算法(DFP algorithm)
<p>DFP算法中选择 <img src="https://www.zhihu.com/equation?tex=G_%7Bk%7D" alt="G_{k}" eeimg="1"/> 作为 <img src="https://www.zhihu.com/equation?tex=H_%7Bk%7D%5E%7B-1%7D" alt="H_{k}^{-1}" eeimg="1"/> 的近似，假设每一步迭代中矩阵 <img src="https://www.zhihu.com/equation?tex=G_%7Bk%2B1%7D" alt="G_{k+1}" eeimg="1"/> 是由 <img src="https://www.zhihu.com/equation?tex=G_%7Bk%7D" alt="G_{k}" eeimg="1"/> 加上两个附加项构成，即<br/> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5C%5C%26+G_%7Bk%2B1%7D%3DG_%7Bk%7D%2BP_%7Bk%7D%2BQ_%7Bk%7D%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \\&amp; G_{k+1}=G_{k}+P_{k}+Q_{k}\end{align*}\\" eeimg="1"/> </p>

### 2.BFGS(Broyden-Fletcher-Goldfard-Shano)算法(BFGS algorithm)
### 3.Broyden类算法(Broyden's algorithm)

&nbsp;
## reference
[梯度下降法、牛顿法和拟牛顿法](https://zhuanlan.zhihu.com/p/37524275)  
[牛顿法与拟牛顿法学习笔记（一）牛顿法](https://blog.csdn.net/itplus/article/details/21896453)  
[梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)  
