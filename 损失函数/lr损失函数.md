<p>逻辑回归虽然名字里面有回归，但是主要用来解决分类问题。</p><p class="ztext-empty-paragraph"><br/></p><p><b>一、线性回归（Linear Regression）</b></p><p>线性回归的表达式：</p><p><img src="https://www.zhihu.com/equation?tex=f%28%5Cbm%7Bx%7D%29+%3D+%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D+%2B+b" alt="f(\bm{x}) = \bm{w}^T\bm{x} + b" eeimg="1"/></p><p>线性回归对于给定的输入 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="\bm{x}" eeimg="1"/> ，输出的是一个数值 y ，因此它是一个解决回归问题的模型。</p><p>为了消除掉后面的常数项b，我们可以令 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bmatrix%7D+%5Cbm%7Bx%7D%5E%7B%27%7D+%26%3D+%5B+1+%26+%5Cbm%7Bx%7D%5D%5ET++%5Cend%7Bmatrix%7D" alt="\begin{matrix} \bm{x}^{&#39;} &amp;= [ 1 &amp; \bm{x}]^T  \end{matrix}" eeimg="1"/> ，同时 <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bmatrix%7D+%5Cbm%7Bw%7D%5E%7B%27%7D+%26%3D+%5B+b+%26+%5Cbm%7Bw%7D%5D%5ET+%5Cend%7Bmatrix%7D" alt="\begin{matrix} \bm{w}^{&#39;} &amp;= [ b &amp; \bm{w}]^T \end{matrix}" eeimg="1"/> ，也就是说给x多加一项而且值恒为1，这样b就到了w里面去了，直线方程可以化简成为：</p><p><img src="https://www.zhihu.com/equation?tex=f%28%5Cbm%7Bx%27%7D%29+%3D+%5Cbm%7Bw%27%7D%5ET%5Cbm%7Bx%27%7D+" alt="f(\bm{x&#39;}) = \bm{w&#39;}^T\bm{x&#39;} " eeimg="1"/> </p><p>在接下来的文章中为了方便，我们所使用的 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%2C%5Cbm%7Bx%7D" alt="\bm{w},\bm{x}" eeimg="1"/> 其实指代的是 <img src="https://www.zhihu.com/equation?tex=w%27%2Cx%27" alt="w&#39;,x&#39;" eeimg="1"/> 。</p><p class="ztext-empty-paragraph"><br/></p><p><b>二、分类问题（Classification）</b></p><p>二分类问题就是给定的输入 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="\bm{x}" eeimg="1"/>，判断它的标签是A类还是类。二分类问题是最简单的分类问题。我们可以把多分类问题转化成一组二分类问题。比如最简单的是OVA(One-vs-all)方法，比如一个10分类问题，我们可以分别判断输入 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="\bm{x}" eeimg="1"/> 是否属于某个类，从而转换成10个二分类问题。</p><p>因此，解决了二分类问题，相当于解决了多分类问题。</p><p class="ztext-empty-paragraph"><br/></p><p><b>三、如何用连续的数值去预测离散的标签值呢？</b></p><p>线性回归的输出是一个数值，而不是一个标签，显然不能直接解决二分类问题。那我如何改进我们的回归模型来预测标签呢？</p><p>一个最直观的办法就是设定一个阈值，比如0，如果我们预测的数值 y &gt; 0 ，那么属于标签A，反之属于标签B，采用这种方法的模型又叫做<b>感知机</b>（Perceptron）。</p><p>另一种方法，我们不去直接预测标签，而是去预测标签为A概率，我们知道概率是一个[0,1]区间的连续数值，那我们的输出的数值就是标签为A的概率。一般的如果标签为A的概率大于0.5，我们就认为它是A类，否则就是B类。这就是我们的这次的主角<b>逻辑回归模型 </b>(Logistics Regression)。</p><p class="ztext-empty-paragraph"><br/></p><p><b>四、逻辑回归（logistics regression）</b></p><p>明确了预测目标是标签为A的概率。</p><p>我们知道，概率是属于[0,1]区间。但是线性模型 <img src="https://www.zhihu.com/equation?tex=f%28%5Cbm%7Bx%7D%29+%3D+%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D" alt="f(\bm{x}) = \bm{w}^T\bm{x}" eeimg="1"/> 值域是 <img src="https://www.zhihu.com/equation?tex=%28-%5Cinfty%2C%5Cinfty%29" alt="(-\infty,\infty)" eeimg="1"/> 。</p><p>我们不能直接基于线性模型建模。需要找到一个模型的值域刚好在[0,1]区间，同时要足够好用。</p><p>于是，选择了我们的sigmoid函数。</p><p>它的表达式为： <img src="https://www.zhihu.com/equation?tex=%5Csigma%28x%29+%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D" alt="\sigma(x) =\frac{1}{1+e^{-x}}" eeimg="1"/> 。</p><p>它的图像：</p><figure data-size="normal"><noscript><img src="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_b.jpg" data-size="normal" data-rawwidth="468" data-rawheight="167" class="origin_image zh-lightbox-thumb" width="468" data-original="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;468&#39; height=&#39;167&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="468" data-rawheight="167" class="origin_image zh-lightbox-thumb lazy" width="468" data-original="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_r.jpg" data-actualsrc="https://picb.zhimg.com/v2-ab82d755dba95f0585678fe2d4af28d6_b.jpg"/><figcaption>sigmoid函数</figcaption></figure><p>这个函数的有很多非常好的性质，一会儿你就会感受到。但是我们不能直接拿了sigmoid函数就用，毕竟它连要训练的参数 w 都没得。</p><p>我们结合sigmoid函数，线性回归函数，把线性回归模型的输出作为sigmoid函数的输入。于是最后就变成了逻辑回归模型：</p><p><img src="https://www.zhihu.com/equation?tex=y%3D%5Csigma%28f%28%5Cbm%7Bx%7D%29%29+%3D+%5Csigma%28%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D" alt="y=\sigma(f(\bm{x})) = \sigma(\bm{w}^T\bm{x})=\frac{1}{1+e^{-\bm{w}^T\bm{x}}}" eeimg="1"/> </p><p>假设我们已经训练好了一组权值 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5ET" alt="\bm{w}^T" eeimg="1"/> 。只要把我们需要预测的 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D" alt="\bm{x}" eeimg="1"/> 代入到上面的方程，输出的y值就是这个标签为A的概率，我们就能够判断输入数据是属于哪个类别。</p><p>接下来就来详细介绍，如何利用一组采集到的真实样本，训练出参数w的值。</p><p class="ztext-empty-paragraph"><br/></p><p><b>五、逻辑回归的损失函数（Loss Function）</b></p><p>损失函数就是用来衡量模型的输出与真实输出的差别。</p><p>假设只有两个标签1和0， <img src="https://www.zhihu.com/equation?tex=y_n+%5Cin%5C%7B0%2C+1%5C%7D" alt="y_n \in\{0, 1\}" eeimg="1"/> 。我们把采集到的任何一组样本看做一个事件的话，那么这个事件发生的概率假设为p。我们的模型y的值等于标签为1的概率也就是p。</p><p><img src="https://www.zhihu.com/equation?tex=P_%7By%3D1%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%3D+p" alt="P_{y=1}=\frac{1}{1+e^{-\bm{w}^T\bm{x}}} = p" eeimg="1"/> </p><p>因为标签不是1就是0，因此标签为0的概率就是： <img src="https://www.zhihu.com/equation?tex=P_%7By%3D0%7D+%3D+1-p" alt="P_{y=0} = 1-p" eeimg="1"/> 。</p><p>我们把单个样本看做一个事件，那么这个事件发生的概率就是：</p><p><img src="https://www.zhihu.com/equation?tex=P%28y%7C%5Cbm%7Bx%7D%29%3D%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+p%2C+y%3D1+%5C%5C+1-p%2Cy%3D0+%5Cend%7Baligned%7D+%5Cright." alt="P(y|\bm{x})=\left\{ \begin{aligned} p, y=1 \\ 1-p,y=0 \end{aligned} \right." eeimg="1"/> </p><p>这个函数不方便计算，它等价于:</p><p><img src="https://www.zhihu.com/equation?tex=P%28y_i%7C%5Cbm%7Bx%7D_i%29+%3D+p%5E%7By_i%7D%281-p%29%5E%7B1-%7By_i%7D%7D" alt="P(y_i|\bm{x}_i) = p^{y_i}(1-p)^{1-{y_i}}" eeimg="1"/> 。</p><p>解释下这个函数的含义，我们采集到了一个样本 <img src="https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29" alt="(\bm{x_i},y_i)" eeimg="1"/> 。对这个样本，它的标签是 <img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i" eeimg="1"/> 的概率是 <img src="https://www.zhihu.com/equation?tex=p%5E%7By_i%7D%281-p%29%5E%7B1-%7By_i%7D%7D" alt="p^{y_i}(1-p)^{1-{y_i}}" eeimg="1"/> 。 （当y=1，结果是p；当y=0，结果是1-p）。</p><p>如果我们采集到了一组数据一共N个， <img src="https://www.zhihu.com/equation?tex=%5C%7B%28%5Cbm%7Bx%7D_1%2Cy_1%29%2C%28%5Cbm%7Bx%7D_2%2Cy_2%29%2C%28%5Cbm%7Bx%7D_3%2Cy_3%29...%28%5Cbm%7Bx%7D_N%2Cy_N%29%5C%7D" alt="\{(\bm{x}_1,y_1),(\bm{x}_2,y_2),(\bm{x}_3,y_3)...(\bm{x}_N,y_N)\}" eeimg="1"/> ，这个合成在一起的合事件发生的总概率怎么求呢？其实就是将每一个样本发生的概率相乘就可以了，即采集到这组样本的概率：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P_%7B%E6%80%BB%7D+%26%3D+P%28y_1%7C%5Cbm%7Bx%7D_1%29P%28y_2%7C%5Cbm%7Bx%7D_2%29P%28y_3%7C%5Cbm%7Bx%7D_3%29....P%28y_N%7C%5Cbm%7Bx%7D_N%29+%5C%5C++%26%3D+%5Cprod_%7Bn%3D1%7D%5E%7BN%7Dp%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D++%5Cend%7Baligned%7D" alt="\begin{aligned} P_{总} &amp;= P(y_1|\bm{x}_1)P(y_2|\bm{x}_2)P(y_3|\bm{x}_3)....P(y_N|\bm{x}_N) \\  &amp;= \prod_{n=1}^{N}p^{y_n}(1-p)^{1-y_n}  \end{aligned}" eeimg="1"/> </p><p>注意<img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB+%7D" alt="P_{总 }" eeimg="1"/> 是一个函数，并且未知的量只有 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> （在p里面）。</p><p>由于连乘很复杂，我们通过两边取对数来把连乘变成连加的形式，即：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+F%28%5Cbm%7Bw%7D%29%3Dln%28P_%7B%E6%80%BB%7D+%29++%26%3D+ln%28%5Cprod_%7Bn%3D1%7D%5E%7BN%7Dp%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D+%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7Dln+%28p%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%28y_n+ln+%28p%29+%2B+%281-y_n%29ln%281-p%29%29+%5Cend%7Baligned%7D+" alt="\begin{aligned} F(\bm{w})=ln(P_{总} )  &amp;= ln(\prod_{n=1}^{N}p^{y_n}(1-p)^{1-y_n} ) \\ &amp;= \sum_{n=1}^{N}ln (p^{y_n}(1-p)^{1-y_n}) \\ &amp;= \sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) \end{aligned} " eeimg="1"/> </p><p>其中， <img src="https://www.zhihu.com/equation?tex=p+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+" alt="p = \frac{1}{1+e^{-\bm{w}^T\bm{x}}} " eeimg="1"/> </p><p>这个函数 <img src="https://www.zhihu.com/equation?tex=F%28%5Cbm%7Bw%7D%29" alt="F(\bm{w})" eeimg="1"/>  又叫做它的<b>损失函数</b>。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。这里的损失函数的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号。</p><p class="ztext-empty-paragraph"><br/></p><p><b>六、最大似然估计MLE(Maximum Likelihood Estimation)</b></p><p>我们在真实世界中并不能直接看到概率是多少，我们只能观测到事件是否发生。也就是说，我们只能知道一个样本它实际的标签是1还是0。那么我们如何估计参数 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 跟b的值呢？</p><p>最大似然估计MLE(Maximum Likelihood Estimation)，就是一种估计参数 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的方法。在这里如何使用MLE来估计 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 呢？</p><p>在上一节，我们知道损失函数 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/> 是正比于总概率 <img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D" alt="P_{总}" eeimg="1"/> 的，而 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/> 又只有一个变量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 。也就是说，通过改变 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的值，就能得到不同的总概率值 <img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D" alt="P_{总}" eeimg="1"/> 。那么当我们选取的某个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> 刚好使得总概率 <img src="https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D" alt="P_{总}" eeimg="1"/> 取得最大值的时候。我们就认为这个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> 就是我们要求得的 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的值，这就是最大似然估计的思想。</p><p>现在我们的问题变成了，找到一个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> ，使得我们的总事件发生的概率，即损失函数 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/> 取得最大值，这句话用数学语言表达就是：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D+%3D+arg%5Cmax_%7Bw%7DF%28%5Cbm%7Bw%7D%29+%3D+-arg%5Cmin_%7Bw%7DF%28%5Cbm%7Bw%7D%29+" alt="\bm{w^*} = arg\max_{w}F(\bm{w}) = -arg\min_{w}F(\bm{w}) " eeimg="1"/> </p><p class="ztext-empty-paragraph"><br/></p><p><b>七、 求</b><img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/><b> 的梯度</b> <img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/> </p><p><b>梯度的定义</b> </p><p>我们知道对于一个一维的标量x，它有导数 <img src="https://www.zhihu.com/equation?tex=x%27" alt="x&#39;" eeimg="1"/> 。</p><p>对一个多维的向量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D+%3D+%28x_1%2Cx_2%2Cx_3%2C..%2Cx_n%29" alt="\bm{x} = (x_1,x_2,x_3,..,x_n)" eeimg="1"/>  来说，它的导数叫做梯度，也就是分别对于它的每个分量求导数 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D%27+%3D+%28x_1%27%2Cx_2%27%2Cx_3%27%2C..%2Cx_n%27%29" alt="\bm{x}&#39; = (x_1&#39;,x_2&#39;,x_3&#39;,..,x_n&#39;)" eeimg="1"/> 。</p><p class="ztext-empty-paragraph"><br/></p><p>接下来请拿出纸笔，一起动手来推导出 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/> 的表达式。请尽量尝试自己动手推导出来，如果哪一步不会了再看我的推导。</p><p class="ztext-empty-paragraph"><br/></p><p><b>七（二）、求梯度的推导过程</b></p><p>为了求出 <img src="https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="F（\bm{w}）" eeimg="1"/>的梯度<img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/>，我们需要做一些准备工作。原谅我非常不喜欢看大串的数学公式，所以我尽可能用最简单的数学符号来描述。当然可能不够严谨，但是我觉得更容易看懂。</p><p class="ztext-empty-paragraph"><br/></p><p>首先，我们需要知道向量是如何求导的。具体的推导过程以及原理请参见 <a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/mounty_fsc/article/details/51588794" class=" wrap external" target="_blank" rel="nofollow noreferrer">矩阵求导</a></p><p>我们只要记住几个结论就行了：对于一个矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D" alt="\bm{A}" eeimg="1"/> 乘以一个向量的方程 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D%5Cbm%7Bx%7D" alt="\bm{A}\bm{x}" eeimg="1"/> ，对向量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 求导的结果是 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D%5ET" alt="\bm{A}^T" eeimg="1"/> 。在这里我们把函数 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D%5Cbm%7Bx%7D" alt="\bm{A}\bm{x}" eeimg="1"/> 对 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 求梯度简单记作 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbm%7BA%7D%5Cbm%7Bx%7D%EF%BC%89%27" alt="（\bm{A}\bm{x}）&#39;" eeimg="1"/> 。因此<img src="https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbm%7BA%7D%5Cbm%7Bx%7D%EF%BC%89%27+%3D+%5Cbm%7BA%7D%5ET" alt="（\bm{A}\bm{x}）&#39; = \bm{A}^T" eeimg="1"/> , 推论是 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbm%7Bx%7D%5ET%5Cbm%7BA%7D%EF%BC%89%27+%3D+%5Cbm%7BA%7D" alt="（\bm{x}^T\bm{A}）&#39; = \bm{A}" eeimg="1"/> ，我们把 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D%2C%5Cbm%7Bw%7D%5ET" alt="\bm{x},\bm{w}^T" eeimg="1"/> 代入进去，可以知道 <img src="https://www.zhihu.com/equation?tex=%28%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%29%27+%3D+%5Cbm%7Bx%7D" alt="(\bm{w}^T\bm{x})&#39; = \bm{x}" eeimg="1"/> 。</p><p class="ztext-empty-paragraph"><br/></p><p>然后求 <img src="https://www.zhihu.com/equation?tex=1-p" alt="1-p" eeimg="1"/> 的值：</p><p><img src="https://www.zhihu.com/equation?tex=1-p%3D%5Cfrac%7Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D%7B+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D" alt="1-p=\frac{e^{-\bm{w}^T\bm{x}} }{ 1+e^{-\bm{w}^T\bm{x}} }" eeimg="1"/> </p><p>p是一个关于变量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 的函数，我们对p求导，通过链式求导法则，慢慢展开可以得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++p%27+%3D+f%27%28%5Cbm%7Bw%7D%29%26%3D+%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%29%27+%5C%5C+%26%3D+-%5Cfrac%7B1%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+++%C2%B7+%28+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%29%27+%5C%5C+%26%3D+-%5Cfrac%7B1%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+++%C2%B7+e%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D++%C2%B7+%28-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%29%27+%5C%5C+%26%3D+-%5Cfrac%7B1%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+++%C2%B7+e%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D++%C2%B7+%28-%5Cbm%7Bx%7D+%29+%5C%5C+%26%3D+%5Cfrac%7Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+%C2%B7+++%5Cbm%7Bx%7D+%5C%5C+%26%3D++%5Cfrac%7B1%7D%7B+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D++++%C2%B7+%5Cfrac%7Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D%7B+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D++%C2%B7+%5Cbm%7Bx%7D+%5C%5C+%26%3D+p%281-p%29%5Cbm%7Bx%7D+%5Cend%7Baligned%7D" alt="\begin{aligned}  p&#39; = f&#39;(\bm{w})&amp;= (\frac{1}{1+e^{-\bm{w}^T\bm{x}}} )&#39; \\ &amp;= -\frac{1}{ (1+e^{-\bm{w}^T\bm{x}} )^2}   · ( 1+e^{-\bm{w}^T\bm{x}})&#39; \\ &amp;= -\frac{1}{ (1+e^{-\bm{w}^T\bm{x}} )^2}   · e^{-\bm{w}^T\bm{x}}  · (-\bm{w}^T\bm{x})&#39; \\ &amp;= -\frac{1}{ (1+e^{-\bm{w}^T\bm{x}} )^2}   · e^{-\bm{w}^T\bm{x}}  · (-\bm{x} ) \\ &amp;= \frac{e^{-\bm{w}^T\bm{x}} }{ (1+e^{-\bm{w}^T\bm{x}} )^2} ·   \bm{x} \\ &amp;=  \frac{1}{ 1+e^{-\bm{w}^T\bm{x}} }    · \frac{e^{-\bm{w}^T\bm{x}} }{ 1+e^{-\bm{w}^T\bm{x}} }  · \bm{x} \\ &amp;= p(1-p)\bm{x} \end{aligned}" eeimg="1"/> </p><p>上面都是我们做的准备工作，总之我们得记住： <img src="https://www.zhihu.com/equation?tex=p%27+%3D+p%281-p%29%5Cbm%7Bx%7D" alt="p&#39; = p(1-p)\bm{x}" eeimg="1"/> , 并且可以知道 <img src="https://www.zhihu.com/equation?tex=%281-p%29%27+%3D+-p%281-p%29%5Cbm%7Bx%7D" alt="(1-p)&#39; = -p(1-p)\bm{x}" eeimg="1"/> 。</p><p class="ztext-empty-paragraph"><br/></p><p>下面我们正式开始对 <img src="https://www.zhihu.com/equation?tex=F%28%5Cbm%7Bw%7D%29" alt="F(\bm{w})" eeimg="1"/> 求导，求导的时候请始终记住，我们的变量只有 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> ，其他的什么 <img src="https://www.zhihu.com/equation?tex=y_n%2C%5Cbm%7Bx%7D_n+" alt="y_n,\bm{x}_n " eeimg="1"/> 都是已知的，可以看做常数。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26+%3D+%5Cnabla+%EF%BC%88+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%28y_n+ln+%28p%29+%2B+%281-y_n%29ln%281-p%29%29+%EF%BC%89%5C%5C+%26%3D+%5Csum+%28+y_n+ln%27%28p%29+%2B+%281-y_n%29+ln%27%281-p%29%29+%5C%5C+%26%3D+%5Csum%28+%28y_n+%5Cfrac%7B1%7D%7Bp%7Dp%27%29%2B%281-y_n%29%5Cfrac%7B1%7D%7B1-p%7D%281-p%29%27%29+%5C%5C+%26%3D+%5Csum%28y_n%281-p%29%5Cbm%7Bx%7D_n+-+%281-y_n%29p%5Cbm%7Bx%7D_n%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-p%29%5Cbm%7Bx%7D_n%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} \nabla F（\bm{w}）&amp; = \nabla （ \sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) ）\\ &amp;= \sum ( y_n ln&#39;(p) + (1-y_n) ln&#39;(1-p)) \\ &amp;= \sum( (y_n \frac{1}{p}p&#39;)+(1-y_n)\frac{1}{1-p}(1-p)&#39;) \\ &amp;= \sum(y_n(1-p)\bm{x}_n - (1-y_n)p\bm{x}_n) \\ &amp;= \sum_{n=1}^{N}{(y_n-p)\bm{x}_n} \end{aligned}" eeimg="1"/> </p><p>终于，我们求出了梯度 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/> 的表达式了，现在我们再来看看它长什么样子：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-p%29%5Cbm%7Bx%7D_n%7D++%5Cend%7Baligned%7D" alt="\begin{aligned} \nabla F（\bm{w}）&amp;= \sum_{n=1}^{N}{(y_n-p)\bm{x}_n}  \end{aligned}" eeimg="1"/> </p><p>它是如此简洁优雅，这就是我们选取sigmoid函数的原因之一。当然我们也能够把p再展开，即：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26%3D++%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D+++%5Cend%7Baligned%7D" alt="\begin{aligned} \nabla F（\bm{w}）&amp;=  \sum_{n=1}^{N}{(y_n- \frac{1}{1+e^{-\bm{w}^T\bm{x}_n}} )\bm{x}_n}   \end{aligned}" eeimg="1"/> </p><p class="ztext-empty-paragraph"><br/></p><p><b>八、梯度下降法（GD）与随机梯度下降法（SGD）</b></p><p>现在我们已经解出了损失函数 <img src="https://www.zhihu.com/equation?tex=+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt=" F（\bm{w}）" eeimg="1"/>在任意 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 处的梯度 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/>，可是我们怎么算出来 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D" alt="\bm{w^*}" eeimg="1"/> 呢？ 回到之前的问题，我们现在要求损失函数取最大值时候的<img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D" alt="\bm{w^*}" eeimg="1"/>的值：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D+%3D+arg%5Cmax_%7Bw%7DF%28%5Cbm%7Bw%7D%29+" alt="\bm{w^*} = arg\max_{w}F(\bm{w}) " eeimg="1"/></p><p><b>梯度下降法(Gradient Descent)</b>，可以用来解决这个问题。核心思想就是先随便初始化一个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_0" alt="\bm{w}_0" eeimg="1"/> ，然后给定一个步长 <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="\eta" eeimg="1"/> ，通过不断地修改 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+" alt="\bm{w}_{t+1} " eeimg="1"/>  &lt;- <img src="https://www.zhihu.com/equation?tex=+++%5Cbm%7Bw%7D_%7Bt%7D" alt="   \bm{w}_{t}" eeimg="1"/> ，从而最后靠近到达取得最大值的点，即不断进行下面的迭代过程，直到达到指定次数，或者梯度等于0为止。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\bm{w}_{t+1} = \bm{w}_t + \eta\nabla F（\bm{w}）" eeimg="1"/> </p><p><b>随机梯度下降法（Stochastic Gradient Descent）</b>，如果我们能够在每次更新过程中，加入一点点噪声扰动，可能会更加快速地逼近最优值。在SGD中，我们不直接使用 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/> ，而是采用另一个输出为随机变量的替代函数 <img src="https://www.zhihu.com/equation?tex=G%28%5Cbm%7Bw%7D%29" alt="G(\bm{w})" eeimg="1"/> :</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta++G%28%5Cbm%7Bw%7D%29" alt="\bm{w}_{t+1} = \bm{w}_t + \eta  G(\bm{w})" eeimg="1"/> </p><p>当然，这个替代函数 <img src="https://www.zhihu.com/equation?tex=G%28%5Cbm%7Bw%7D%29" alt="G(\bm{w})" eeimg="1"/>需要满足它的期望值等于<img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/>，相当于这个函数围绕着 <img src="https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89" alt="\nabla F（\bm{w}）" eeimg="1"/> 的输出值随机波动。</p><p class="ztext-empty-paragraph"><br/></p><p>在这里我先解释一个问题：<b>为什么可以用梯度下降法？</b></p><p>因为逻辑回归的损失函数L是一个连续的凸函数（conveniently convex）。这样的函数的特征是，它只会有一个全局最优的点，不存在局部最优。对于GD跟SGD最大的潜在问题就是它们可能会陷入局部最优。然而这个问题在逻辑回归里面就不存在了，因为它的损失函数的良好特性，导致它并不会有好几个局部最优。当我们的GD跟SGD收敛以后，我们得到的极值点一定就是全局最优的点，因此我们可以放心地用GD跟SGD来求解。</p><p class="ztext-empty-paragraph"><br/></p><p>好了，那我们要怎么实现学习算法呢？其实很简单，注意我们GD求导每次都耿直地用到了所有的样本点，从1一直到N都参与梯度计算。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26%3D++%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D+%5Cend%7Baligned%7D" alt="\begin{aligned} \nabla F（\bm{w}）&amp;=  \sum_{n=1}^{N}{(y_n- \frac{1}{1+e^{-\bm{w}^T\bm{x}_n}} )\bm{x}_n} \end{aligned}" eeimg="1"/> </p><p>在SGD中，我们每次只要均匀地、随机选取其中一个样本 <img src="https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29" alt="(\bm{x_i},y_i)" eeimg="1"/> ,用它代表整体样本，即把它的值乘以N，就相当于获得了梯度的无偏估计值，即 <img src="https://www.zhihu.com/equation?tex=E%28G%28%5Cbm%7Bw%7D%29%29+%3D+%5Cnabla+F%28%5Cbm%7Bw%7D%29" alt="E(G(\bm{w})) = \nabla F(\bm{w})" eeimg="1"/> ，因此SGD的更新公式为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta++N++%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D" alt="\bm{w}_{t+1} = \bm{w}_t + \eta  N  {(y_n- \frac{1}{1+e^{-\bm{w}^T\bm{x}_n}} )\bm{x}_n}" eeimg="1"/> </p><p>这样我们前面的求和就没有了，同时 <img src="https://www.zhihu.com/equation?tex=%5Ceta++N" alt="\eta  N" eeimg="1"/> 都是常数， <img src="https://www.zhihu.com/equation?tex=N" alt="N" eeimg="1"/> 的值刚好可以并入 <img src="https://www.zhihu.com/equation?tex=%5Ceta+" alt="\eta " eeimg="1"/>  当中,因此SGD的迭代更新公式为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta+++%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D" alt="\bm{w}_{t+1} = \bm{w}_t + \eta   {(y_n- \frac{1}{1+e^{-\bm{w}^T\bm{x}_n}} )\bm{x}_n}" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29" alt="(\bm{x_i},y_i)" eeimg="1"/> 是对所有样本随机抽样的一个结果。</p><p class="ztext-empty-paragraph"><br/></p><p><b>九、逻辑回归的可解释性</b></p><p>逻辑回归最大的特点就是<b>可解释性</b>很强。</p><p>在模型训练完成之后，我们获得了一组n维的权重向量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> 跟偏差 b。</p><p>对于权重向量 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/>，它的每一个维度的值，代表了这个维度的特征对于最终分类结果的贡献大小。假如这个维度是正，说明这个特征对于结果是有正向的贡献，那么它的值越大，说明这个特征对于分类为正起到的作用越重要。</p><p>对于偏差b (Bias)，一定程度代表了正负两个类别的判定的容易程度。假如b是0，那么正负类别是均匀的。如果b大于0，说明它更容易被分为正类，反之亦然。</p><p>根据逻辑回归里的权重向量在每个特征上面的大小，就能够对于每个特征的重要程度有一个量化的清楚的认识，这就是为什么说逻辑回归模型有着很强的解释性的原因。</p><p><b>十、决策边界</b></p><p>补充评论里的一个问题，逻辑回归的决策边界是否是线性的，相当于问曲线：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%3D+0.5" alt="\frac{1}{1+e^{-\bm{w}^T\bm{x}}} = 0.5" eeimg="1"/> </p><p>是不是的线性的，我们可以稍微化简一下上面的曲线公式，得到：</p><p><img src="https://www.zhihu.com/equation?tex=e%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%3D+1+%3D+e%5E%7B0%7D+%5C%5C+%E5%8D%B3++-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D+%3D+0" alt="e^{-\bm{w}^T\bm{x}} = 1 = e^{0} \\ 即  -\bm{w}^T\bm{x} = 0" eeimg="1"/> </p><p>我们得到了一个等价的曲线，显然它是一个超平面（它在数据是二维的情况下是一条直线）。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-3f2227d606b29be9619566b88d1f1912_b.jpg" data-caption="" data-size="normal" data-rawwidth="1934" data-rawheight="1098" class="origin_image zh-lightbox-thumb" width="1934" data-original="https://pic4.zhimg.com/v2-3f2227d606b29be9619566b88d1f1912_r.jpg"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1934&#39; height=&#39;1098&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1934" data-rawheight="1098" class="origin_image zh-lightbox-thumb lazy" width="1934" data-original="https://pic4.zhimg.com/v2-3f2227d606b29be9619566b88d1f1912_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-3f2227d606b29be9619566b88d1f1912_b.jpg"/></figure><p><b>十一、总结</b></p><p>终于一切都搞清楚了，现在我们来理一理思路，首先逻辑回归模型长这样：</p><p><img src="https://www.zhihu.com/equation?tex=y%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D" alt="y=\frac{1}{1+e^{-\bm{w}^T\bm{x}}}" eeimg="1"/> </p><p>其中我们不知道的量是 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/> ，假设我们已经训练好了一个 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> , 我们用模型来判断 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D_i" alt="\bm{x}_i" eeimg="1"/> 的标签呢？很简单，直接将<img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D_i" alt="\bm{x}_i" eeimg="1"/>代入y中，求出来的值就是<img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D_i" alt="\bm{x}_i" eeimg="1"/>的标签是1的概率，如果概率大于0.5，那么我们认为它就是1类，否则就是0类。</p><p>那怎么得到 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/> 呢？</p><p>如果采用随机梯度下降法的话，我们首先随机产生一个<img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D" alt="\bm{w}" eeimg="1"/>的初始值 <img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_0" alt="\bm{w}_0" eeimg="1"/> ,然后通过公式不断迭代从而求得<img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A" alt="\bm{w}^*" eeimg="1"/>的值：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta+++%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D" alt="\bm{w}_{t+1} = \bm{w}_t + \eta   {(y_n- \frac{1}{1+e^{-\bm{w}^T\bm{x}_n}} )\bm{x}_n}" eeimg="1"/> </p><p>每次迭代都从所有样本中随机抽取一个 <img src="https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29" alt="(\bm{x_i},y_i)" eeimg="1"/> 来代入上述方程。</p>

[逻辑回归 logistics regression 公式推导](https://zhuanlan.zhihu.com/p/44591359)

&nbsp;
## 为什么要求最大对数似然估计而不是最大似然估计：
```
其中最重要的一点就是为什么取−log函数为损失函数，损失函数的本质就是，如果我们预测对了，能够不惩罚，
如果预测错误，会导致损失函数变得很大，也就是惩罚较大，而−log函数在[0,1]之间正好符合这一点，
另外还有一点需要说明，LR是一种广义的线性回归模型，平方损失函数对于 Sigmoid函数求导计算,无法保证是凸函数，
在优化的过程中，求得的解有可能是局部最小，不是全局的最优值。

其二：取完对数之后，对我们的后续求导比较方便。
如果根据似然函数，直接计算，有两点缺点：(1)不利于后续的求导,(2)似然函数的计算会导致下溢出。
```
### 损失函数
对数损失函数：L(Y,P(Y∣X))=−logP(Y∣X)

平方损失函数：L(Y,f(X))=(Y−f(X)2  线性回归模型使用了平方损失函数：

指数损失函数：L(y,f(x))=e−yf(x)

Hinge损失函数：L(y,f(x))=max(0,w(y))

0-1损失函数：

绝对值损失函数：L(Y,f(X))=∣Y−f(X)∣

[LR逻辑回归与损失函数理解](https://blog.csdn.net/weixin_41725746/article/details/93378662)

&nbsp;
## reference
在逻辑回归的推导中，我们假设样本是服从伯努利分布(0-1分布)的，然后求得满足该分布的似然函数，最终求该似然函数的极大值。整体的思想就是求极大似然函数的思想。

而取对数，只是为了方便我们的在求MLE(Maximum Likelihood Estimation)过程中采取的一种数学手段而已。

[logistic回归详解(二）：损失函数（cost function）详解](https://blog.csdn.net/bitcarmanlee/article/details/51165444)
